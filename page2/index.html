<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>KevinGuo</title>
    <link rel="stylesheet" href="https://kevinguo.me/assets/vendor/primer-css/css/primer.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/vendor/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/vendor/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/components/collection.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/components/repo-card.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/sections/repo-list.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/sections/mini-repo-list.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/components/boxed-group.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/globals/common.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/vendor/share.js/dist/css/share.min.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/globals/responsive.css">
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/posts/index.css">
    <!-- Latest compiled and minified CSS -->
    
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/pages/index.css">
    

    
    <link rel="canonical" href="https://kevinguo.me/page2/">
    <link rel="alternate" type="application/atom+xml" title="KevinGuo" href="https://kevinguo.me/feed.xml">
    <link rel="shortcut icon" href="https://kevinguo.me/favicon.ico">
    
    <meta name="keywords" content="KevinGuo">
    <meta name="description" content="KevinGuo's blog">
    
    
        
    
    <meta property="og:url" content="https://kevinguo.me/page2/">
    <meta property="og:site_name" content="KevinGuo">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh_CN" />
    
    <script src="https://kevinguo.me/assets/vendor/jquery/dist/jquery.min.js"></script>
    <script src="https://kevinguo.me/assets/js/jquery-ui.js"></script>
    <script type="text/javascript">
    function toggleMenu() {
        var nav = document.getElementsByClassName("site-header-nav")[0];
        if (nav.style.display == "inline-flex") {
          nav.style.display = "none";
        } else {
          nav.style.display = "inline-flex";
        }
    }
    </script>
</head>
<body class="home" data-mz="home">
    <header class="site-header">
        <div class="container">
            <h1><a href="https://kevinguo.me/" title="KevinGuo"><span class="octicon octicon-mark-github"></span> KevinGuo</a></h1>
            <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <nav class="site-header-nav" role="navigation">
                
                <a href="https://kevinguo.me/" class=" site-header-nav-item" target="" title="首页">首页</a>
                
                <a href="https://kevinguo.me/categories/" class=" site-header-nav-item" target="" title="分类">分类</a>
                
                <a href="https://kevinguo.me/wiki/" class=" site-header-nav-item" target="" title="维基">维基</a>
                
                <a href="https://kevinguo.me/open-source/" class=" site-header-nav-item" target="" title="开源">开源</a>
                
                <a href="https://kevinguo.me/links/" class=" site-header-nav-item" target="" title="链接">链接</a>
                
                <a href="https://kevinguo.me/about/" class=" site-header-nav-item" target="" title="关于">关于</a>
                
            </nav>
        </div>
    </header>
    <!-- / header -->

    <section class="banner">
    <div class="collection-head">
        <div class="container">
            <div class="collection-title">
              <h1 class="collection-header" id="sub-title"><span>Just do it now !</span></h1>
                <div class="collection-info">
                    <span class="meta-info mobile-hidden">
                        <span class="octicon octicon-location"></span>
                        Wuhan, China
                    </span>
                    <span class="meta-info">
                        <span class="octicon octicon-organization"></span>
                        <a href="" target="_blank">Thoughtworks,Inc.</a>
                    </span>
                     <span class="meta-info">
                        <span class="octicon octicon-mark-github"></span>
                        <a href="https://github.com/chinakevinguo" target="_blank">chinakevinguo</a>
                    </span>
                </div>
            </div>
        </div>
    </div>
</section>
<!-- /.banner -->
<section class="container content">
    <div class="columns">
        <div class="column two-thirds" >
            <ol class="repo-list">
                
                <li class="repo-list-item">
                    <h3 class="repo-list-name">
                      <a href="https://kevinguo.me/2017/09/12/kubernetes-ceph-2/">kubernetes ceph 笔记 2</a>
                    </h3>
                    <p class="repo-list-description">
                        其实我们在上一章的时候，已经简单的介绍了ceph中的基础组件以及其他软体架构，那么这一章，主要是来详细的介绍下ceph的工作原理、流程以及更实际的操作，比如ceph 内部的CRUSH bucket调整，PG/PGP参数调整等。并在最后简单的介绍了些CEPH集群硬件要求等。


寻址流程

通过对Ceph系统中寻址流程的了解来熟悉Ceph中的几个概念，寻址流程如下图



上图的几个概念说明如下：

1.File

此处的file就是用户需要存储或者访问的文件。对于一个基于Ceph开发的对象存储而言，这个file也就对应于应用中的”对象”，也就是用户直接操作的“对象”。

2.Object

Object是Ceph中最小存储单元，是由一个数据和一个元数据绑定的整体，元数据中存放了具体数据的属性信息等。Object和File区别在于，object的最大size由RADOS限定（通常为2MB或者4MB），以便实现底层存储的组织管理。因此，当上层应用向RADOS存入size很大的File时，需要将File切分成统一大小的一系列Object(最后一个大小可以不同)进行存储。

3.PG

PG的用途就是对Object的存储进行组织和位置映射。具体而言，一个PG负责组织若干个object(可以为数千个甚至更多)，但一个object只能被映射到一个PG中，即PG和object之间是“一对多”映射关系。同时，一个PG会被映射到n个OSD上，而每个OSD上都会承载大量的PG，即PG和OSD之间是“多对多”映射关系。在实践中，n至少为2，如果用于生产环境，则至少为3。一个OSD上的PG则可达到数百个。事实上，PG数量的设置牵扯到数据分布的均匀性问题。这一点后面再讲。

4.OSD

这个上一篇已经讲过了，这里就不再赘述。唯一需要说明的是，OSD的数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。在实践中，至少也得数十上百个的量级才有助于Ceph系统的设计发挥其应有的优势。

5.CRUSH

在传统的文件存储系统中，元数据占着及其重要的位置，每次系统中数据更新时，元数据会首先被更新，然后才是实际数据的写入；在较小的存储系统中(GB/TB)，这种将元数据存储在某个固定几点或者磁盘阵列中的做法还能满足需求；但当数据量增大到PB/ZB时，元数据查找的性能会成为很大的一个瓶颈；而且元数据的统一存放还可能造成单点故障，即当元数据丢失后，实际数据将无法被找回；与传统文件存储系统不同的是，Ceph使用CRUSH算法来精确的计算数据应该被写入那里/从哪里读取；CRUSH按需计算元数据，而不是存储元数据，从而解决了传统文件存储系统的瓶颈。

6.CRUSH计算

CRUSH是伪随机算法，算法通过每个设备的权重(容量大小)来计算数据对象的分布。对象分布式由cluster map和data distribution policy决定的。
cluster map由device和bucket组成。描述了可用存储资源和层级结构(比如有多少个机架，每个机架上有多少个服务器，每个服务器上有多少个磁盘)。
data distribution policy由 placement rules组成。rule决定了每个数据对象有多少个副本，这些副本存储的限制条件(比如3个副本放在不同的机架中)。

在Ceph中，元数据的计算和负载也是分布式的，并且只有在需要的时候才会执行，元数据的计算过程称之为CRUSH计算，不同于其他分布式文件系统，Ceph的CRUSH计算是由客户端使用自己的资源来完成，从而去除了中心查找带来的性能及单节点故障问题。

CRUSH查找时候，ceph client先通过MON 获取cluster map，然后获取到通过Hash算法算出的PG ID，最后再根据CRUSH算法计算出PG中，主和次OSD的ID，最终找到OSD的位置，进行数据的读写。

7.层级的Cluster map

在Ceph中，CRUSH是可以支持各种基础设施和用户自定义的；CRUSH设备列表中预先定义了一系列的设备，用户可以通过自定义配置将不同的OSD分配到不同的区域。这样就在物理上避免了所有数据都在同一个机架上，防止某个机架突然塌了以后数据全部丢失。

8.恢复和再平衡

在Ceph集群中，如果OSD挂了，且老师处于degraded状态，Ceph 都会将其标记为 down 和 out 状态；然后默认情况下 Ceph 会等待 300秒之后进行数据恢复和再平衡，这个值可以通过在配置文件中的 mon osd down out interval 参数来调整

总结

基于上述的定义，便可以对寻址流程进行解释了。具体而言，Ceph中的寻址至少要经历三次映射：

1.File -&gt; Object

将用户操作的File，映射为RADOS能处理的object，每个object都会有一个唯一的oid。本质上就是按照object的最大size对file进行切分，切分成大小一致的object(最后的大小可以不一样)

2.Object -&gt; PG

当File被映射为一个或多个object后，就需要将object映射到PG中,得到PG ID。这有一个计算公式

hash(oid) &amp; mask –&gt; pgid

由此可见，其计算由两步组成。首先是使用Ceph指定的静态hash算法计算出oid的hash值。然后将这个随机值和mask(mask的值=PG数-1)按位相与，最终得到PG ID。

3.PG -&gt; OSD

最后根据前面得到的PG ID，通过CRUSH算法最终找到OSD的位置。

Ceph组件调整及操作

1.pool 操作

# 列出池
ceph osd lspools

# 在配置文件中调整默认PG 数量以及副本数
osd pool default size = 5
osd pool default pg num = 100
osd pool default pgp num = 100

# 创建池
ceph osd pool create k8s-pool 30

# 获取存储池选项值
ceph osd pool get k8s-pool pg_num/pgp_num

# 调整副本数
ceph osd pool set k8s-pool size 10

# 获取对象副本数
ceph osd dump | grep 'replicated size'

# 删除池
ceph osd pool delete k8s-pool k8s-pool --yes-i-really-really-mean-it


2.object操作

# 将对象放入到池内
rados put test-object testfile.txt -p cephfs_data

# 列出池中对象
rados ls -p cephfs_data

# 检查池中对象位置
ceph osd map cephfs_data test-object

# 删除对象
rados rm test-object -p cephfs_data


3.PG和PGP操作

预设Ceph集群中的PG数至关重要，公式如下:

PG 总数 = (OSD 数 * 100) / 最大副本数


集群中单个池的PG数计算公式如下：

PG 总数 = (OSD 数 * 100) / 最大副本数 / 池数


PGP是为了实现定位而设计的PG，PGP的值应该和PG数量保持一致；pgp_num 数值才是 CRUSH 算法采用的真实值。虽然 pg_num 的增加引起了PG的分割，但是只有当 pgp_num增加以后，数据才会被迁移到新PG中，这样才会重新开始平衡。

获取PG和PGP的方式如下：

ceph osd pool get cephfs_data pg_num
ceph osd pool get cephfs_data pgp_num


调整方式如下：

ceph osd pool set cephfs_data pgp_num 32
ceph osd pool set cephfs_data pgp_num 32


3.Cluster map 操作

# 查看现有集群布局
# 基本上一台机器上一个osd
$  ceph osd tree
ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.28134 root default                                            
-2 0.04689     host k8s-master01                                   
 0 0.04689         osd.0              up  1.00000          1.00000
-3 0.04689     host k8s-master02                                   
 1 0.04689         osd.1              up  1.00000          1.00000
-4 0.04689     host k8s-master03                                   
 2 0.04689         osd.2              up  1.00000          1.00000
-5 0.04689     host k8s-node01                                     
 3 0.04689         osd.3              up  1.00000          1.00000
-6 0.04689     host k8s-node02                                     
 4 0.04689         osd.4              up  1.00000          1.00000
-7 0.04689     host k8s-registry                                   
 5 0.04689         osd.5              up  1.00000          1.00000


# 添加逻辑上的机架
ceph osd crush add-bucket rack01 rack
ceph osd crush add-bucket rack02 rack
ceph osd crush add-bucket rack03 rack

# 将机器移动到不同的机架上
ceph osd crush move k8s-master01 rack=rack01
ceph osd crush move k8s-master02 rack=rack02
ceph osd crush move k8s-master03 rack=rack03

# 移动每个机架到默认的根下
ceph osd crush move rack01 root=default
ceph osd crush move rack02 root=default
ceph osd crush move rack03 root=default


最终集群整体布局如下，我们可以看到每个机器都被分配到了对应的机架下面，从逻辑上进行了分隔
$ ceph osd tree
ID  WEIGHT  TYPE NAME                 UP/DOWN REWEIGHT PRIMARY-AFFINITY
 -1 0.28134 root default                                                
 -5 0.04689     host k8s-node01                                         
  3 0.04689         osd.3                  up  1.00000          1.00000
 -6 0.04689     host k8s-node02                                         
  4 0.04689         osd.4                  up  1.00000          1.00000
 -7 0.04689     host k8s-registry                                       
  5 0.04689         osd.5                  up  1.00000          1.00000
 -8 0.04689     rack rack01                                             
 -2 0.04689         host k8s-master01                                   
  0 0.04689             osd.0              up  1.00000          1.00000
 -9 0.04689     rack rack02                                             
 -3 0.04689         host k8s-master02                                   
  1 0.04689             osd.1              up  1.00000          1.00000
-10 0.04689     rack rack03                                             
 -4 0.04689         host k8s-master03                                   
  2 0.04689             osd.2              up  1.00000          1.00000


Ceph硬件配置

MON需求

Ceph monitor通过维护整个集群的map从而完成集群的健康处理；但是monitor并不参与实际的数据存储，所以实际上monitor节点CPU占用、内存占用都比较少；一般单核CPU加几个G的内存即可满足需求；虽然monitor节点不参与实际存储工作，但是monitor的网卡至少应该是冗余的，否则一旦网络出现故障则集群健康会难以保证。

OSD需求

OSD作为Ceph的主要存储设备，其会占用一定的CPU和内存资源，一般推荐做法是每个节点的每块硬盘作为一个OSD；同时OSD还需写入日志，所以应当为OSD集成日志留充足的空间；在出现故障时，OSD需求的资源可能会更多，所以OSD节点根据实际情况(每个OSD会有一个线程)应该分配更多的CPU和内存；固态硬盘也会增加OSD存取速度和恢复速度

MDS需求

MDS服务专门为CephFS存储元数据，所以相对于monitor和OSD节点，这个MDS节点的CPU需求会大得多，同时内存占用也是海量的，所以MDS一般会使用一个强劲的物理机单独搭建。
                    </p>
                    <p class="repo-list-meta">
                        <span class="meta-info">
                          <span class="octicon octicon-calendar"></span> 2017/09/12
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#kubernetes" title="kubernetes">kubernetes</a>
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#ceph" title="ceph">ceph</a>
                        </span>
                        
                    </p>
                </li>
                
                <li class="repo-list-item">
                    <h3 class="repo-list-name">
                      <a href="https://kevinguo.me/2017/09/06/kubernetes-ceph-1/">kubernetes ceph 笔记 1</a>
                    </h3>
                    <p class="repo-list-description">
                        该教程主要是为statefulset有状态服务集群提供持久化存储提供基础，在讲statefulset之前，我们先搭建我们的ceph集群
；具备了极好的可靠性、统一性；经过近几年的发展，ceph开辟了一个全新的数据存储途径。ceph具备了企业级存储的分布式、可大规模扩展、没有当节点故障等特点，越来越受人们的青睐。


简介

Ceph是一个符合POSIX、开源的分布式存储系统，不论你是想提供ceph 对象存储或ceph 块设备，还是想部署一个ceph文件系统或者把ceph作为他用，所有 Ceph存储集群 的部署都始于部署一个个 ceph节点、网络和ceph存储集群，ceph 存储集群至少需要一个 ceph monitor和两个osd守护进程。而运行ceph文件系统客户端，则必须需要MDS(元数据服务器)。

基础组件:


  
    Ceph OSDs: Ceph OSD 守护进程的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当Ceph 存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能到到active+clean状态(Ceph 默认有3个副本，你可以调整osd poll default size)
  
  
    Ceph Monitors: Ceph Monitor基于PAXOS算法维护着 展示集群状态的各种图表，包括监视器图、OSD图、PG图、CRUSH图。Ceph 保存着发生在Monitors、OSD和PG上的每一次状态变更的历史信息(称为 epoch)
  
  
    MDSs：Ceph MDS为 Ceph 文件系统存储元数据(也就是说，Ceph块设备和Ceph对象存储是不是用MDS)。MDS使得POSIX文件系统的用户们，可以在部队Ceph存储集群造成负担的前提下，执行诸如 ls、find等基本命令
  


下图展示了Ceph的基础架构图



1.基础存储系统RADOS

RADOS (Reliable Autonomic Distributed Object Store),这一层本身就是一个完整的对象存储系统，包括Ceph的基础服务(OSD,MON,MDS)，所有存储在ceph中的用户数据实际上最终都是由这一层来存储的。而Ceph的高可用，高扩展性，高自动化等特性本质上也是有这一层所提供的，因此，RADOS是ceph的核心精华部分。

2.基础库LibRados

这一层是对RADOS进行抽象和封装，并向上层提供不同的API，这样上层的RBD、RGW、CephFS才能访问RADOS，RADOS所提供的原生librados API包括C和C++两种。

3.高层存储应用接口

这一层包含了RGW、RBD和CephFS这几个部分，其作用是在librados库的基础上提供抽象层次更高，更便于应用和用户端使用的上层接口。


  
    RGW: 是一个提供与S3和Swift兼容的RESTful API的gateway，以供对应的对象存储应用开发使用。通过RGW可以将RADOS响应转化为HTTP响应，同样也可以将外部的HTTP响应状花为RADOS调用。
  
  
    RBD：提供一个标准的块设备接口服务。
  
  
    CephFS: 提供一个POSIX兼容的分布式文件系统。
  


4.client层

这一层其实就是不同场景下对于ceph各个应用接口的各种应用方式，例如基于librados直接开发的对象存储应用，基于RGW开发的对象存储应用，基于RBD实现的云硬盘等等。

5.其他

一个Cluster逻辑上可以划分为多个Pool，一个Pool由若干个逻辑PG组成。

一个文件会被切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD，其中第一个OSD（Primary OSD）为主，其余是备，OSD间通过心跳来互相监控存活状态。

CRUSH： CRUSH是ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。

一.快速安装

1.1 安装前准备

所需机器如下


  
    
      IP
      HostName
      OS
      role
    
  
  
    
      172.30.33.31
      deploy-node
      centos7.3.1611
      deploy node
    
    
      172.30.33.90
      k8s-master01
      centos7.3.1611
      monitor osd node1
    
    
      172.30.33.91
      k8s-master02
      centos7.3.1611
      monitor osd node2
    
    
      172.30.33.92
      k8s-master03
      centos7.3.1611
      monitor osd node3
    
    
      172.30.33.89
      k8s-registry
      centos7.3.1611
      osd mds
    
    
      172.30.33.93
      k8s-node01
      centos7.3.1611
      osd mds
    
    
      172.30.33.94
      k8s-node02
      centos7.3.1611
      osd mds
    
  


在管理节点上操作
1.添加ceph源

[ceph-noarch]
name=Ceph noarch packages
baseurl=https://download.ceph.com/rpm-jewel/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

$ yum update -y &amp;&amp; yum install ceph-deploy -y


2.更新并安装ceph-deploy

$ sudo yum update &amp;&amp; sudo yum install ceph-deploy


2.配置从部署机器到所有其他节点的免密钥登录，具体参考这里

在节点上操作

1.安装epel源

$ yum install yum-plugin-priorities
$ yum install epel-release -y


2.校对时间，由于ceph使用Paxos算法保证数据一致性，所以安装前要先保证各个节点的时间同步

$ sudo yum install ntp ntpdate ntp-doc

$ ntpdate 0.cn.pool.ntp.org


3.开放所需端口或关闭防火墙

$ system stop firewalld
$ sudo firewall-cmd --zone=public --add-port=6789/tcp --permanent


4.关闭selinux

$ sudo setenforce 0


1.2 创建集群

1.由于ceph-deploy工具部署集群前需要创建一些集群配置信息，其保存在ceph.conf文件中，这个文件将来会被复制到每个节点的 /etc/ceph/ceph.conf

# 创建集群配置目录
mkdir ceph-cluster &amp;&amp; cd ceph-cluster
# 创建 monitor-node
ceph-deploy new k8s-master01
# 追加 OSD 副本数量(测试虚拟机总共有3台)
echo "osd pool default size = 3" &gt;&gt; ceph.conf


2.创建集群使用 ceph-deploy工具在部署节点上执行即可

# 安装ceph
$ ceph-deploy install k8s-master01 k8s-master02 k8s-master03

注意：在部署节点部署的时候，可能会因为网络原因导致无法安装ceph和ceph-radosgw，这时候，我们在各个节点上手动安装一下
# 添加ceph 源
[Ceph]
name=Ceph packages for $basearch
baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[Ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-jewel/el7/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1

[ceph-source]
name=Ceph source packages
baseurl=http://download.ceph.com/rpm-jewel/el7/SRPMS
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc
priority=1


$ yum install ceph ceph-radosgw -y


3.初始化monitor node 和密钥文件

$ ceph-deploy mon create-initial


4.在osd节点上创建一个目录作为 osd 存储，并修改其权限,千万别创建在/usr/local目录下，否则，你的osd可能会无法挂载

$ ssh k8s-master01
$ sudo mkdir /data/osd1
$ chown -R ceph:ceph osd1
$ exit

$ ssh k8s-master2
$ sudo mkdir /data/osd2
$ chown -R ceph:ceph osd2
$ exit

$ ssh k8s-master3
$ sudo mkdir /data/osd3
$ chown -R ceph:ceph osd3
$ exit


5.然后，在管理节点上初始化 osd

$ ceph-deploy osd prepare k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd3


6.最后，在管理节点上激活 osd

$ ceph-deploy osd activate k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd3


7.在管理节点上部署 ceph cli 工具和密钥文件

$ ceph-deploy admin k8s-master01 k8s-master02 k8s-master03


8.确保你对 ceph.client.admin.keyring有正确的操作权限，在每个节点上执行

$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring


9.最后，检查集群的健康状态

$ ceph health
HEALTH_OK

$ ceph osd tree
ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.14067 root default                                            
-2 0.04689     host k8s-master01                                   
 0 0.04689         osd.0              up  1.00000          1.00000
-3 0.04689     host k8s-master02                                   
 1 0.04689         osd.1              up  1.00000          1.00000
-4 0.04689     host k8s-master03                                   
 2 0.04689         osd.2              up  1.00000          1.00000


我们看到状态是OK的，而且每个节点上的osd的状态都是up的

如果在某些地方碰到麻烦，想从头再来，可以用下列命令来清楚配置：

$ ceph-deploy purgedata {ceph-node} [{ceph-node}]
$ ceph-deploy forgetkeys


用下列命令可以连Ceph安装包一起清除：

$ ceph-deploy purge {ceph-node} [{ceph-node}]


二.操作集群

2.1 基础操作

# 创建MDS
$ ceph-deploy mds create {ceph-node}

# 创建RGW
$ ceph-deploy rgw create {ceph-node}

# 添加mon
$ echo "public network = 172.30.33.0/24" &gt;&gt; ceph.conf
$ ceph-deploy mon add {ceph-node}

# 查看仲裁
$ ceph quorum_status --format json-pretty


2.2 对象存储测试

# 创建pool
# rados mkpool {pool-name}
$ rados mkpool test-pool

# 创建测试文件
$ dd if=/dev/zero of=testfile bs=1G count=1

# 创建一个对象(这时候也将对象放入了pool中)
# rados put {object-name} {file-path} --pool={pool-name}
$ rados put test-object testfile --pool=test-pool

# 检查存储池，确认ceph存储了此对象
$ rados -p test-pool ls

# 定位对象，会输出对象位置
$ ceph osd map test-pool test-object
osdmap e42 pool 'test-pool' (3) object 'test-file' -&gt; pg 3.b79653d4 (3.4) -&gt; up ([1,5,2,3,4], p1) acting ([1,5,2,3,4], p1)

# 删除对象
$ rados -p data rm test-object

# 删除存储池
$ rados rmpool test-pool test-pool --yes-i-really-really-mean-it

随着集群的运行，对象的位置可能会动态改变。Ceph有动态均衡机制，无需手动干预即可完成。

2.3 块存储测试

官方建议使用RBD的客户端最好不要和OSD在同一台物理机上(除非它们都是VM)

1.确认你使用了合适的内核版本，详情参见

lab_release -a
uname -a


2.在管理节点上用 ceph-deploy安装ceph

ceph-deploy install {rbd-client}


3.在管理节点上部署ceph cli工具和密钥

ceph-deploy admin {rbd-client}


4.在rbd节点上创建块设备image

rbd create test-block --size 4096


5.映射image到块设备

rbd map test-block --name client.admin


在上面的map映射操作时，会出现如下报错

$ rbd map test-block --name client.admin
rbd: sysfs write failed
RBD image feature set mismatch. You can disable features unsupported by the kernel with "rbd feature disable".
In some cases useful info is found in syslog - try "dmesg | tail" or so.
rbd: map failed: (6) No such device or address


大致意思是说features不匹配，可以通过disable features关掉一些特性来让内核支持。这是因为在Ceph高本本进行 map image时，默认ceph在创建image(上文test-block)时，会增加很多features，这些features需要内核支持，centos7上的支持有限，所以，我们需要关掉一些

我们可以用 rbd info data 看看创建的image目前有哪些features

$ rbd info test-block
rbd image 'test-block':
	size 4096 MB in 1024 objects
	order 22 (4096 kB objects)
	block_name_prefix: rbd_data.10bb238e1f29
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	flags:


在features中，我们可以看到默认开启了很多：


  layering 支持分层
  exclusive-lock 支持独占锁
  object-map 支持对象映射(依赖exclusive-lock)
  fast-diff 快速计算差异(依赖object-map)
  deep-flatten 支持快照扁平化操作


而实际上在CentOS7的3.10内核中只支持layering，所以我们需要手动关闭一些features，然后重新map；如果想要一劳永逸，可以在 ceph.conf 中加入 rbd_default_features = 1 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)

6.关闭不支持的特性之后重新map

# 关闭不支持的features
$ rbd feature disable test-block exclusive-lock, object-map, fast-diff, deep-flatten

# 重新map
$ rbd map test-block --name client.admin
/dev/rbd0


7.格式化之后挂载到系统目录

# 格式化
$ mkfs.xfs /dev/rbd0
meta-data=/dev/rbd0              isize=512    agcount=9, agsize=130048 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=0, sparse=0
data     =                       bsize=4096   blocks=1048576, imaxpct=25
         =                       sunit=1024   swidth=1024 blks
naming   =version 2              bsize=4096   ascii-ci=0 ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=512   sunit=8 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

# 挂载
$ mkdir test-block
$ mount /dev/rbd0 test-block

# 写入测试
$ dd if=/dev/zero of=test-block/test-file bs=1G count=1
1+0 records in
1+0 records out
1073741824 bytes (1.1 GB) copied, 2.96071 s, 363 MB/s

$ ls test-block/
test-file


2.4 CephFS 测试

1.创建MDS

$ ceph-deploy mds create k8s-node01 k8s-node02 k8s-registry


2.创建pool和fs，创建pool需要指定PG数量

ceph osd pool create cephfs_data 32
ceph osd pool create cephfs_metadata 32
ceph fs new test-fs cephfs_metadata cephfs_data


PG 概念：


  当Ceph集群接受到存储请求时，ceph会将一个文件会切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD(根据副本数)；一般来说增加PG的数量能降低OSD负载，一般每个OSD大约分配50～100PG，关于PG数量指定，一般遵循以下公式
  
    集群PG总数 = (OSD总数 * 100)/数据最大副本数
    单个存储池PG数 = (OSD总数 * 100)/数据最大副本数/存储池数
  


注意：PG的最终结果应当以最接近以上计算公式的2的N次幂(向上取值)；如我的虚拟机环境的每个存储池 PG数 = 6(OSD) * 100 / 5(副本数) / 4（4个存储池）= 30，向上取2的N次幂为32(即，2的5次方=32，最接近30)

3.挂载CephFS有两种方式，一种是使用内核驱动挂载，一种是使用 ceph-fuse用户空间挂载

内核挂载需要提取ceph管理key，方式如下：

在密钥文件中找到与某用户对于的密钥

$ cat /etc/ceph/ceph.client.admin.keyring
[client.admin]
	key = AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA==


复制密钥到文件中保存，并确保其权限

echo "AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA==" &gt; ceph-key


创建目录挂载

mkdir test-fs
mount -t ceph 172.30.33.90:6789:/ /root/test-fs -o name=admin,secretfile=ceph-key

#写入数据测试
$ dd if=/dev/zero of=test-fs/test-fs bs=1G count=1
1+0 records in
1+0 records out
1073741824 bytes (1.1 GB) copied, 2.77355 s, 387 MB/s


ceph-fuse用户空间挂载的方式也比较简单，需要先安装ceph-fuse，同时也需要key

# 按照前面的步骤添加ceph源
$ vi /etc/yum.repos.d/ceph.repo
[ceph]
name=ceph
baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/
gpgcheck=0
[ceph-noarch]
name=cephnoarch
baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/
gpgcheck=0

# 安装ceph-fuse
yum install ceph-fuse -y

# 复制配置和ceph key到client端
sudo mkdir -p /etc/ceph
sudo scp root@172.30.33.91:/etc/ceph/ceph.conf /etc/ceph/ceph.conf
sudo scp root@172.30.33.91:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring

# 创建目录挂载
mkdir test-fs-fuse
$ sudo ceph-fuse -m 172.30.33.91:6789 test-fs-fuse
ceph-fuse[60551]: starting ceph client
2017-09-12 14:47:24.137929 7f63d9719ec0 -1 init, newargv = 0x7f63e51d4840 newargc=11
ceph-fuse[60551]: starting fuse

# 写入数据测试
$ sudo dd if=/dev/zero of=test-fs-fuse/test-fs-fuse bs=1G count=1
1+0 records in
1+0 records out
1073741824 bytes (1.1 GB) copied, 10.5426 s, 102 MB/s

# 查看确认，发现我们上面通过内核挂载的文件也还在
$ ls -lh
total 2.0G
-rw-r--r-- 1 root root 1.0G Sep 12 13:59 test-fs
-rw-r--r-- 1 root root 1.0G Sep 12 14:49 test-fs-fuse


2.5 Ceph对象网关

1.对象网关创建

# 创建RGW
$ ceph-deploy rgw create k8s-node02


2.直接访问http://ceph-node-ip:7480返回结果如下

&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;
&lt;Owner&gt;
&lt;ID&gt;anonymous&lt;/ID&gt;
&lt;DisplayName/&gt;
&lt;/Owner&gt;
&lt;Buckets/&gt;
&lt;/ListAllMyBucketsResult&gt;


这就说明网关OK了，但是因为没有读写环境，所以暂时测不了。

本文参考了ceph官方文档及漠然的ceph笔记(一)部分
                    </p>
                    <p class="repo-list-meta">
                        <span class="meta-info">
                          <span class="octicon octicon-calendar"></span> 2017/09/06
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#kubernetes" title="kubernetes">kubernetes</a>
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#ceph" title="ceph">ceph</a>
                        </span>
                        
                    </p>
                </li>
                
                <li class="repo-list-item">
                    <h3 class="repo-list-name">
                      <a href="https://kevinguo.me/2017/09/01/kubernetes-one-section/">kubernetes 入门</a>
                    </h3>
                    <p class="repo-list-description">
                        入门概念

为什么要使用kubernetes

1.新技术

2.精简.只需要一个架构师专注于“服务组件”的提炼，几名开发工程师专注于代码开发，几名系统运维工程师负责kubernetes的部署和运维

3.kubernetes使用微服务架构

4.更方便迁移

5.超强的横向扩容能力

master


  
    kube-apiserver
提供HTTP RESET接口的关键服务进程，是kubernetes所有资源增删改查的唯一入口，也是集群控制的入口进程
  
  
    kube-controller-manager
kubernetes里所有资源对象的自动化控制中心。
  
  
    kube-scheduler
kubernetes里所有资源的调度中心
  
  
    kube-proxy
实现kubernetes service的通信与负载均衡机制的重要组件
  
  
    kubelet
负责pod对应容器的创建，启停等任务，同时与Master节点密切协作，实现集群管理的基本功能
  
  
    etcd
保存kubernetes里所有数据的存储
  
  
    docker
运行kubernetes 里面的容器
  


node


  
    kube-proxy
实现kubernetes service的通信与负载均衡机制的重要组件
  
  
    kubelet
负责pod对应容器的创建，启停等任务，同时与Master节点密切协作，实现集群管理的基本功能
  
  
    docker
运行kubernetes 里面的容器
  


pod

一个pause容器和一组业务容器组成，是kubernetes里面最基本的单元。

pause 容器


  既然pod是一组容器组成，那么如何来判断这个pod的状态呢，是其中一个容器死亡了，就算整个pod死亡了，还是说按照某种N/M的死亡率来算呢？kubernetes里面引入了一个不易死亡又和业务无关的pause容器，以它的状态来表示整个pod的状态。
  pod里面的多个容器共享pause容器里面的网络和volumes。


普通pod和静态pod

pod有两种类型：普通pod和静态pod

  普通pod即是那些通过deployment，replicationcontroller，daemonset等部署的，这些pod一旦创建会被放入到etcd中，然后会被kubernetes master调度到某个node上，通过node上的kubelet进程实例化成一组相关的容器并启动起来。
  静态pod是通过放在某个node上的一个具体的文件运行起来的。比如我们放在/etc/kubernetes/manifests下的某些静态文件。


endpoints

kubernetes中，每个pod都有一个属于他的IP。Pod IP + 需要暴露出来的ContainerPort 就组成了endpoint
说到service的endpoint，这里就不得不说下targetPort，targetPort属性用来确定提供该服务的容器所暴露的端口号，如果你不指定targetPort，kubernetes默认使用你提供的port为targetPort

所以他们的关系应该是如下：

service 暴露一个提供服务的端口—&gt;{pod IP+targetPort}(容器内部暴露出来提供服务的端口号)
                                         ||
                                     {endpoint}
pod volume

pod中的volume能够被pod中的多个容器访问。kubernetes中的volume与pod的生命周期相同，但与容器的生命周期不相关。
通常是声明一个volume，然后在容器中引用这个volume并mount到容器的某个目录上。
常用的类型有：

  emptyDir
  hostPath
  Persistent Volume(GCE Persistent Disks、NFS、RBD、ISCSCI、AWS ElasticBlockStore、GlusterFS)，这就设计到分布式存储和外部存储的一些操作了，后续再讲吧


apiVersion: v1
kind: ReplicationController
metadata:
  name: myweb
spec:
  replicas: 5
  # selector 如果不指定，默认和.spec.template.labels的值相同
  selector:
    app: myweb
  template:
    metadata:
      labels:
        app: myweb
    spec:
      # 该处声明一个volume
      volumes:
      - name: datavol
        emptyDir: {}
      containers:
      - name: myweb
        image: kubeguide/tomcat-app:v1
        ports:
        - containerPort: 8080
        env:
        - name: MYSQL_SERVICE_HOST
          value: 'mysql-service'
        - name: MYSQL_SERVICE_PORT
          value: '3306'
        # 该处进行引用并挂在到容器内部
        volumeMounts:
        - name: datavol
          mountPath: /mydata-data


pod 资源限制

每个pod都能对其能使用的服务器上的资源来进行配额限制，当前能限制的只有CPU和Memory。在kubernetes里面，计算资源的限制主要是设定两个参数

  Limits 资源允许使用的最大值，不能超过
  Requests 资源允许使用的最小值，最少必须满足这个需求


我们通常将request 设置为容器平时正常运行时所需的资源，而将Limits设置为容器峰值负载情况下的最大使用量

label and label selector

label即标签，是Kubernetes系统中另一个核心概念。一个label是一个key=value的键值对。label可以附加到各种资源对象上，如Node、Pod、RC、Service等，一个资源对象可以附加无数的label，同一个label也可以附加到无数的资源对象上。

通过指定label selector来查询和筛选某些拥有label的资源对象，常用到的两种表达式

  等式 (key=value)
  集合式 (key in values)


label selector在kubernetes中常用的场景如下：

  kub-controller-manager通过在RC上定义的label selector来监控并控制POD的数量
  kube-proxy 通过service上的label selector来选择对应的pod，自动建立每个service到pod的路由请求，从而实现service的智能负载均衡
  通过NodeSelector，将某些pod调度到指定的node上


注意：我们在指定label selector时，需要和.spec.template.metadata.labels下的值相同，如果不指定label selector则默认保持和.spec.template.metadata.labels的值相同

init容器

init容器在1.6版本已经推出了beta版本，但是以前的语法仍然被保留，未来可能会被抛弃掉，所以建议使用最新的语法。

init容器主要作用是为了执行一些在pod就绪前的一些初始化步骤，比如：


  把一些安全级别的程序放在init container中执行，避免放在images中暴露
  它们必须在应用程序启动之前完成，并且只有在init容器成功完成后，应用容器才能启动，这就提供了一种简单的阻塞或延迟应用容器启动的方法


1.6之后的写法如下：

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  # init 容器
  initContainers:
    - name: init-myservice
      image: busybox
      command: ['sh','-c','until nslookup myservice;do echo waiting for myservice; sleep 2;done;']
  # 业务容器
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh','-c','echo The app is running! &amp;&amp; sleep 3600']


ReplicationController(RC)
我们将上面那个例子的yaml文件直接拿来解析
# api版本，类型，全局唯一名称，这是所有kubernetes yaml文件都需要的
apiVersion: v1
kind: ReplicationController
metadata:
  name: myweb
# 定义pod期望数量
spec:
  replicas: 5
  # 用于筛选目标的selector 如果不指定，默认和.spec.template.labels的值相同
  selector:
    app: myweb
  # 当Pod副本数和期望数不一致时，用于创建新pod的模板
  template:
    metadata:
      labels:
        app: myweb
    spec:
      # 该处声明一个volume
      volumes:
      - name: datavol
        emptyDir: {}
      containers:
      - name: myweb
        image: kubeguide/tomcat-app:v1
        ports:
        - containerPort: 8080
        env:
        - name: MYSQL_SERVICE_HOST
          value: 'mysql-service'
        - name: MYSQL_SERVICE_PORT
          value: '3306'
        # 该处进行引用并挂在到容器内部
        volumeMounts:
        - name: datavol
          mountPath: /mydata-data


我们可以通过命令的形式来动态缩放POD的数量
kubectl scale rc mysql --replicas=2

# 删除pod
kubectl delete -f mysql-rc.yml
kubectl scale rc mysql --replicas=0


ReplicaSet

ReplicaSet和ReplicationController唯一的区别就是:ReplicaSet支持集合式的label selector，我们平时很少单独使用Replica Set，它主要是被Deployment这个更高层的资源对象使用。

Deployment
Deployment在内部使用Replica Set来实现目的，它管理着Replica Set，而它管理Replica Set的主要目的是为了支持版本回滚Rollback

从下面命令所展示出来的命名规则我们不难发现，Deployment创建的时候创建了Replica Set，而Replica Set创建了Pod
$ kubectl get deployment -l k8s-app=kube-dns -n kube-system
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-dns   2         2         2            2           6d

$ kubectl get replicaset -l k8s-app=kube-dns -n kube-system
NAME                  DESIRED   CURRENT   READY     AGE
kube-dns-1446441763   2         2         2         6d

$ kubectl get pods -l k8s-app=kube-dns -n kube-system
NAME                        READY     STATUS    RESTARTS   AGE
kube-dns-1446441763-0th37   3/3       Running   0          6d
kube-dns-1446441763-1w5gx   3/3       Running   0          6d


稍后，用实验来说明Deployment在滚动升级中的作用

Pod滚动升级

在说Deployment的滚动升级之前，我们先来看看ReplicationController的滚动升级

ReplicationController的滚动升级和Deployment的滚动升级有所不同，命令都不一样，通过执行kubectl rolling-update来一键完成，该命令创建了一个新的RC，然后自动控制旧的RC中的POD副本数两逐渐减少到0，同时新RC中的POD数量从0逐步增加到目标值，最终实现POD的升级，滚动升级的配置文件必须满足如下三个条件：

  metadata.name必须和旧RC文件中的不同
  spec.selector至少有一个与旧RC的不同(这里有个BUG，具体参考这里)
  metadata.namespace 命名空间必须的是一样的


下面我们将上面的myweb进行一下升级，内容如下
apiVersion: v1
kind: ReplicationController
metadata:
  name: myweb-v2
spec:
  replicas: 5
  selector:
    version: v2
    deployment: v2
    app: myweb
  template:
    metadata:
      labels:
        version: v2
        deployment: v2
        app: myweb
    spec:
      # 该处声明一个volume
      volumes:
      - name: datavol
        emptyDir: {}
      containers:
      - name: myweb
        image: kubeguide/tomcat-app:v2
        ports:
        - containerPort: 8080
        env:
        - name: MYSQL_SERVICE_HOST
          value: 'mysql-service'
        - name: MYSQL_SERVICE_PORT
          value: '13306'
        # 该处进行引用并挂在到容器内部
        volumeMounts:
        - name: datavol
          mountPath: /mydata-data


啊哦，升级的时候报错了
error: myweb-rc-update.yml must specify a matching key with non-equal value in Selector for myweb
See 'kubectl rolling-update -h' for help and examples.


没关系，既然用yaml文件无法升级，我们用命令的形式来试试
kubectl rolling-update myweb --image=kubeguide/tomcat-app:v2


OK，成了，开始升级了

# 我们发现旧的rc正在逐步减少，新的rc正在增多
$ kubectl get rc
NAME                                     DESIRED   CURRENT   READY     AGE
mysql                                    1         1         1         34m
myweb                                    2         2         2         19m
myweb-c1dc64330c885b62eca9fb5aafbfecc6   4         4         4         3m

# 旧的pods也正在一个个的被新的pod替换
$ kubectl get pods
NAME                                           READY     STATUS    RESTARTS   AGE
mysql-65lw6                                    1/1       Running   0          35m
myweb-c1dc64330c885b62eca9fb5aafbfecc6-88f8v   1/1       Running   0          4m
myweb-c1dc64330c885b62eca9fb5aafbfecc6-c97fh   1/1       Running   0          58s
myweb-c1dc64330c885b62eca9fb5aafbfecc6-ttlpm   1/1       Running   0          2m
myweb-c1dc64330c885b62eca9fb5aafbfecc6-wd6kj   1/1       Running   0          3m
myweb-c1dc64330c885b62eca9fb5aafbfecc6-zr57k   1/1       Running   0          1m
myweb-k9tsj                                    1/1       Running   0          20m



成功了～

# 成功后，将RC名称改为myweb，升级完成
Created myweb-c1dc64330c885b62eca9fb5aafbfecc6
Scaling up myweb-c1dc64330c885b62eca9fb5aafbfecc6 from 0 to 5, scaling down myweb from 5 to 0
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 1
Scaling myweb down to 4
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 2
Scaling myweb down to 3
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 3
Scaling myweb down to 2
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 4
Scaling myweb down to 1
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 5
Scaling myweb down to 0
Update succeeded. Deleting old controller: myweb
Renaming myweb-c1dc64330c885b62eca9fb5aafbfecc6 to myweb
replicationcontroller "myweb" rolling updated


那么为什么，我们在上面用yaml文件无法升级呢？

经过一段时间的折腾，我发现，我旧的RC文件只有一个label，app=myweb;而当我用命令升级成功后,新的RC有了两个label，app=myweb和deployment=xxxxx
# 未升级之前的RC
$ kubectl get rc -o wide
NAME      DESIRED   CURRENT   READY     AGE       CONTAINER(S)   IMAGE(S)                  SELECTOR
mysql     1         1         1         48m       mysql          mysql                     app=mysql
myweb     5         5         5         8s        myweb          kubeguide/tomcat-app:v1   app=myweb

# 升级后的RC
$ kubectl get rc -o wide
NAME       DESIRED   CURRENT   READY     AGE       CONTAINER(S)   IMAGE(S)                  SELECTOR
mysql      1         1         1         43m       mysql          mysql                     app=mysql
myweb      4         4         4         6m        myweb          kubeguide/tomcat-app:v2   app=myweb,deployment=c1dc64330c885b62eca9fb5aafbfecc6


然后当我再次对这个新的RC进行升级的时候，我发现是可以用yaml文件升级的，这说明什么？是不是旧的RC文件至少需要两个label才能用yaml文件升级呢？我们试试再说


  为myweb添加不少于一个的label
    # api版本，类型，全局唯一名称，这是所有kubernetes yaml文件都需要的
apiVersion: v1
kind: ReplicationController
metadata:
name: myweb
# 定义pod期望数量
spec:
replicas: 5
# 用于筛选目标的selector 如果不指定，默认和.spec.template.labels的值相同
selector:
  app: myweb
  version: v1
# 当Pod副本数和期望数不一致时，用于创建新pod的模板
template:
  metadata:
    labels:
      app: myweb
      version: v1
  spec:
    # 该处声明一个volume
    volumes:
    - name: datavol
      emptyDir: {}
    containers:
    - name: myweb
      image: kubeguide/tomcat-app:v1
      ports:
      - containerPort: 8080
      env:
      - name: MYSQL_SERVICE_HOST
        value: 'mysql-service'
      - name: MYSQL_SERVICE_PORT
        value: '3306'
      # 该处进行引用并挂在到容器内部
      volumeMounts:
      - name: datavol
        mountPath: /mydata-data
    
  
  新建一个用来升级myweb的yaml文件


apiVersion: v1
kind: ReplicationController
metadata:
  name: myweb-v2
spec:
  replicas: 5
  selector:
    version: v2
    deployment: v2
    app: myweb
  template:
    metadata:
      labels:
        version: v2
        deployment: v2
        app: myweb
    spec:
      # 该处声明一个volume
      volumes:
      - name: datavol
        emptyDir: {}
      containers:
      - name: myweb
        image: kubeguide/tomcat-app:v2
        ports:
        - containerPort: 8080
        env:
        - name: MYSQL_SERVICE_HOST
          value: 'mysql-service'
        - name: MYSQL_SERVICE_PORT
          value: '13306'
        # 该处进行引用并挂在到容器内部
        volumeMounts:
        - name: datavol
          mountPath: /mydata-data


我发现，真的可以升级了!!!
$ kubectl rolling-update myweb -f myweb-rc-update.yml
Created myweb-v2
Scaling up myweb-v2 from 0 to 5, scaling down myweb from 5 to 0
Scaling myweb-v2 up to 1
Scaling myweb down to 4
Scaling myweb-v2 up to 2
Scaling myweb down to 3
Scaling myweb-v2 up to 3
Scaling myweb down to 2
Scaling myweb-v2 up to 4


这说明什么？这说明，以后如果你要用ReplicationController的时候，至少要给他指定不少于2个的label，否则，你无法用yaml来进行升级，这真的是很蛋疼的一件事。

当然了，保不齐以后kubernetes会将ReplicationController抛弃掉，毕竟现在它已经有了更好的Deployment和StatefulSet等…

Horizontal Pod Autoscaler (HPA)

前面我们说到能用kubectl scale命令来手动实现 Pod 的扩容和缩容，这未免也太 low 了吧，我们既然用了kubernetes，那就是因为他的智能化，自动化。所以这里我们要讲讲kubernetes的智能扩容 HPA。

HPA 基于获取到的metrics value(CPU utilization,custom metrics),对RC,Deployment管理的pods进行自动伸缩。HPA是kubernetes autoscaling API组中的一个API资源，当前的stable版本只支持CPU，alpha版本中红，已经开始支持memory和custom metrics。

HPA 以kubernetes API resource 和一个controller来实现，resource决定了controller的行为，而controller控制着pods的数量。


  截至到kubernetes 1.6 ，Release特性中仅支持CPU utilization这一 resource metrics, 对custom metrics的支持目前仍在alpha阶段。


HPA controller 周期性的调整对应rc，deployment中的pods数量，使得获取到的metrics value能匹配用户指定的target utilization。这个周期默认为30s，可以通过kube-controller-manager的flag --horizontal-pod-autoscaler-sync-period进行设置。

在每个HPA Controller的处理周期中，kube-controller-manager都去查询HPA获取到的metrics的utilization。查询方式根据metric类型不同而不同：

如果metric type是resource metrics，则通过resource metrics API查询，直接通过Heapster访问
如果metric type属于custom metrics，则通过custom metrics API查询，通过REST client来访问

计算伸缩比例算法：


  对于 resource metrics,比如 CPU，HPA controller 从resource metrics API中获取CPU metrics，如果HPA中设定了target utilization，则HPA controller 会将获取到的CPU metrics 除以对应容器的resource request值作为检测到的当前pod的resource utilization。如此计算完所有HPA对应的pods后，对该resource utilization values取平均值。最后将平均值除与定义的target utilization，得到伸缩比例。



  注意：如果HPA对应的某些pods中的容器没有定义resource request，则HPA不会对这些pods进行scale



  
    对于custome metrics，HPA Controller的伸缩算法几乎与resource metrics一样，不同的是：此时是根据custome metrics API查询到的metrics value对比target metrics value计算得到的，而不是通过utilization计算得到的
  
  
    对于object metrics，HPA Controller获取到一个metric 值，然后与target metrics比较，得到如上所说的比率
  


HPA可以通过命令来实现，也可以通过配置文件的方式来实现。

$ kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
deployment "php-apache" autoscaled


下面我们用实验来感受下HPA

首先我们来新建一个php-apache的服务

php-apache-deploy.yml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: php-apache
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-quark: php-apache
  template:
    metadata:
      labels:
        k8s-quark: php-apache
    spec:
      containers:
      - name: php-apache
        image: gcr.io/google_containers/hpa-example:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
        resources:
          requests:
            cpu: 200m


php-apache-svc.yml
apiVersion: v1
kind: Service
metadata:
  name: php-apache
spec:
  ports:
  - port: 80
  selector:
    k8s-quark: php-apache


kubectl create -f php-apache-deploy.yml
kubectl create -f php-apache-svc.yml

# 查看当前的deployment
$ kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   1         1         1            1           10m

# 查看php-apache的pods
$ kubectl get pods |grep php-apache
php-apache-3548797493-twq7k       1/1       Running   0          17m


然后，我们用命令来创建一个HPA
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10

# 看看我们创建好的hpa
$  kubectl get hpa
NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0% / 50%   1         10        1          19m


这条命令的意思是，我们为php-apache创建了一个HPA，指定了target metrics value是cpu利用率50%，而伸缩最小值为1，最大值为10

最后，我们来持续访问php-apache来给它压力，看看HPA会不会自动为我们扩容呢？

# 进入一个容器
kubectl run -ti load-generator --image=busybox /bin/sh

# 持续访问
while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done


过了几分钟，我们看看结果咋样

# HPA状态
$  kubectl get hpa
NAME         REFERENCE               TARGETS      MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   313% / 50%   1         10        4          22m

# deployment状态
$  kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   4         4         4            4           25m

# HPA状态
$  kubectl get hpa
NAME         REFERENCE               TARGETS     MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   90% / 50%   1         10        8          26m



我们发现，随着我们的持续访问增压，HPA会自动的为我们将php-apache进行扩容，随着php-apahce的扩容，CPU开始慢慢下降，直到最终符合我们指定的低于50%的标准，或者达到最大值10个POD。而当我们停止对php-apache的访问，最终，HPA会恢复到默认1个pod的状态。

用yaml文件的方式，最终的效果和上面是一样的
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
  namespace: default
spec:
  # 指定针对谁来使用HPA
  scaleTargetRef:
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  # 用户定义的CPU利用率
  targetCPUUtilizationPercentage: 50


至于其它的HPA metrica，改天再讲吧，毕竟现在还只是alpha版本，而且需要heapster目前也无法收集那么多metrics

Service



透过上图，我们可以发现，service定义了一个服务的访问入口地址，前端的pod通过这个入口地址来访问其背后一组由pod组成的集群实例，而背后这组pod则是通过RC来生成并保持住的。他们三者之间，通过label selector来保持关联。

我们知道，正常情况下，要通过一个如果访问后端的集群服务，最好的办法是在前端弄一个负载均衡(nginx,haproxy…)，暴露一个对外服务的端口，然后反代到后端的ip+port。而kubernetes也遵循了这样的做法。
在上图中，frontal pod 访问 service 时，kubernetes其实是通过其内部的 kube-proxy 来进行负载均衡，然后将请求转发到后端的某个pod上。但kubernetes不是使用的一个实际的负载均衡IP地址，而是为每个service分配了一个全局唯一的虚拟IP地址，这个虚拟IP被称为Cluster IP，只能在kubernetes 集群内部被访问(意思就是只能在集群内的pod中才能访问)，而且一旦创建，在service的整个生命周期内，都不会发生变动。

下面我们来创建一个service看看
apiVersion: v1
kind: Service
metadata:
  # service全局唯一名称，后面在cluster中可以直接使用的名称
  name: tomcat-service
spec:
  ports:
    # service提供服务的端口
  - port: 8080
    name: service-port
  # 后端容器提供服务暴露的端口，如果不指定，默认暴露service提供服务的端口
    targetPort: 8080
  - port: 8005
    name: shutown-port
    targetPort: 8005
  # 指定label selector 确认该服务和后端那些pod关联起来
  selector:
    app: mysql


kubernetes服务发现机制

最开始的时候kubernetes使用变量的形式，来发现服务，后来使用内部DNS来进行服务名解析，还有人认为使用consul的服务发现机制，其实我并不觉得这比内部的DNS服务发现好多少。

kubernetes内部dns寻址服务发现

1.kubectl 执行创建的时候会向APIServer请求创建一个service。APIServer获取到请求后调用相应的api创建一个service对象，并写入etcd保存。
2.kube-dns通过list/watch操作向APIServer发送GET请求。这时因为有service的创建，所以APIServer会相应这个请求并把service回复给kube-dns。
3.APIServer将创建的service信息回复给kube-dns,还会附带一个APIServer分配给service的Cluster IP。
4.kube-dns通过检测并得到APIServer回复的service信息，会生成DNS条目，并把这个DNS条目存储到内存(Tree-Cache)中
5.kubernetes中访问service的时候，会先去dnsmasq中查找缓存，找不到则去kubedns中查找dns条目，最终实现service的解析。(sidecar是用于检查其他两个容器的健康状态)

通过dns的服务发现机制，有个弊端就是服务的健康检查，不过这一点通过pod的健康检查可以填补。

consul、etcd等服务发现机制

consul：这个具体还没实施过，大致意思就是，容器启动时注册自己的ip+port到consul，然后consul自己做健康检查，最终将其发往fabio，fabio是个大路由，前端统一反代到fabio。
etcd：大体实现方式，就是写脚本通过etcd的api注册服务，然后再写一个service discover的脚本循环查询注册进去的service，对比template中的内容，然后生成新的配置文件进行更新。

kubernetes service 暴露

目前kubernetes service 暴露的方式有如下几种


  ClusterIP 只提供kubernetes集群内部的服务发现
  NodePort 在每个节点上提供端口暴露服务
  LoadBlancer 只能在云平台上使用，使用云平台提供的LB来暴露服务
  ExternalName 与另一个域名绑定，通过该service访问另一个服务
  ingress/traefik 使用第三方插件将pod暴露出来


而无论是ClusterIP 还是 NodePort 都是通过kube-proxy来对service进行实现的，而kube-proxy又有两种方式来实现负载，userspace和iptables，下面我来说一下kubernetes默认的iptables方式的kube-proxy

首先，我们来新建一个NodePort类型的服务

apiVersion: v1
kind: Service
metadata:
  name: myweb-service
spec:
  type: NodePort
  ports:
  - port: 8080
    nodePort: 30001
  selector:
    app: myweb         


myweb-service 代理了后端的一个pod，ip为10.233.88.53，看看iptables

下面来逐条分析

如果是通过node的30001来访问，则会进入如下的链

# 看看和NodePort  30001有关的iptables

$ iptables -S -t nat |grep 30001
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/myweb-service:" -m tcp --dport 30001 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/myweb-service:" -m tcp --dport 30001 -j KUBE-SVC-KINM4OXG42E5QTAT


然后进一步跳转到KUBE-SVC-KINM4OXG42E5QTAT的链

-A KUBE-SVC-KINM4OXG42E5QTAT -m comment --comment "default/myweb-service:" -j KUBE-SEP-I4OJ7A6SXM5YG2QP


然后会跳转到KUBE-SEP-I4OJ7A6SXM5YG2QP链，最终将请求转发到10.233.88.53的pod上

-A KUBE-SEP-I4OJ7A6SXM5YG2QP -p tcp -m comment --comment "default/myweb-service:" -m tcp -j DNAT --to-destination 10.233.88.53:8080



  注意：如果service代理了多个pod的话，会利用iptables的–probability特性，按一定的比例转发，如下


假若我们的myweb-service代理了3个pod

如果是通过node的30001来访问，则会进入如下的链

# 看看和NodePort  30001有关的iptables

$ iptables -S -t nat |grep 30001
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/myweb-service:" -m tcp --dport 30001 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment "default/myweb-service:" -m tcp --dport 30001 -j KUBE-SVC-KINM4OXG42E5QTAT


然后进一步跳转到KUBE-SEP-5I5KUCBAI2CKFMN2、KUBE-SEP-I4OJ7A6SXM5YG2QP、KUBE-SEP-FDQHGZ7N6PHRDJRL的链

# 有分为30%，50%，20%的几率
-A KUBE-SVC-KINM4OXG42E5QTAT -m comment --comment "default/myweb-service:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-5I5KUCBAI2CKFMN2

-A KUBE-SVC-KINM4OXG42E5QTAT -m comment --comment "default/myweb-service:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-I4OJ7A6SXM5YG2QP

-A KUBE-SVC-KINM4OXG42E5QTAT -m comment --comment "default/myweb-service:" -j KUBE-SEP-FDQHGZ7N6PHRDJRL


然后会分别跳到上面每个链下对应的链，最终转发到对应的pod上

# KUBE-SEP-5I5KUCBAI2CKFMN2
-A KUBE-SEP-5I5KUCBAI2CKFMN2 -p tcp -m comment --comment "default/myweb-service:" -m tcp -j DNAT --to-destination 10.233.87.108:8080

# KUBE-SEP-I4OJ7A6SXM5YG2QP
-A KUBE-SEP-I4OJ7A6SXM5YG2QP -p tcp -m comment --comment "default/myweb-service:" -m tcp -j DNAT --to-destination 10.233.88.53:8080

# KUBE-SEP-FDQHGZ7N6PHRDJRL
-A KUBE-SEP-FDQHGZ7N6PHRDJRL -p tcp -m comment --comment "default/myweb-service:" -m tcp -j DNAT --to-destination 10.233.96.254:8080


好了，说完了NodePort，然后我们再说ClusterIP

继续从上面看到尾就行了。

ingress和traefik留到后面单独留个篇幅来说吧，这里就不说了。

Volume

kubernetes 的volume要说的内容就很多了。

首先volume是一个在pod中能被多个容器访问的共享目录，它是定义在pod上，然后挂载到容器下，而且它的生命周期只和pod有关。常见的存储类型有如下几种


  emptyDir
  hostPath
  GCE Persistent Disks
  NFS
  RBD
  ISCSCI
  AWS ElasticBlockStore
  GlusterFS
  secret
  PersistentVolume
  downwardAPI
  projected
  configmap
  local


注意：gitRepo实际上也是挂载一个空目录，从GIT仓库中clone内容下来供pod使用，所以它的数据也无法永久保存

emptyDir

正如它的名字一样，这是一个空的目录，它是在Pod被分配到node节点上的时候被创建的，无需在宿主机node上指定对应的目录文件。而且当pod从节点上被删除之后，emptyDir中的数据也会被删除，emptyDir 一般用在如下几个地方


  暂存空间，例如用于基于磁盘的归并排序或长计算的检查点的暂存空间
  临时目录，临时储存那些无需持久化的数据


默认情况下，emptyDir 数据存储在SSD或者网络存储上。但是，你也可以设置emptyDir.medium为Memory来启用tmpfs,tmpfs会将数据写入到内存中，因此，当机器重启之后，数据也会永久删除，并且，因为是存放在内存中，这些数据会占用你容器内存的limit指标。

apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: nginx
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir:
      medium: Memory


secret

secret volume 使用来传递敏感信息，比如说密码之类的。我们可以将在kubernetes中定义的secret直接挂载为文件让pod访问。secret volume实际是通过tmpfs(内存文件系统)来实现的，所以这些信息不会持久保存。

downwardAPI

如果你想将pod或container里面的字段暴露给其他正在运行的容器，那么downwardAPI正是你所需要的，它会将pod或container里面的字段以文件的形式存储下来，然后挂载到对应的容器中。

目前能暴露的字段

  node’s name
  pod’s name
  pod’s namespace
  pod’s ip
  pod’s serviceAccount name
  pod’s UID
  pod’s labels
  pod’s annotations
  容器 CPU limit
  容器 CPU request
  容器 memory limit
  容器 memory request


apiVersion: v1
kind: Pod
metadata:
  name: kubernetes-downwardapi-volume-example
  labels:
    zone: us-est-coast
    cluster: test-cluster1
    rack: rack-22
  annotations:
    build: two
    builder: john-doe
spec:
  containers:
    - name: client-container
      image: gcr.io/google_containers/busybox
      command: ["sh", "-c"]
      args:
      - while true; do
          if [[ -e /etc/labels ]]; then
            echo -en '\n\n'; cat /etc/labels; fi;
          if [[ -e /etc/annotations ]]; then
            echo -en '\n\n'; cat /etc/annotations; fi;
          sleep 5;
        done;
      volumeMounts:
        - name: podinfo
          mountPath: /etc
          readOnly: false
  volumes:
    - name: podinfo
      downwardAPI:
        items:
          - path: "labels"
            fieldRef:
              fieldPath: metadata.labels
          - path: "annotations"
            fieldRef:
              fieldPath: metadata.annotations


projected

projected volume 可以将几个volume内容隐射到同样的目录中，当前只支持secret、configmap、downwardAPI

apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - downwardAPI:
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "cpu_limit"
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config


local

local volume在1.7中目前还是alpha版本，主要是用来将本地的disk，分区或者目录进行挂载。local volume只能以静态创建的PV使用。相对于HostPath，localhost可以直接以持久化的方式使用（它总是通过NodeAffinity调度在某个指定的节点上），而hostpath是无法直接以pv来使用的。

hostPath

hostPath 就是将宿主机上的目录或文件挂在到pod里。比如我们常用到hostPath的几个例子


  容器应用程序的日志，需要永久保存时，可以使用hostPath隐射宿主机上的高速文件存储
  运行的容器需要访问Docker内部结构：使用hostPath映射/var/lib/docker
  在容器中运行cAdvisor，使用hostPath映射/dev/cgroups


而当我们在使用hostPath的时候需要注意以下几点


  在不同Node上具有相同配置的pod(通过podTemplate创建的)，可能会因为宿主机上的目录和文件不同而导致volume上目录和文件的访问结果不一致
  如果使用资源配额，无法将hostPath在宿主机上使用的资源纳入管理
  如果宿主机上的目录是root权限，那么你也必须以root身份来运行你的进程，或者，修改你的目录权限以便于能让hostPath卷有写权限


apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: nginx
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      path: /data


其实hostPath也能算是持久存储的一种，只不过局限性太大了。这里我们详细的讲讲外部存储和分布式存储在kubernetes中的使用(aws和gce的就不讲了，没环境)。

详细的例子大家可以参考官方的例子

NFS

NFS是Network File System的缩写，即网络文件系统。kubernetes中通过简单的配置就可以挂载NFS到Pod中，而NFS中的数据是可以永久保存的，同时NFS支持同时写操作。

首先，你得有个已经搭建好的NFS服务

apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: nginx
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    nfs:
      server: 163.xx.xx.xx
      path: "/"


persistent volume

事实上，我们可以单独在Pod中指定外部存储，也可以将这些外部存使用PersistentVolume资源化。

之前我们提到的volume都是定义在pod上的，和 pod 是一种静态绑定关系，属于”计算资源”的一部分，而实际上，“网络存储”是独立于“计算资源”之外而存在的一种实体资源。比如我们在使用虚机的情况下，我们通常会定义一个网络存储，然后从中划出一定的空间连接到虚拟机。而persistent volume和与之关联的persistent volume claim就起到了类似的作用。

PersistentVolume(PV)是集群中的一块网络存储，用来提供网络存储资源 。跟Node一样，也是集群的资源。PV 跟volume类似，不过会有独立于Pod的生命周期。
PersistentVolumeClaim(PVC)是对PV的请求。PVC有点类似于Pod，pod消耗node的资源，而PVC消耗PV的资源。Pod请求CPU和内存，而PVC请求特定大小和访问模式的数据卷。

PV的访问模式有三种：


  ReadWriteOnce(RWO):最基本的访问方式，可读可写，但只支持被耽搁pod挂载
  ReadOnlyMany(ROX):可以以只读的方式被多个pod挂载
  ReadWriteMany(RWX):以读写的方式被多个POD挂载。


并不是所有存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是NFS。在PVC绑定PV时通常根据两个条件来绑定，一个实存储大小，一个是访问方式。
需要注意的是，虽然PV支持三种访问模式，但它同时只支持一种方式来访问PV

创建PV的方式有两种

有两种创建PV的方式：静态和动态

静态

所谓静态，就是管理员手动创建一堆PV，组成一个PV池，供PVC来绑定。

动态
经过API抽象，用户可以通过PVC使用存储资源，通常用户还会关心PV的很多属性，例如对不同的应用场景需要不同的性能，仅仅提供存储大小和访问模式不能满足要求。集群管理员一方面要提供不同PV的多种属性，一方面要隐藏底层的细节，还有一点是不再需要管理员手动去创建PV,这就引入了StorageClass资源。管理员用存储级别StorageClass描述存储的分类，不同的分类可以对应不同的质量服务Qos等级、备份策略和其他自定义的策略。kubernetes本身不参与存储级别的划分，StorageClass概念在有的存储系统里被称为”profiles”

所谓动态，就是当所有的静态PV都不匹配用户的PVC时，集群通过storageClass的对象由存储系统根据PVC的要求自动创建。这种基于StorageClass的PV，管理员必须事先创建和配置这样的storage class。请求等级配置为" "的PVC，有效地禁用了它自身的动态供给功能。

class

我们可以根据不同的需求创建不同类型的PV(这种PV是基于StorageClass，是自动创建的)，然后我们可以通过为PVC指定storageClassName来请求PV，如果集群中有这个class，那么当用户请求的时候，kubernetes会自动的创建PV，如果集群中有默认的storageclass，那么你只需要创建PVC即可，无需指定storageClassName,剩下的都有默认的动态配置来搞定。

举个例子：比如我现在需要两种类型的存储，一种是SSD，一种是普通的硬盘，那么这时候，我就可以创建两种class的StorageClass,然后在创建PVC时，指定不同的storageClassName即可，如下：

每个StorageClass都包含provisioner和parameters这个两个字段，具体怎么配置这些storageclass，有哪些存储支持storageclass,请参考官网

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: slow
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard


kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-fast
  storageClassName: fast
spec:
  accessModes:
    - ReadOnlyMany
  resources:
    requests:
      storage: 2Gi

当用户请求资源的时候，如果是通过pvc-fast请求，这时候就会绑定到一个SSD，而不会绑定到普通硬盘。

绑定

在动态配置的情况下，当用户创建或者之前就已经创建了具有特定数量或具有某些访问模式的PVC的时候。master中的loop control会监视新的PVC，找到匹配的PV，然后将它们绑定到一起。如果这个PV是通过动态提供给PVC，那么loop control会始终绑定这个PV给这个PVC。否则，用户可能会一直请求PV，但是实际上又没有那么多PV资源。还需要注意的事，一旦绑定了PVC，就不能再绑定其他。

使用

Pod使用PVC和使用Volume一样。集群检查PVC然后找到绑定的pv，然后隐射给pod使用。
一旦用户拥有了一个PVC，并且PVC被绑定，那么只要用户还需要，PV就一直属于这个用户。

回收

当用户不在需要PV时，我们可以删除PVC来回收PV，PersistentVolume中的回收策略会告诉kubernetes当PVC被释放后该怎么做，目前，支持的策略如下


  Retained(保留)


当PVC被删除后，PV仍然被保留下来，并且会变成released。但是它还不能被其他PVC使用，因为现在PV上仍然有上一个PVC所请求的数据。


  Recycled(再利用)


当PVC被删除后，kubernetes会将PV里的数据删除，然后把PV变成Available，然后又可以被新的PVC绑定

这个原理实际上是：在删除PVC之后，会运行一个POD来执行一个(rm -rf /thevolume/*)的操作，删除pv下的所有数据

默认回收运行的POD用的image是gcr上的busybox，而且image策略是always，因为这个原因，你可能始终无法回收PV，这时候，就需要去重新配置回收POD的模板了，模板内容如下
apiVersion: v1
kind: Pod
metadata:
  name: recycler-for-nfs
  namespace: default
spec:
  restartPolicy: Never
  volumes:
  - name: vol
    nfs:
      path: /
      server: 163.xx.xx.xx
  containers:
  - name: pv-recycler
    image: "gcr.io/google_containers/busybox"
    imagePullPolicy: IfNotPresent
    command: ["/bin/sh", "-c", "test -e /scrub &amp;&amp; rm -rf /scrub/..?* /scrub/.[!.]* /scrub/*  &amp;&amp; test -z \"$(ls -A /scrub)\" || exit 1"]
    volumeMounts:
    - name: vol
      mountPath: /scrub


写好模板了，还需要在kube-controller-manager中去配置模板，更多详细信息可参考官网

# 指定nfs回收模板的位置
- --pv-recycler-pod-template-filepath-nfs=/etc/kubernetes/recycler-for-nfs.yaml

# 在容器下挂载模板文件
volumeMounts:
- mountPath: "/etc/kubernetes/recycler-for-nfs.yaml"
  name: recycler-nfs

# 将宿主机上的模板文件挂载到pod
volumes:
- name: recycler-nfs
  hostPath:
    path: "/etc/kubernetes/recycler-for-nfs.yaml"


下面我们来新建PV和PVC来测试一下

nfs-pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  nfs:
    path: "/"
    server: 163.44.165.142


nfs-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi


nfs-nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: nginx
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    persistentVolumeClaim:
      claimName: nfs



命令如下：

# 创建PV,PVC以及关联的POD
kubectl create -f nfs-pv.yaml
kubectl create -f nfs-pvc.yaml
kubectl create -f nfs-nginx.yaml

# 查看创建的PV
$ kubectl get pv
NAME      CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM         STORAGECLASS   REASON    AGE
nfs       5Gi        RWO           Recycle         Bound     default/nfs                            1h

# 查看创建的PVC
$ kubectl get pvc
NAME      STATUS    VOLUME    CAPACITY   ACCESSMODES   STORAGECLASS   AGE
nfs       Bound     nfs       5Gi        RWO                          8m

# 然后我们进入pod中新建一个文件recycle-file
$ kubectl exec -ti test-pd /bin/bash
$ touch recycle-file


我们发现nfs的回收策略是Recycle，当前状态是Bound，那么假如我现在将PVC删除掉呢，下面我们来操作试试看

kubectl delete -f nfs-nginx.yaml
kubectl delete -f nfs-pvc.yaml

# 查看现在的pv
$ kubectl get pv
NAME      CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS     CLAIM         STORAGECLASS   REASON    AGE
nfs       5Gi        RWO           Recycle         Released   default/nfs                            1h

# 过了一会儿，再看pv
$ kubectl get pv
NAME      CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE
nfs       5Gi        RWO           Recycle         Available                                      1h

# 再重新使用PVC关联POD
kubectl create -f nfs-pvc.yaml
kubectl create -f nfs-nginx.yaml

# 进入pod中看看数据是否还存在
$ kubectl exec -ti test-pd ls /test-pd


我们发现当我们的回收策略是Recycle时，删除PVC之后，PV的状态先是Released，然后过一会儿之后，会变成Available状态，而且PV上的数据也已经被删除了，这时候就可以再次被其他的PVC使用了。

注意：当前只有NFS和HostPath支持回收利用操作


  Delete(删除)


当PVC被删除后，kubernetes会删除PV及里面的数据。

注意：当前只有AWS EBS,GCE PD,AZURE DISK,OPENSTACK CINDER卷支持删除操作

注意：动态PV，总会在PVC被删除后被删除

PV阶段状态

一个volume会处于下面的几个状态之一


  Avaliable 尚未绑定到PVC上的可用资源
  Bound 已经被绑定到PVC
  Released PVC已被删除，但是资源尚未回收
  Failed 自动回收失败


Capacity

通常，我们在创建PV的时候，会从存储上给它划定一定大小的容量，这就使用capacity来指定即可。

resource

pvc，就像pod一样，可以指定request资源的大小

selector

PVC也可以指定标签选择器进行深度过滤PV，只有匹配了selector的PV才能绑定给PVC


  matchLabels 单个匹配
  matchExpressions 表达式匹配


Namespace

Namespace一般用于实现多租户的资源隔离。namespace 通过将集群内部的资源分配到不同的Namespace中，形成逻辑上不同项目、小组或环境的隔离，同时利用resource quota实现资源的管控限制，而随着kubernetes访问控制的深入，namespace开始与kubernetes的认证和授权机制结合。

Annotation

annotation 和label类似，也是使用 key/value的形式进行定义。不同的是label具有严格的命名规则，它定义的是kubernetes对象的元数据，并且用于label selector，而annotation测试用户任意定义的”附加”信息，以便于外部工具进行查找，很多时候，kubernetes的模块会通过annotation的方式标记资源的特殊信息。
                    </p>
                    <p class="repo-list-meta">
                        <span class="meta-info">
                          <span class="octicon octicon-calendar"></span> 2017/09/01
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#kubernetes" title="kubernetes">kubernetes</a>
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#docker" title="docker">docker</a>
                        </span>
                        
                    </p>
                </li>
                
                <li class="repo-list-item">
                    <h3 class="repo-list-name">
                      <a href="https://kevinguo.me/2017/09/01/docker-consul-consul-template-registrator-nginx/">docker+consul+consul-template+registrator+nginx 的容器服务注册发现</a>
                    </h3>
                    <p class="repo-list-description">
                        前言


  以前了解过一段时间的consul，只知道consul是一个服务发现的工具，但是具体是怎么注册的，又是怎么服务发现的，一点也不清楚，这次趁着研究kubernetes的服务发现，顺带研究了下consul，在此记录下来。


概念

简单来说，consul是一个提供服务注册、服务发现、键值存储、健康检查的工具，并且它支持多数据中心。

举个简单的例子，假若我们有一个暴露REST API的服务，为了高可用，我们决定为该服务提供3个服务实例，但是每个容器的地址和端口都是随机的，那么我们的服务之间怎么通信呢，我们又该怎么在前端LB上添加我们的后端服务呢？这时候就需要用到我们的服务发现工具consul了，其实还有很多其他的服务发现工具，比如etcd，zookeeper等等，这里我们重点说下consul。


  服务之间的通信，这个很简单，我们通过LB即可，所有的服务之间的互相访问，都通过LB即可，我们只需要确定每个服务对于的域名即可。
  在LB上动态添加后端，这个通过consul-template+consul+registrator即可,consul-template会监控consul中的对应内容，然后根据consul模板文件生成新的配置


该实验所需的所有配置文件内容都放在我的Github上:


  该项目提供了一种简单的方法，使用consul-template将consul中的值生成具体所需的配置文件，并且实时监控consul，根据模板文件生成最新的配置文件，然后运行某些指定命令。


由于环境有限，我下面所有的实验都是在一台机器上用docker完成，你只需要修改对应的IP地址即可

Consul

提供服务注册和服务发现

# 注意这里我指定了-client是为了方便我通过外网访问consul ui

docker run -d --net=host --name consul -h consul -e 'CONSUL_LOCAL_CONFIG={"skip_leave_on_interrupt": true}' consul agent -server -client $HOST_IP -ui -bootstrap


Registrator

将宿主机上的容器自动注册到consul中


docker run -d --net=host -v /var/run/docker.sock:/tmp/docker.sock --name registrator -h registrator gliderlabs/registrator:latest -internal consul://$HOST_IP:8500


Nginx with consul-template

利用consul-template监控consul，根据模板生成新的配置，并提供负载均衡

将上面地址中的内容clone到本地,build nginx-consul镜像

git clone https://github.com/chinakevinguo/docker-consul.git
cd docker-consul
chmod +x start.sh
docker build -t docker-consul .


启动nginx

docker run -p 8080:80 -d --name nginx -e CONSUL_URL=$HOST_IP:8500 --volume ~/docker-consul/service.ctmpl:/templates/service.ctmpl  nginx-consul


启动一些服务实例

具体提供服务的实例

docker run -d -P --name node1 -h node1 jlordiales/python-micro-service:latest
docker run -d -P --name node2 -h node2 jlordiales/python-micro-service:latest
docker run -d -P --name node3 -h node3 jlordiales/python-micro-service:latest


启动完成后，我们来看看nginx中是否已经动态添加了这些后端呢

$ docker exec -ti nginx cat /etc/nginx/conf.d/service.conf
upstream python-service {
  least_conn;
  server 172.17.0.5:5000 max_fails=3 fail_timeout=60 weight=1;
  server 172.17.0.6:5000 max_fails=3 fail_timeout=60 weight=1;
  server 172.17.0.7:5000 max_fails=3 fail_timeout=60 weight=1;
  server 172.17.0.8:5000 max_fails=3 fail_timeout=60 weight=1;

}

server {
  listen 80 default_server;

  charset utf-8;

  location /{
    proxy_pass http://python-service;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header Host $host;
    proxy_set_header X-Real-IP $remote_addr;
  }
}



然后我们来看看我们的consul ui中展现了那些内容呢



我们发现我们的python-micro-service 服务目前有4个，而且当我们通过如下命令访问的时候，也是4个轮询着被访问

$ while true; do curl $HOST_IP:8080; echo ----; sleep 1; done;
Hello World from node4----
Hello World from node1----
Hello World from node2----
Hello World from node3----


这时候，如果我马上停掉其中一个呢,我们发现consul ui中的服务也相应的减少，而且用命令访问的时候，也已经变成了3个



$ while true; do curl $HOST_IP:8080; echo ----; sleep 1; done;
Hello World from node1----
Hello World from node2----
Hello World from node3----


Conclusion

文章写的比较简单，其实就是通过一个简单的实验，了解了下consul的服务注册，服务发现，以及如何使用consul-template来动态的生成对应的配置文件，而关于服务注册，我们使用的是registrator，也许你的项目需要调用consul的HTTP API来注册也说不定，具体你可以去consul官网了解更多。
                    </p>
                    <p class="repo-list-meta">
                        <span class="meta-info">
                          <span class="octicon octicon-calendar"></span> 2017/09/01
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#consul" title="consul">consul</a>
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#docker" title="docker">docker</a>
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#registrator" title="registrator">registrator</a>
                        </span>
                        
                    </p>
                </li>
                
                <li class="repo-list-item">
                    <h3 class="repo-list-name">
                      <a href="https://kevinguo.me/2017/08/23/Docker-container-communication/">Docker基础-理解容器之间的通信</a>
                    </h3>
                    <p class="repo-list-description">
                        本节主要是讲解docker容器是如何在默认的bridge网络上进行通信的。



和外网通信

容器能否和外网进行通信，取决两个因素。


  主机上的ip_forward是否打开，默认为true
  iptables规则是否允许被修改，默认为true


如果在启动 Docker 服务的时候设定 –ip-forward=true, Docker 就会自动设定系统的 ip_forward 参数为 1。如果你的系统上打开了ip_froward，而你在docker中设定--ip-forward=flase，这不会起作用。

关于ip_froward的实验

docker run -d --name  db  training/postgres

# 这时候首先去将ip_froward设置为false，发现无法ping通外网
 docker exec -ti db ping www.baidu.com

# 这时候再将ip_forward设置为true，发现可以ping通外网
docker exec -ti db ping www.baidu.com
PING www.a.shifen.com (103.235.46.39) 56(84) bytes of data.
64 bytes from 103.235.46.39: icmp_seq=1 ttl=54 time=56.3 ms
64 bytes from 103.235.46.39: icmp_seq=2 ttl=54 time=56.3 ms



默认情况下，docker会在启动容器的时候，在iptables中添加规则，如果你设定--iptables=false，docker将不会修改你的防火墙规则。

关于--iptables的实验

# 首先，确认下有没有关于docker0的防火墙规则，有就删除掉
iptables -S |grep docker0

# 然后测试ping外网，发现无法ping通
docker exec -ti db ping www.baidu.com

# 这时候将iptables改为true，查看防火墙规则，又添加了关于docker0的规则
iptables -S |grep docker
-A FORWARD -o docker0 -j DOCKER
-A FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
-A FORWARD -i docker0 ! -o docker0 -j ACCEPT
-A FORWARD -i docker0 -o docker0 -j ACCEPT

# 测试发现能ping通外网

docker exec -ti db ping www.baidu.com
PING www.a.shifen.com (103.235.46.39) 56(84) bytes of data.
64 bytes from 103.235.46.39: icmp_seq=1 ttl=54 time=56.3 ms
64 bytes from 103.235.46.39: icmp_seq=2 ttl=54 time=56.3 ms


默认情况下，容器可以和任何外部地址进行通信。除非你设定仅仅只有那些地址可以访问内部的容器。如下，设定只有8.8.8.8能访问内部容器

iptables -I DOCKER -i ext_if ! -s 8.8.8.8 -j DROP


容器之间通信

容器之间是否能进行通信，取决于系统层面的两个因素。


  容器网络是否连接到docker0网络，默认是连接到docker0
  你的iptables是否允许被修改，默认为true,你的--icc状态，默认为true


注意：如果你使用默认的iptables=true，则当--icc=true时会添加 ACCEPT 规则，当--icc=false时会添加 DROP 规则; 当然如果你的iptables=false则无所谓了，反正不会修改你的防火墙规则

关于--icc的实验

# 设置--icc=false发现容器之间无法ping通,如果你发现你依然能ping通，那可能是前面的防火墙配置，执行iptables -F 清空下再试
docker exec -ti db ping 172.17.0.3


# 设置--icc=true发现容器之间可以ping通
docker exec -ti db ping 172.17.0.3
PING 172.17.0.3 (172.17.0.3) 56(84) bytes of data.
64 bytes from 172.17.0.3: icmp_seq=1 ttl=64 time=0.128 ms
64 bytes from 172.17.0.3: icmp_seq=2 ttl=64 time=0.110 ms
64 bytes from 172.17.0.3: icmp_seq=3 ttl=64 time=0.096 ms




  注意：所以，我们启动docker之前，就要确定好，到底是否需要允许docker修改iptables，是否允许容器之间通信，是否允许容器访问外网等


主机之间的容器通信

官网上说的是将默认FORWARD DROP策略改为 ACCEPT，其实没那个必要，使用overlay网络就好了,比如:flannel,weave,calico等等…
                    </p>
                    <p class="repo-list-meta">
                        <span class="meta-info">
                          <span class="octicon octicon-calendar"></span> 2017/08/23
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#docker" title="docker">docker</a>
                        </span>
                        
                    </p>
                </li>
                
                <li class="repo-list-item">
                    <h3 class="repo-list-name">
                      <a href="https://kevinguo.me/2017/08/22/Rancher-for-docker-hosts/">High Availability Ranchers for docker hosts</a>
                    </h3>
                    <p class="repo-list-description">
                        前言


  由于公司目前有部分业务跑在docker上，但又还没有上诸如:swarm,kubernetes之类的容器编排工具，但是又想要能对docker主机及容器进行一个简单可视化的管理，筛选来，筛选去，发现无论是:DockerUI、Shipyard、portainer还是Daocloud都不能符合我们的心意，最终决定使用rancher来进行管理。


Rancher是什么

借用官方文档里面的话来说，Rancher是一个开源的容器管理平台。

实际上Rancher能够整合目前市面上大多数的容器编排工具，如：swarm、kubernetes、mesos等，而且，它本身最拥有一个最基础的编排工具cattle，本次我们使用的就是它的cattle编排器。

环境准备

所有系统均为CentOS 7.2


  
    
      IP
      ROLE
    
  
  
    
      172.30.33.44
      Rancher Server 01
    
    
      172.30.33.45
      Rancher Server 02
    
    
      172.30.33.227
      External MySQL Server
    
    
      172.30.33.183
      Nginx LB
    
  


所需镜像如下


  
    
      IMAGE
      VERSION
    
  
  
    
      rancher/server
      latest
    
    
      rancher/agent
      v1.2.5
    
    
      rancher/scheduler
      v0.8.2
    
    
      rancher/healthcheck
      v0.3.1
    
    
      rancher/dns
      v0.15.1
    
    
      rancher/metadata
      v0.9.3
    
    
      rancher/net
      v0.11.7
    
    
      rancher/net
      holder
    
    
      rancher/network-manager
      v0.7.7
    
  


Rancher Server

Ecternal mySQL Server

使用如下命令，创建cattle数据库和cattle用户

CREATE DATABASE IF NOT EXISTS cattle COLLATE = 'utf8_general_ci' CHARACTER SET = 'utf8';
GRANT ALL ON cattle.* TO 'cattle'@'%' IDENTIFIED BY 'cattle';
GRANT ALL ON cattle.* TO 'cattle'@'localhost' IDENTIFIED BY 'cattle';


Rancher Server

在每台Node上执行如下命令

docker run -d --restart=unless-stopped -p 8080:8080 -p 9345:9345 rancher/server \
     --db-host myhost.example.com --db-port 3306 --db-user username --db-pass password --db-name cattle \
     --advertise-address &lt;IP_of_the_Node&gt;



  注意： 如果你改变了-p 8080:8080端口，则需要额外添加一个参数--addvertise-http-port &lt;host_port&gt;


Nginx LB

在nginx的Vhost中添加如下两个文件

rancher-upstream.conf

upstream rancher-server {
        server 172.30.33.44:8080;
        server 172.30.33.45:8080;
}
map $http_upgrade $connection_upgrade {
    default Upgrade;
    ''      close;
}         


rancher.conf

server {
        listen 80;
        server_name rancher.quark.com;

        access_log  logs/rancher_access.log main;
        location / {
            #internal;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header X-Forwarded-Port $server_port;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_pass http://rancher-server;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection $connection_upgrade;
        proxy_read_timeout 900s;
        }
    }       


Rancher agent

上面的步骤完成后，我们来访问我们的rancher ui，然后来添加host


依次添加完你的docker host之后，如下图


后记

rancher上还有很多功能，感兴趣的同学可以去自行研究下，我这里只是一个临时的需求，所以就不做过多的实例了。
                    </p>
                    <p class="repo-list-meta">
                        <span class="meta-info">
                          <span class="octicon octicon-calendar"></span> 2017/08/22
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#rancher" title="rancher">rancher</a>
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#docker" title="docker">docker</a>
                        </span>
                        
                    </p>
                </li>
                
                <li class="repo-list-item">
                    <h3 class="repo-list-name">
                      <a href="https://kevinguo.me/2017/07/13/jekyll-create-a-static-blog(3)/">jekyll 搭建静态博客之https番外篇</a>
                    </h3>
                    <p class="repo-list-description">
                        上一篇HTTPS是使用cloudflare进行加密的，所有的请求都交给cloudflare来进行转发，就会出现域名指向的IP不是主机的问题，虽然安全些，但是总感觉怪怪的，所以这一篇，直接通过 nginx+docker+let’s encrypt 搭建HTTPS认证



前提

首先记得将 cloudflare 上有关你的站点的https配置删掉，如果没有做过，则忽略，同时记得去你的 DNS 控制器上将你的域名解析改为默认

搭建过程

搭建过程比较简单


  准备好你前端的 nginx 服务器
  准备好你后端的 docker 容器
  clone let’s encrypt 到服务器
  生成证书
  配置你的 nginx


1.准备 nginx

yum update

yum install nginx


2.clone let’s encrypt 到服务器

git clone https://github.com/certbot/certbot.git


3.准备 docker 容器

一条命令即可

docker run -d --name chinakevinguo_jekyll_kevinguo_me --restart=always -p 127.0.0.1:1080:80 chinakevinguo_jekyll_kevinguo_me


4.生成证书

注意：如果你开了防火墙，记得一定要将防火墙关闭，否则生成证书的时候可能会报错

systemctl stop firewalld

# 将这里的域名，换成你自己的域名
cd certbot
./certbot-auto certonly --nginx -d kevinguo.me -d www.kevinguo.me


成功后，内容如下

IMPORTANT NOTES:
 - Congratulations! Your certificate and chain have been saved at
   /etc/letsencrypt/live/kevinguo.me/fullchain.pem. Your cert will
   expire on 2017-10-11. To obtain a new or tweaked version of this
   certificate in the future, simply run certbot-auto again. To
   non-interactively renew *all* of your certificates, run
   certbot-auto renew
 - If you like Certbot, please consider supporting our work by:

   Donating to ISRG / Lets Encrypt:   https://letsencrypt.org/donate
   Donating to EFF:                    https://eff.org/donate-le


然后生成dhparam证书，可能会花费一段时间

openssl dhparam -out /etc/letsencrypt/live/kevinguo.me/dhparam.pem 2048


5.配置你的 nginx

nginx 的配置尽量模块化，这里通过 nginx 作为代理，访问后端的 docker 容器

nginx.conf 内容如下
user nginx;
worker_processes auto;
pid /run/nginx.pid;

# Load dynamic modules. See /usr/share/nginx/README.dynamic.
include /usr/share/nginx/modules/*.conf;

events {
    use epoll;
    worker_connections 1024;
}

http {
    server_tokens off;
    log_format  main  '$remote_addr||$time_local||"$request"||'
                      '$status||$body_bytes_sent||"$http_referer"'
                      '||$http_x_forwarded_for||'
                      '||$upstream_status||$upstream_addr||$request_time||$upstream_response_time||' ;

    sendfile            on;
    tcp_nopush          on;
    tcp_nodelay         on;
    keepalive_timeout   65;
    types_hash_max_size 2048;

    include             /etc/nginx/mime.types;
    default_type        application/octet-stream;

    gzip on;
    gzip_disable "msie6";
    gzip_proxied any;
    gzip_min_length 1000;
    gzip_comp_level 6;
    gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;

    include /etc/nginx/conf.d/*.conf;
    }



对应站点.conf 内容如下
upstream chinakevinguo_jekyll_kevinguo_me {
    server 127.0.0.1:1080;
}
server {
    listen 80 default_server;
    listen [::]:80 default_server;
    server_name kevinguo.me www.kevinguo.me;

    # Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response.
    return 301 https://$server_name$request_uri;
    #return 404;
}

server {
    listen 443 ssl http2 default_server;
    listen [::]:443 ssl http2 default_server;
    server_name kevinguo.me www.kevinguo.me;

    # certs sent to the client in SERVER HELLO are concatenated in ssl_certificate
    ssl_certificate /etc/letsencrypt/live/kevinguo.me/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/kevinguo.me/privkey.pem;
    ssl_dhparam /etc/letsencrypt/live/kevinguo.me/dhparam.pem;
    ssl_session_timeout 1d;
    ssl_session_cache shared:SSL:50m;
    ssl_session_tickets off;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    ssl_ciphers 'ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA:ECDHE-RSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-RSA-AES256-SHA256:DHE-RSA-AES256-SHA:ECDHE-ECDSA-DES-CBC3-SHA:ECDHE-RSA-DES-CBC3-SHA:EDH-RSA-DES-CBC3-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:DES-CBC3-SHA:!DSS';
    ssl_prefer_server_ciphers on;
    ssl_stapling on;
    ssl_stapling_verify on;
    # HSTS (ngx_http_headers_module is required) (15768000 seconds = 6 months)
    add_header Strict-Transport-Security max-age=15768000;


    add_header X-Frame-Options DENY;
    add_header  X-Content-Type-Options  nosniff;
    add_header X-XSS-Protection "1";
    access_log /var/log/nginx/chinakevinguo_jekyll_kevinguo_me_access.log ;
    error_log /var/log/nginx/chinakevinguo_jekyll_kevinguo_me_error.log ;

    location / {
       root html;
       index index.html;
       proxy_pass         http://chinakevinguo_jekyll_kevinguo_me;
       proxy_set_header   Host             $host:443;
       proxy_set_header   X-Real-IP        $remote_addr;
       proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;
       proxy_connect_timeout   10s;
       proxy_send_timeout      100s;
       proxy_read_timeout      300s;
       proxy_next_upstream error timeout http_404;
    }
}


最后，来访问试试看呢
                    </p>
                    <p class="repo-list-meta">
                        <span class="meta-info">
                          <span class="octicon octicon-calendar"></span> 2017/07/13
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#jekyll" title="jekyll">jekyll</a>
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#travis" title="travis">travis</a>
                        </span>
                        
                    </p>
                </li>
                
                <li class="repo-list-item">
                    <h3 class="repo-list-name">
                      <a href="https://kevinguo.me/2017/07/12/jekyll-create-a-static-blog(2)/">jekyll 搭建静态博客(2)</a>
                    </h3>
                    <p class="repo-list-description">
                        接上一篇，上一篇我们的博客已经可以自动化部署了，但是我们仍然不满足，还想要有一个好的评论系统和一个安全的https连接




Disqus

disqus 是一家第三方社会化评论系统，主要为网站主提供评论托管服务

1.在disqus官网注册一个账号

2.点击Admin，然后新建站点，在Website Name处输入你的站点名称，并且根据你的情况选择站点种类



3.在Website URL处输入你的站点 url，保存即可



4.修改你的_config.yml文件中的 comments_provider 和 username

重要： 这里的username，实际上是 disqus 中的shortname

comments_provider: disqus
disqus:
    username: kevinguo


5.修改完成后，push 到镜像站点，触发 Travis CI 重新发布博客，最终成功加载 disqus



如果无法加载 disqus ，可能是因为被墙了，FQ 出去试试

Cloudflare

cloudflare 主要是为客户提供网站安全管理，性能优化等，比如 HTTPS,CDN

1.在godaddy上去申请一个域名吧，一年也就 5$

2.在cloudflare官网注册一个账号

3.点击 Add Site，添加一个站点，然后 Begin Scan,大概需要60s



4.扫描完成后，会看到 DNS 记录，自行添加(其中彩色的云朵表示开启SSL，否则就只是DNS)

注意 如果你的A记录启用的SSL，那么所有关于这个A记录的请求都会转发给 Cloudflare，然后通过 Cloudflare再转发到你的服务器，所以这个时候，你通过 nslookup kevinguo.me 的时候解析出来的地址，并不是你的服务器地址，而是 Cloudflare的地址;如果你有其他服务(诸如VPS,FTP等)使用的是这个地址的话，最好是再添加一条不走SSL的A记录



5.完成上面的步骤后，到你的域名控制面板修改DNS服务



6.修改完成后，在 Cloudflare点击继续，大概5~30分钟后 Overview 状态会变成 Status: Active



7.点击 Crypto来设置SSL 级别



8.点击 Page Rules来设置域名重定向


  将顶级域名都重定向到 https://www.kevinguo.me





  添加自动使用 HTTPS，所有访问http://www.kevinguo.me的请求都使用HTTPS




9.最后，访问你的blog试试呢
                    </p>
                    <p class="repo-list-meta">
                        <span class="meta-info">
                          <span class="octicon octicon-calendar"></span> 2017/07/12
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#jekyll" title="jekyll">jekyll</a>
                        </span>
                        
                        <span class="meta-info">
                          <span class="octicon octicon-file-directory"></span>
                          <a href="https://kevinguo.me/categories/#travis" title="travis">travis</a>
                        </span>
                        
                    </p>
                </li>
                
            </ol>
        </div>
        <div class="column one-third">
            
<h3>Search</h3>
<div id="site_search">
    <input type="text" id="search_box" placeholder="Search">
</div>

<ul id="search_results"></ul>

<link rel="stylesheet" type="text/css" href="https://kevinguo.me/assets/css/modules/sidebar-search.css">
<script src="https://kevinguo.me/assets/js/simple-jekyll-search.min.js"></script>
<script src="https://kevinguo.me/assets/js/search.js"></script>

<script type="text/javascript">
SimpleJekyllSearch({
    searchInput: document.getElementById('search_box'),
    resultsContainer: document.getElementById('search_results'),
    json: 'https://kevinguo.me/assets/search_data.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a></li>',
    noResultsText: 'No results found',
    limit: ,
    fuzzy: false,
    exclude: ['Welcome']
})
</script>

            <h3>My Popular Repositories</h3>



<a href="https://github.com/chinakevinguo/kubernetes-custom" target="_blank" class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="kubernetes-custom">
            <div class="card-image-cell">
                <h3 class="card-title">
                    kubernetes-custom
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text"></p>
            </div>
            <div class="card-text">
                <span class="meta-info" title="3 stars">
                    <span class="octicon octicon-star"></span> 3
                </span>
                <span class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span class="meta-info" title="Last updated：2017-12-22 02:41:39 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2017-12-22 02:41:39 UTC">2017-12-22</time>
                </span>
            </div>
        </div>
    </div>
</a>

<a href="https://github.com/chinakevinguo/learn-python" target="_blank" class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="learn-python">
            <div class="card-image-cell">
                <h3 class="card-title">
                    learn-python
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text"></p>
            </div>
            <div class="card-text">
                <span class="meta-info" title="1 stars">
                    <span class="octicon octicon-star"></span> 1
                </span>
                <span class="meta-info" title="1 forks">
                    <span class="octicon octicon-git-branch"></span> 1
                </span>
                <span class="meta-info" title="Last updated：2018-03-02 03:11:20 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2018-03-02 03:11:20 UTC">2018-03-02</time>
                </span>
            </div>
        </div>
    </div>
</a>

<a href="https://github.com/chinakevinguo/learn-groovy" target="_blank" class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="learn-groovy">
            <div class="card-image-cell">
                <h3 class="card-title">
                    learn-groovy
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text"></p>
            </div>
            <div class="card-text">
                <span class="meta-info" title="1 stars">
                    <span class="octicon octicon-star"></span> 1
                </span>
                <span class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span class="meta-info" title="Last updated：2018-01-03 06:06:38 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2018-01-03 06:06:38 UTC">2018-01-03</time>
                </span>
            </div>
        </div>
    </div>
</a>

<a href="https://github.com/chinakevinguo/sharelibrary" target="_blank" class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="sharelibrary">
            <div class="card-image-cell">
                <h3 class="card-title">
                    sharelibrary
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text"></p>
            </div>
            <div class="card-text">
                <span class="meta-info" title="0 stars">
                    <span class="octicon octicon-star"></span> 0
                </span>
                <span class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span class="meta-info" title="Last updated：2017-12-07 03:41:29 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2017-12-07 03:41:29 UTC">2017-12-07</time>
                </span>
            </div>
        </div>
    </div>
</a>

<a href="https://github.com/chinakevinguo/kubernetes-jenkins" target="_blank" class="card text-center">
    <div class="thumbnail">
        <div class="card-image geopattern" data-pattern-id="kubernetes-jenkins">
            <div class="card-image-cell">
                <h3 class="card-title">
                    kubernetes-jenkins
                </h3>
            </div>
        </div>
        <div class="caption">
            <div class="card-description">
                <p class="card-text"></p>
            </div>
            <div class="card-text">
                <span class="meta-info" title="0 stars">
                    <span class="octicon octicon-star"></span> 0
                </span>
                <span class="meta-info" title="0 forks">
                    <span class="octicon octicon-git-branch"></span> 0
                </span>
                <span class="meta-info" title="Last updated：2017-12-27 07:13:36 UTC">
                    <span class="octicon octicon-clock"></span>
                    <time datetime="2017-12-27 07:13:36 UTC">2017-12-27</time>
                </span>
            </div>
        </div>
    </div>
</a>



        </div>
    </div>
    <div class="pagination text-align">
      <div class="btn-group">
        
          
              <a href="https://kevinguo.me/" class="btn btn-outline">&laquo;</a>
          
        
        
            <a href="https://kevinguo.me/"  class="btn btn-outline">1</a>
        
        
          
              <a href="javascript:;"  class="active btn btn-outline">2</a>
          
        
          
              <a href="https://kevinguo.me/page3"  class="btn btn-outline">3</a>
          
        
          
              <a href="https://kevinguo.me/page4"  class="btn btn-outline">4</a>
          
        
          
              <a href="https://kevinguo.me/page5"  class="btn btn-outline">5</a>
          
        
          
              <a href="https://kevinguo.me/page6"  class="btn btn-outline">6</a>
          
        
          
              <a href="https://kevinguo.me/page7"  class="btn btn-outline">7</a>
          
        
          
              <a href="https://kevinguo.me/page8"  class="btn btn-outline">8</a>
          
        
        
            <a href="https://kevinguo.me/page3"  class="btn btn-outline">&raquo;</a>
        
        </div>
    </div>
    <!-- /pagination -->
</section>
<!-- /section.content -->

    <footer class="container">
        <div class="site-footer" role="contentinfo">
            <div class="copyright left mobile-block">
                    © 2015
                    <span title="KevinGuo">KevinGuo</span>
                    <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
            </div>

            <ul class="site-footer-links right mobile-hidden">
                <li>
                    <a href="javascript:window.scrollTo(0,0)" >TOP</a>
                </li>
            </ul>
            <a href="http://github.com/chinakevinguo/chinakevinguo.github.io" target="_blank" aria-label="view source code">
                <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
            </a>
            <ul class="site-footer-links mobile-hidden">
                
                <li>
                    <a href="https://kevinguo.me/" title="首页" target="">首页</a>
                </li>
                
                <li>
                    <a href="https://kevinguo.me/categories/" title="分类" target="">分类</a>
                </li>
                
                <li>
                    <a href="https://kevinguo.me/wiki/" title="维基" target="">维基</a>
                </li>
                
                <li>
                    <a href="https://kevinguo.me/open-source/" title="开源" target="">开源</a>
                </li>
                
                <li>
                    <a href="https://kevinguo.me/links/" title="链接" target="">链接</a>
                </li>
                
                <li>
                    <a href="https://kevinguo.me/about/" title="关于" target="">关于</a>
                </li>
                
                <li><a href="https://kevinguo.me/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li>
            </ul>

        </div>
    </footer>
    <!-- / footer -->
    <script src="https://kevinguo.me/assets/vendor/share.js/dist/js/share.min.js"></script>
    <script src="https://kevinguo.me/assets/js/geopattern.js"></script>
    <script src="https://kevinguo.me/assets/js/prism.js"></script>
    <link rel="stylesheet" href="https://kevinguo.me/assets/css/globals/prism.css">
    <script>
      jQuery(document).ready(function($) {
        // geopattern
        $('.geopattern').each(function(){
          $(this).geopattern($(this).data('pattern-id'));
        });
       // hljs.initHighlightingOnLoad();
      });
    </script>

    

    

    

    

    
    <div style="display:none">
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-80669434-1', 'auto');
        ga('send', 'pageview');

      </script>
    </div>
    
</body>
</html>
