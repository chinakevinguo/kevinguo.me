<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>kubernetes 入门 &mdash; KevinGuo</title>
    <link rel="stylesheet" href="/assets/vendor/primer-css/css/primer.css">
    <link rel="stylesheet" href="/assets/vendor/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="/assets/vendor/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="/assets/css/components/collection.css">
    <link rel="stylesheet" href="/assets/css/components/repo-card.css">
    <link rel="stylesheet" href="/assets/css/sections/repo-list.css">
    <link rel="stylesheet" href="/assets/css/sections/mini-repo-list.css">
    <link rel="stylesheet" href="/assets/css/components/boxed-group.css">
    <link rel="stylesheet" href="/assets/css/globals/common.css">
    <link rel="stylesheet" href="/assets/vendor/share.js/dist/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/globals/responsive.css">
    <link rel="stylesheet" href="/assets/css/posts/index.css">
    <!-- Latest compiled and minified CSS -->
    

    
    <link rel="canonical" href="https://kevinguo.me/2017/09/01/kubernetes-one-section/">
    <link rel="alternate" type="application/atom+xml" title="KevinGuo" href="/feed.xml">
    <link rel="shortcut icon" href="/favicon.ico">
    
    <meta property="og:title" content="kubernetes 入门">
      
    <meta name="keywords" content="kubernetes,docker">
    <meta name="og:keywords" content="kubernetes,docker">
      
    <meta name="description" content="入门概念">
    <meta name="og:description" content="入门概念">
      
    
    
        
    
    <meta property="og:url" content="https://kevinguo.me/2017/09/01/kubernetes-one-section/">
    <meta property="og:site_name" content="KevinGuo">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh_CN" />
    
    <meta property="article:published_time" content="2017-09-01">
    
    <script src="/assets/vendor/jquery/dist/jquery.min.js"></script>
    <script src="/assets/js/jquery-ui.js"></script>
    <script type="text/javascript">
    function toggleMenu() {
        var nav = document.getElementsByClassName("site-header-nav")[0];
        if (nav.style.display == "inline-flex") {
          nav.style.display = "none";
        } else {
          nav.style.display = "inline-flex";
        }
    }
    </script>
</head>
<body class="" data-mz="">
    <header class="site-header">
        <div class="container">
            <h1><a href="/" title="KevinGuo"><span class="octicon octicon-mark-github"></span> KevinGuo</a></h1>
            <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <nav class="site-header-nav" role="navigation">
                
                <a href="/" class=" site-header-nav-item" target="" title="首页">首页</a>
                
                <a href="/categories/" class=" site-header-nav-item" target="" title="分类">分类</a>
                
                <a href="/wiki/" class=" site-header-nav-item" target="" title="维基">维基</a>
                
                <a href="/open-source/" class=" site-header-nav-item" target="" title="开源">开源</a>
                
                <a href="/links/" class=" site-header-nav-item" target="" title="链接">链接</a>
                
                <a href="/about/" class=" site-header-nav-item" target="" title="关于">关于</a>
                
            </nav>
        </div>
    </header>
    <!-- / header -->

    <section class="collection-head small geopattern" data-pattern-id="kubernetes 入门">
<div class="container">
  <div class="columns">
    <div class="column three-fourths">
      <div class="collection-title">
        <h1 class="collection-header">kubernetes 入门</h1>
        <div class="collection-info">
          
          <span class="meta-info">
            <span class="octicon octicon-calendar"></span> 2017/09/01
          </span>
          
          
          <span class="meta-info">
            <span class="octicon octicon-file-directory"></span>
            <a href="/categories/#kubernetes" title="kubernetes">kubernetes</a>
          </span>
          
          <span class="meta-info">
            <span class="octicon octicon-file-directory"></span>
            <a href="/categories/#docker" title="docker">docker</a>
          </span>
          
        </div>
      </div>
    </div>
  </div>
</div>
</section>
<!-- / .banner -->
<section class="container content">
<div class="columns">
  <div class="column three-fourths" >
    <article class="article-content markdown-body">
    <h2 id="入门概念">入门概念</h2>

<p>为什么要使用kubernetes</p>

<p>1.新技术</p>

<p>2.精简.只需要一个架构师专注于“服务组件”的提炼，几名开发工程师专注于代码开发，几名系统运维工程师负责kubernetes的部署和运维</p>

<p>3.kubernetes使用微服务架构</p>

<p>4.更方便迁移</p>

<p>5.超强的横向扩容能力</p>

<h3 id="master">master</h3>

<ul>
  <li>
    <p>kube-apiserver
提供HTTP RESET接口的关键服务进程，是kubernetes所有资源增删改查的唯一入口，也是集群控制的入口进程</p>
  </li>
  <li>
    <p>kube-controller-manager
kubernetes里所有资源对象的自动化控制中心。</p>
  </li>
  <li>
    <p>kube-scheduler
kubernetes里所有资源的调度中心</p>
  </li>
  <li>
    <p>kube-proxy
实现kubernetes service的通信与负载均衡机制的重要组件</p>
  </li>
  <li>
    <p>kubelet
负责pod对应容器的创建，启停等任务，同时与Master节点密切协作，实现集群管理的基本功能</p>
  </li>
  <li>
    <p>etcd
保存kubernetes里所有数据的存储</p>
  </li>
  <li>
    <p>docker
运行kubernetes 里面的容器</p>
  </li>
</ul>

<h3 id="node">node</h3>

<ul>
  <li>
    <p>kube-proxy
实现kubernetes service的通信与负载均衡机制的重要组件</p>
  </li>
  <li>
    <p>kubelet
负责pod对应容器的创建，启停等任务，同时与Master节点密切协作，实现集群管理的基本功能</p>
  </li>
  <li>
    <p>docker
运行kubernetes 里面的容器</p>
  </li>
</ul>

<h3 id="pod">pod</h3>

<p>一个pause容器和一组业务容器组成，是kubernetes里面最基本的单元。</p>

<h4 id="pause-容器">pause 容器</h4>

<ul>
  <li>既然pod是一组容器组成，那么如何来判断这个pod的状态呢，是其中一个容器死亡了，就算整个pod死亡了，还是说按照某种N/M的死亡率来算呢？kubernetes里面引入了一个不易死亡又和业务无关的<code class="highlighter-rouge">pause</code>容器，以它的状态来表示整个pod的状态。</li>
  <li>pod里面的多个容器共享<code class="highlighter-rouge">pause</code>容器里面的网络和volumes。</li>
</ul>

<h4 id="普通pod和静态pod">普通pod和静态pod</h4>

<p>pod有两种类型：普通pod和静态pod</p>
<ul>
  <li>普通pod即是那些通过deployment，replicationcontroller，daemonset等部署的，这些pod一旦创建会被放入到etcd中，然后会被kubernetes master调度到某个node上，通过node上的kubelet进程实例化成一组相关的容器并启动起来。</li>
  <li>静态pod是通过放在某个node上的一个具体的文件运行起来的。比如我们放在/etc/kubernetes/manifests下的某些静态文件。</li>
</ul>

<h4 id="endpoints">endpoints</h4>

<p>kubernetes中，每个pod都有一个属于他的IP。<code class="highlighter-rouge">Pod IP + 需要暴露出来的ContainerPort</code> 就组成了endpoint
说到service的endpoint，这里就不得不说下targetPort，targetPort属性用来确定提供该服务的容器所暴露的端口号，如果你不指定targetPort，kubernetes默认使用你提供的port为targetPort</p>

<p>所以他们的关系应该是如下：</p>

<p>service 暴露一个提供服务的端口—&gt;{pod IP+targetPort}(容器内部暴露出来提供服务的端口号)
                                         ||
                                     {endpoint}</p>
<h4 id="pod-volume">pod volume</h4>

<p>pod中的volume能够被pod中的多个容器访问。kubernetes中的volume与pod的生命周期相同，但与容器的生命周期不相关。
通常是声明一个volume，然后在容器中引用这个volume并mount到容器的某个目录上。
常用的类型有：</p>
<ul>
  <li>emptyDir</li>
  <li>hostPath</li>
  <li>Persistent Volume(GCE Persistent Disks、NFS、RBD、ISCSCI、AWS ElasticBlockStore、GlusterFS)，这就设计到分布式存储和外部存储的一些操作了，后续再讲吧</li>
</ul>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">ReplicationController</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">myweb</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">replicas</span><span class="pi">:</span> <span class="s">5</span>
  <span class="c1"># selector 如果不指定，默认和.spec.template.labels的值相同</span>
  <span class="s">selector</span><span class="pi">:</span>
    <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
  <span class="s">template</span><span class="pi">:</span>
    <span class="s">metadata</span><span class="pi">:</span>
      <span class="s">labels</span><span class="pi">:</span>
        <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
    <span class="s">spec</span><span class="pi">:</span>
      <span class="c1"># 该处声明一个volume</span>
      <span class="s">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
        <span class="s">emptyDir</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="s">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">myweb</span>
        <span class="s">image</span><span class="pi">:</span> <span class="s">kubeguide/tomcat-app:v1</span>
        <span class="s">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">containerPort</span><span class="pi">:</span> <span class="s">8080</span>
        <span class="s">env</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_HOST</span>
          <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">mysql-service'</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_PORT</span>
          <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3306'</span>
        <span class="c1"># 该处进行引用并挂在到容器内部</span>
        <span class="s">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
          <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/mydata-data</span>
</code></pre>
</div>

<h4 id="pod-资源限制">pod 资源限制</h4>

<p>每个pod都能对其能使用的服务器上的资源来进行配额限制，当前能限制的只有CPU和Memory。在kubernetes里面，计算资源的限制主要是设定两个参数</p>
<ul>
  <li>Limits 资源允许使用的最大值，不能超过</li>
  <li>Requests 资源允许使用的最小值，最少必须满足这个需求</li>
</ul>

<p>我们通常将<code class="highlighter-rouge">request</code> 设置为容器平时正常运行时所需的资源，而将<code class="highlighter-rouge">Limits</code>设置为容器峰值负载情况下的最大使用量</p>

<h4 id="label-and-label-selector">label and label selector</h4>

<p>label即标签，是Kubernetes系统中另一个核心概念。一个label是一个<code class="highlighter-rouge">key=value</code>的键值对。label可以附加到各种资源对象上，如Node、Pod、RC、Service等，一个资源对象可以附加无数的label，同一个label也可以附加到无数的资源对象上。</p>

<p>通过指定<code class="highlighter-rouge">label selector</code>来查询和筛选某些拥有label的资源对象，常用到的两种表达式</p>
<ul>
  <li>等式 (key=value)</li>
  <li>集合式 (key in values)</li>
</ul>

<p><code class="highlighter-rouge">label selector</code>在kubernetes中常用的场景如下：</p>
<ul>
  <li>kub-controller-manager通过在RC上定义的<code class="highlighter-rouge">label selector</code>来监控并控制POD的数量</li>
  <li>kube-proxy 通过service上的<code class="highlighter-rouge">label selector</code>来选择对应的pod，自动建立每个service到pod的路由请求，从而实现service的智能负载均衡</li>
  <li>通过NodeSelector，将某些pod调度到指定的node上</li>
</ul>

<p><strong>注意：我们在指定<code class="highlighter-rouge">label selector</code>时，需要和<code class="highlighter-rouge">.spec.template.metadata.labels</code>下的值相同，如果不指定<code class="highlighter-rouge">label selector</code>则默认保持和<code class="highlighter-rouge">.spec.template.metadata.labels</code>的值相同</strong></p>

<h3 id="replicationcontrollerrc">ReplicationController(RC)</h3>
<p>我们将上面那个例子的yaml文件直接拿来解析</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="c1"># api版本，类型，全局唯一名称，这是所有kubernetes yaml文件都需要的</span>
<span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">ReplicationController</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">myweb</span>
<span class="c1"># 定义pod期望数量</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">replicas</span><span class="pi">:</span> <span class="s">5</span>
  <span class="c1"># 用于筛选目标的selector 如果不指定，默认和.spec.template.labels的值相同</span>
  <span class="s">selector</span><span class="pi">:</span>
    <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
  <span class="c1"># 当Pod副本数和期望数不一致时，用于创建新pod的模板</span>
  <span class="s">template</span><span class="pi">:</span>
    <span class="s">metadata</span><span class="pi">:</span>
      <span class="s">labels</span><span class="pi">:</span>
        <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
    <span class="s">spec</span><span class="pi">:</span>
      <span class="c1"># 该处声明一个volume</span>
      <span class="s">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
        <span class="s">emptyDir</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="s">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">myweb</span>
        <span class="s">image</span><span class="pi">:</span> <span class="s">kubeguide/tomcat-app:v1</span>
        <span class="s">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">containerPort</span><span class="pi">:</span> <span class="s">8080</span>
        <span class="s">env</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_HOST</span>
          <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">mysql-service'</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_PORT</span>
          <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3306'</span>
        <span class="c1"># 该处进行引用并挂在到容器内部</span>
        <span class="s">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
          <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/mydata-data</span>
</code></pre>
</div>

<p>我们可以通过命令的形式来动态缩放POD的数量</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>kubectl scale rc mysql --replicas<span class="o">=</span>2

<span class="c"># 删除pod</span>
kubectl delete -f mysql-rc.yml
kubectl scale rc mysql --replicas<span class="o">=</span>0
</code></pre>
</div>

<h3 id="replicaset">ReplicaSet</h3>

<p>ReplicaSet和ReplicationController唯一的区别就是:ReplicaSet支持集合式的<code class="highlighter-rouge">label selector</code>，我们平时很少单独使用<code class="highlighter-rouge">Replica Set</code>，它主要是被<code class="highlighter-rouge">Deployment</code>这个更高层的资源对象使用。</p>

<h3 id="deployment">Deployment</h3>
<p>Deployment在内部使用<code class="highlighter-rouge">Replica Set</code>来实现目的，它管理着<code class="highlighter-rouge">Replica Set</code>，而它管理<code class="highlighter-rouge">Replica Set</code>的主要目的是为了支持版本回滚<code class="highlighter-rouge">Rollback</code></p>

<p>从下面命令所展示出来的命名规则我们不难发现，<code class="highlighter-rouge">Deployment</code>创建的时候创建了<code class="highlighter-rouge">Replica Set</code>，而<code class="highlighter-rouge">Replica Set</code>创建了<code class="highlighter-rouge">Pod</code></p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>kubectl get deployment -l k8s-app<span class="o">=</span>kube-dns -n kube-system
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-dns   2         2         2            2           6d

<span class="gp">$ </span>kubectl get replicaset -l k8s-app<span class="o">=</span>kube-dns -n kube-system
NAME                  DESIRED   CURRENT   READY     AGE
kube-dns-1446441763   2         2         2         6d

<span class="gp">$ </span>kubectl get pods -l k8s-app<span class="o">=</span>kube-dns -n kube-system
NAME                        READY     STATUS    RESTARTS   AGE
kube-dns-1446441763-0th37   3/3       Running   0          6d
kube-dns-1446441763-1w5gx   3/3       Running   0          6d
</code></pre>
</div>

<p>稍后，用实验来说明<code class="highlighter-rouge">Deployment</code>在滚动升级中的作用</p>

<h3 id="pod滚动升级">Pod滚动升级</h3>

<p>在说Deployment的滚动升级之前，我们先来看看<code class="highlighter-rouge">ReplicationController</code>的滚动升级</p>

<p><code class="highlighter-rouge">ReplicationController</code>的滚动升级和<code class="highlighter-rouge">Deployment</code>的滚动升级有所不同，命令都不一样，通过执行<code class="highlighter-rouge">kubectl rolling-update</code>来一键完成，该命令创建了一个新的RC，然后自动控制旧的RC中的POD副本数两逐渐减少到0，同时新RC中的POD数量从0逐步增加到目标值，最终实现POD的升级，滚动升级的配置文件必须满足如下三个条件：</p>
<ul>
  <li><code class="highlighter-rouge">metadata.name</code>必须和旧RC文件中的不同</li>
  <li><code class="highlighter-rouge">spec.selector</code>至少有一个与旧RC的不同(<strong>这里有个BUG，具体参考<a href="http://valleylord.github.io/post/201603-kubernetes-roll/">这里</a></strong>)</li>
  <li><code class="highlighter-rouge">metadata.namespace</code> 命名空间必须的是一样的</li>
</ul>

<p>下面我们将上面的<code class="highlighter-rouge">myweb</code>进行一下升级，内容如下</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">ReplicationController</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">myweb-v2</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">replicas</span><span class="pi">:</span> <span class="s">5</span>
  <span class="s">selector</span><span class="pi">:</span>
    <span class="s">version</span><span class="pi">:</span> <span class="s">v2</span>
    <span class="s">deployment</span><span class="pi">:</span> <span class="s">v2</span>
    <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
  <span class="s">template</span><span class="pi">:</span>
    <span class="s">metadata</span><span class="pi">:</span>
      <span class="s">labels</span><span class="pi">:</span>
        <span class="s">version</span><span class="pi">:</span> <span class="s">v2</span>
        <span class="s">deployment</span><span class="pi">:</span> <span class="s">v2</span>
        <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
    <span class="s">spec</span><span class="pi">:</span>
      <span class="c1"># 该处声明一个volume</span>
      <span class="s">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
        <span class="s">emptyDir</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="s">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">myweb</span>
        <span class="s">image</span><span class="pi">:</span> <span class="s">kubeguide/tomcat-app:v2</span>
        <span class="s">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">containerPort</span><span class="pi">:</span> <span class="s">8080</span>
        <span class="s">env</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_HOST</span>
          <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">mysql-service'</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_PORT</span>
          <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">13306'</span>
        <span class="c1"># 该处进行引用并挂在到容器内部</span>
        <span class="s">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
          <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/mydata-data</span>
</code></pre>
</div>

<p>啊哦，升级的时候报错了</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>error: myweb-rc-update.yml must specify a matching key with non-equal value <span class="k">in </span>Selector <span class="k">for </span>myweb
See <span class="s1">'kubectl rolling-update -h'</span> <span class="k">for </span><span class="nb">help </span>and examples.
</code></pre>
</div>

<p>没关系，既然用yaml文件无法升级，我们用命令的形式来试试</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>kubectl rolling-update myweb --image<span class="o">=</span>kubeguide/tomcat-app:v2
</code></pre>
</div>

<p>OK，成了，开始升级了</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 我们发现旧的rc正在逐步减少，新的rc正在增多</span>
<span class="gp">$ </span>kubectl get rc
NAME                                     DESIRED   CURRENT   READY     AGE
mysql                                    1         1         1         34m
myweb                                    2         2         2         19m
myweb-c1dc64330c885b62eca9fb5aafbfecc6   4         4         4         3m

<span class="c"># 旧的pods也正在一个个的被新的pod替换</span>
<span class="gp">$ </span>kubectl get pods
NAME                                           READY     STATUS    RESTARTS   AGE
mysql-65lw6                                    1/1       Running   0          35m
myweb-c1dc64330c885b62eca9fb5aafbfecc6-88f8v   1/1       Running   0          4m
myweb-c1dc64330c885b62eca9fb5aafbfecc6-c97fh   1/1       Running   0          58s
myweb-c1dc64330c885b62eca9fb5aafbfecc6-ttlpm   1/1       Running   0          2m
myweb-c1dc64330c885b62eca9fb5aafbfecc6-wd6kj   1/1       Running   0          3m
myweb-c1dc64330c885b62eca9fb5aafbfecc6-zr57k   1/1       Running   0          1m
myweb-k9tsj                                    1/1       Running   0          20m

</code></pre>
</div>

<p>成功了～</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 成功后，将RC名称改为myweb，升级完成</span>
Created myweb-c1dc64330c885b62eca9fb5aafbfecc6
Scaling up myweb-c1dc64330c885b62eca9fb5aafbfecc6 from 0 to 5, scaling down myweb from 5 to 0
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 1
Scaling myweb down to 4
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 2
Scaling myweb down to 3
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 3
Scaling myweb down to 2
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 4
Scaling myweb down to 1
Scaling myweb-c1dc64330c885b62eca9fb5aafbfecc6 up to 5
Scaling myweb down to 0
Update succeeded. Deleting old controller: myweb
Renaming myweb-c1dc64330c885b62eca9fb5aafbfecc6 to myweb
replicationcontroller <span class="s2">"myweb"</span> rolling updated
</code></pre>
</div>

<p>那么为什么，我们在上面用yaml文件无法升级呢？</p>

<p>经过一段时间的折腾，我发现，我旧的RC文件只有一个label，app=myweb;而当我用命令升级成功后,新的RC有了两个label，app=myweb和deployment=xxxxx</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 未升级之前的RC</span>
<span class="gp">$ </span>kubectl get rc -o wide
NAME      DESIRED   CURRENT   READY     AGE       CONTAINER<span class="o">(</span>S<span class="o">)</span>   IMAGE<span class="o">(</span>S<span class="o">)</span>                  SELECTOR
mysql     1         1         1         48m       mysql          mysql                     <span class="nv">app</span><span class="o">=</span>mysql
myweb     5         5         5         8s        myweb          kubeguide/tomcat-app:v1   <span class="nv">app</span><span class="o">=</span>myweb

<span class="c"># 升级后的RC</span>
<span class="gp">$ </span>kubectl get rc -o wide
NAME       DESIRED   CURRENT   READY     AGE       CONTAINER<span class="o">(</span>S<span class="o">)</span>   IMAGE<span class="o">(</span>S<span class="o">)</span>                  SELECTOR
mysql      1         1         1         43m       mysql          mysql                     <span class="nv">app</span><span class="o">=</span>mysql
myweb      4         4         4         6m        myweb          kubeguide/tomcat-app:v2   <span class="nv">app</span><span class="o">=</span>myweb,deployment<span class="o">=</span>c1dc64330c885b62eca9fb5aafbfecc6
</code></pre>
</div>

<p>然后当我再次对这个新的RC进行升级的时候，我发现是可以用yaml文件升级的，这说明什么？是不是旧的RC文件至少需要两个label才能用yaml文件升级呢？我们试试再说</p>

<ul>
  <li>为myweb添加不少于一个的label
    <div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="c1"># api版本，类型，全局唯一名称，这是所有kubernetes yaml文件都需要的</span>
<span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">ReplicationController</span>
<span class="s">metadata</span><span class="pi">:</span>
<span class="s">name</span><span class="pi">:</span> <span class="s">myweb</span>
<span class="c1"># 定义pod期望数量</span>
<span class="s">spec</span><span class="pi">:</span>
<span class="s">replicas</span><span class="pi">:</span> <span class="s">5</span>
<span class="c1"># 用于筛选目标的selector 如果不指定，默认和.spec.template.labels的值相同</span>
<span class="s">selector</span><span class="pi">:</span>
  <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
  <span class="s">version</span><span class="pi">:</span> <span class="s">v1</span>
<span class="c1"># 当Pod副本数和期望数不一致时，用于创建新pod的模板</span>
<span class="s">template</span><span class="pi">:</span>
  <span class="s">metadata</span><span class="pi">:</span>
    <span class="s">labels</span><span class="pi">:</span>
      <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
      <span class="s">version</span><span class="pi">:</span> <span class="s">v1</span>
  <span class="s">spec</span><span class="pi">:</span>
    <span class="c1"># 该处声明一个volume</span>
    <span class="s">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
      <span class="s">emptyDir</span><span class="pi">:</span> <span class="pi">{}</span>
    <span class="s">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">myweb</span>
      <span class="s">image</span><span class="pi">:</span> <span class="s">kubeguide/tomcat-app:v1</span>
      <span class="s">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">containerPort</span><span class="pi">:</span> <span class="s">8080</span>
      <span class="s">env</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_HOST</span>
        <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">mysql-service'</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_PORT</span>
        <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3306'</span>
      <span class="c1"># 该处进行引用并挂在到容器内部</span>
      <span class="s">volumeMounts</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
        <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/mydata-data</span>
</code></pre>
    </div>
  </li>
  <li>新建一个用来升级myweb的yaml文件</li>
</ul>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">ReplicationController</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">myweb-v2</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">replicas</span><span class="pi">:</span> <span class="s">5</span>
  <span class="s">selector</span><span class="pi">:</span>
    <span class="s">version</span><span class="pi">:</span> <span class="s">v2</span>
    <span class="s">deployment</span><span class="pi">:</span> <span class="s">v2</span>
    <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
  <span class="s">template</span><span class="pi">:</span>
    <span class="s">metadata</span><span class="pi">:</span>
      <span class="s">labels</span><span class="pi">:</span>
        <span class="s">version</span><span class="pi">:</span> <span class="s">v2</span>
        <span class="s">deployment</span><span class="pi">:</span> <span class="s">v2</span>
        <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>
    <span class="s">spec</span><span class="pi">:</span>
      <span class="c1"># 该处声明一个volume</span>
      <span class="s">volumes</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
        <span class="s">emptyDir</span><span class="pi">:</span> <span class="pi">{}</span>
      <span class="s">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">myweb</span>
        <span class="s">image</span><span class="pi">:</span> <span class="s">kubeguide/tomcat-app:v2</span>
        <span class="s">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">containerPort</span><span class="pi">:</span> <span class="s">8080</span>
        <span class="s">env</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_HOST</span>
          <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">mysql-service'</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">MYSQL_SERVICE_PORT</span>
          <span class="s">value</span><span class="pi">:</span> <span class="s1">'</span><span class="s">13306'</span>
        <span class="c1"># 该处进行引用并挂在到容器内部</span>
        <span class="s">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">datavol</span>
          <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/mydata-data</span>
</code></pre>
</div>

<p>我发现，真的可以升级了!!!</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>kubectl rolling-update myweb -f myweb-rc-update.yml
Created myweb-v2
Scaling up myweb-v2 from 0 to 5, scaling down myweb from 5 to 0
Scaling myweb-v2 up to 1
Scaling myweb down to 4
Scaling myweb-v2 up to 2
Scaling myweb down to 3
Scaling myweb-v2 up to 3
Scaling myweb down to 2
Scaling myweb-v2 up to 4
</code></pre>
</div>

<p><strong>这说明什么？这说明，以后如果你要用<code class="highlighter-rouge">ReplicationController</code>的时候，至少要给他指定不少于2个的label，否则，你无法用yaml来进行升级，这真的是很蛋疼的一件事。</strong></p>

<p>当然了，保不齐以后kubernetes会将<code class="highlighter-rouge">ReplicationController</code>抛弃掉，毕竟现在它已经有了更好的<code class="highlighter-rouge">Deployment</code>和<code class="highlighter-rouge">StatefulSet</code>等…</p>

<h3 id="horizontal-pod-autoscaler-hpa">Horizontal Pod Autoscaler (HPA)</h3>

<p>前面我们说到能用<code class="highlighter-rouge">kubectl scale</code>命令来手动实现 Pod 的扩容和缩容，这未免也太 low 了吧，我们既然用了kubernetes，那就是因为他的智能化，自动化。所以这里我们要讲讲kubernetes的智能扩容 HPA。</p>

<p>HPA 基于获取到的metrics value(CPU utilization,custom metrics),对RC,Deployment管理的pods进行自动伸缩。HPA是kubernetes <code class="highlighter-rouge">autoscaling</code> API组中的一个API资源，当前的stable版本只支持CPU，alpha版本中红，已经开始支持memory和custom metrics。</p>

<p>HPA 以kubernetes API resource 和一个controller来实现，resource决定了controller的行为，而controller控制着pods的数量。</p>

<blockquote>
  <p>截至到kubernetes 1.6 ，Release特性中仅支持CPU utilization这一 <code class="highlighter-rouge">resource metrics</code>, 对<code class="highlighter-rouge">custom metrics</code>的支持目前仍在alpha阶段。</p>
</blockquote>

<p>HPA controller 周期性的调整对应rc，deployment中的pods数量，使得获取到的<code class="highlighter-rouge">metrics value</code>能匹配用户指定的<code class="highlighter-rouge">target utilization</code>。这个周期默认为30s，可以通过<code class="highlighter-rouge">kube-controller-manager</code>的flag <code class="highlighter-rouge">--horizontal-pod-autoscaler-sync-period</code>进行设置。</p>

<p>在每个HPA Controller的处理周期中，kube-controller-manager都去查询HPA获取到的metrics的utilization。查询方式根据metric类型不同而不同：</p>

<p>如果metric type是resource metrics，则通过resource metrics API查询，直接通过Heapster访问
如果metric type属于custom metrics，则通过custom metrics API查询，通过REST client来访问</p>

<p>计算伸缩比例算法：</p>

<ul>
  <li>对于 resource metrics,比如 CPU，HPA controller 从resource metrics API中获取CPU metrics，如果HPA中设定了target utilization，则HPA controller 会将获取到的CPU metrics 除以对应容器的resource request值作为检测到的当前pod的resource utilization。如此计算完所有HPA对应的pods后，对该resource utilization values取平均值。最后将平均值除与定义的target utilization，得到伸缩比例。</li>
</ul>

<blockquote>
  <p>注意：如果HPA对应的某些pods中的容器没有定义resource request，则HPA不会对这些pods进行scale</p>
</blockquote>

<ul>
  <li>
    <p>对于custome metrics，HPA Controller的伸缩算法几乎与resource metrics一样，不同的是：此时是根据custome metrics API查询到的metrics value对比target metrics value计算得到的，而不是通过utilization计算得到的</p>
  </li>
  <li>
    <p>对于object metrics，HPA Controller获取到一个metric 值，然后与target metrics比较，得到如上所说的比率</p>
  </li>
</ul>

<p>HPA可以通过命令来实现，也可以通过配置文件的方式来实现。</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>kubectl autoscale deployment php-apache --cpu-percent<span class="o">=</span>50 --min<span class="o">=</span>1 --max<span class="o">=</span>10
deployment <span class="s2">"php-apache"</span> autoscaled
</code></pre>
</div>

<p>下面我们用实验来感受下HPA</p>

<p>首先我们来新建一个<code class="highlighter-rouge">php-apache</code>的服务</p>

<p>php-apache-deploy.yml</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">php-apache</span>
  <span class="s">namespace</span><span class="pi">:</span> <span class="s">default</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">replicas</span><span class="pi">:</span> <span class="s">1</span>
  <span class="s">selector</span><span class="pi">:</span>
    <span class="s">matchLabels</span><span class="pi">:</span>
      <span class="s">k8s-quark</span><span class="pi">:</span> <span class="s">php-apache</span>
  <span class="s">template</span><span class="pi">:</span>
    <span class="s">metadata</span><span class="pi">:</span>
      <span class="s">labels</span><span class="pi">:</span>
        <span class="s">k8s-quark</span><span class="pi">:</span> <span class="s">php-apache</span>
    <span class="s">spec</span><span class="pi">:</span>
      <span class="s">containers</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">php-apache</span>
        <span class="s">image</span><span class="pi">:</span> <span class="s">gcr.io/google_containers/hpa-example:latest</span>
        <span class="s">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
        <span class="s">ports</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">containerPort</span><span class="pi">:</span> <span class="s">80</span>
        <span class="s">resources</span><span class="pi">:</span>
          <span class="s">requests</span><span class="pi">:</span>
            <span class="s">cpu</span><span class="pi">:</span> <span class="s">200m</span>
</code></pre>
</div>

<p>php-apache-svc.yml</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">php-apache</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">port</span><span class="pi">:</span> <span class="s">80</span>
  <span class="s">selector</span><span class="pi">:</span>
    <span class="s">k8s-quark</span><span class="pi">:</span> <span class="s">php-apache</span>
</code></pre>
</div>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>kubectl create -f php-apache-deploy.yml
kubectl create -f php-apache-svc.yml

<span class="c"># 查看当前的deployment</span>
<span class="gp">$ </span>kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   1         1         1            1           10m

<span class="c"># 查看php-apache的pods</span>
<span class="gp">$ </span>kubectl get pods |grep php-apache
php-apache-3548797493-twq7k       1/1       Running   0          17m
</code></pre>
</div>

<p>然后，我们用命令来创建一个HPA</p>
<div class="language-bash highlighter-rouge"><pre class="highlight"><code>kubectl autoscale deployment php-apache --cpu-percent<span class="o">=</span>50 --min<span class="o">=</span>1 --max<span class="o">=</span>10

<span class="c"># 看看我们创建好的hpa</span>
<span class="gp">$  </span>kubectl get hpa
NAME         REFERENCE               TARGETS    MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   0% / 50%   1         10        1          19m

</code></pre>
</div>
<p>这条命令的意思是，我们为<code class="highlighter-rouge">php-apache</code>创建了一个HPA，指定了target metrics value是cpu利用率50%，而伸缩最小值为1，最大值为10</p>

<p>最后，我们来持续访问<code class="highlighter-rouge">php-apache</code>来给它压力，看看HPA会不会自动为我们扩容呢？</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 进入一个容器</span>
kubectl run -ti load-generator --image<span class="o">=</span>busybox /bin/sh

<span class="c"># 持续访问</span>
<span class="k">while </span><span class="nb">true</span>; <span class="k">do </span>wget -q -O- http://php-apache.default.svc.cluster.local; <span class="k">done</span>
</code></pre>
</div>

<p>过了几分钟，我们看看结果咋样</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># HPA状态</span>
<span class="gp">$  </span>kubectl get hpa
NAME         REFERENCE               TARGETS      MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   313% / 50%   1         10        4          22m

<span class="c"># deployment状态</span>
<span class="gp">$  </span>kubectl get deployment php-apache
NAME         DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
php-apache   4         4         4            4           25m

<span class="c"># HPA状态</span>
<span class="gp">$  </span>kubectl get hpa
NAME         REFERENCE               TARGETS     MINPODS   MAXPODS   REPLICAS   AGE
php-apache   Deployment/php-apache   90% / 50%   1         10        8          26m

</code></pre>
</div>

<p>我们发现，随着我们的持续访问增压，HPA会自动的为我们将<code class="highlighter-rouge">php-apache</code>进行扩容，随着<code class="highlighter-rouge">php-apahce</code>的扩容，CPU开始慢慢下降，直到最终符合我们指定的低于50%的标准，或者达到最大值10个POD。而当我们停止对<code class="highlighter-rouge">php-apache</code>的访问，最终，HPA会恢复到默认1个pod的状态。</p>

<p>用yaml文件的方式，最终的效果和上面是一样的</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">autoscaling/v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">HorizontalPodAutoscaler</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">php-apache</span>
  <span class="s">namespace</span><span class="pi">:</span> <span class="s">default</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="c1"># 指定针对谁来使用HPA</span>
  <span class="s">scaleTargetRef</span><span class="pi">:</span>
    <span class="s">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
    <span class="s">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">php-apache</span>
  <span class="s">minReplicas</span><span class="pi">:</span> <span class="s">1</span>
  <span class="s">maxReplicas</span><span class="pi">:</span> <span class="s">10</span>
  <span class="c1"># 用户定义的CPU利用率</span>
  <span class="s">targetCPUUtilizationPercentage</span><span class="pi">:</span> <span class="s">50</span>
</code></pre>
</div>

<p>至于其它的HPA metrica，改天再讲吧，毕竟现在还只是alpha版本，而且需要heapster目前也无法收集那么多metrics</p>

<h3 id="service">Service</h3>

<p><img src="/images/posts/service-rc-pod.png" alt="" /></p>

<p>透过上图，我们可以发现，service定义了一个服务的访问入口地址，前端的pod通过这个入口地址来访问其背后一组由pod组成的集群实例，而背后这组pod则是通过RC来生成并保持住的。他们三者之间，通过<code class="highlighter-rouge">label selector</code>来保持关联。</p>

<p>我们知道，正常情况下，要通过一个如果访问后端的集群服务，最好的办法是在前端弄一个负载均衡(nginx,haproxy…)，暴露一个对外服务的端口，然后反代到后端的ip+port。而kubernetes也遵循了这样的做法。
在上图中，frontal pod 访问 service 时，kubernetes其实是通过其内部的 kube-proxy 来进行负载均衡，然后将请求转发到后端的某个pod上。但kubernetes不是使用的一个实际的负载均衡IP地址，而是为每个service分配了一个全局唯一的虚拟IP地址，这个虚拟IP被称为Cluster IP，只能在kubernetes 集群内部被访问(意思就是只能在集群内的pod中才能访问)，而且一旦创建，在service的整个生命周期内，都不会发生变动。</p>

<p>下面我们来创建一个service看看</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="c1"># service全局唯一名称，后面在cluster中可以直接使用的名称</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">tomcat-service</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">ports</span><span class="pi">:</span>
    <span class="c1"># service提供服务的端口</span>
  <span class="pi">-</span> <span class="s">port</span><span class="pi">:</span> <span class="s">8080</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">service-port</span>
  <span class="c1"># 后端容器提供服务暴露的端口，如果不指定，默认暴露service提供服务的端口</span>
    <span class="s">targetPort</span><span class="pi">:</span> <span class="s">8080</span>
  <span class="pi">-</span> <span class="s">port</span><span class="pi">:</span> <span class="s">8005</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">shutown-port</span>
    <span class="s">targetPort</span><span class="pi">:</span> <span class="s">8005</span>
  <span class="c1"># 指定label selector 确认该服务和后端那些pod关联起来</span>
  <span class="s">selector</span><span class="pi">:</span>
    <span class="s">app</span><span class="pi">:</span> <span class="s">mysql</span>
</code></pre>
</div>

<h4 id="kubernetes服务发现机制">kubernetes服务发现机制</h4>

<p>最开始的时候kubernetes使用变量的形式，来发现服务，后来使用内部DNS来进行服务名解析，还有人认为使用consul的服务发现机制，其实我并不觉得这比内部的DNS服务发现好多少。</p>

<h5 id="kubernetes内部dns寻址服务发现">kubernetes内部dns寻址服务发现</h5>

<p>1.kubectl 执行创建的时候会向APIServer请求创建一个service。APIServer获取到请求后调用相应的api创建一个service对象，并写入etcd保存。
2.kube-dns通过<code class="highlighter-rouge">list/watch</code>操作向APIServer发送GET请求。这时因为有service的创建，所以APIServer会相应这个请求并把service回复给kube-dns。
3.APIServer将创建的service信息回复给kube-dns,还会附带一个APIServer分配给service的Cluster IP。
4.kube-dns通过检测并得到APIServer回复的service信息，会生成DNS条目，并把这个DNS条目存储到内存(Tree-Cache)中
5.kubernetes中访问service的时候，会先去dnsmasq中查找缓存，找不到则去kubedns中查找dns条目，最终实现service的解析。(sidecar是用于检查其他两个容器的健康状态)</p>

<p>通过dns的服务发现机制，有个弊端就是服务的健康检查，不过这一点通过pod的健康检查可以填补。</p>

<h5 id="consuletcd等服务发现机制">consul、etcd等服务发现机制</h5>

<p>consul：这个具体还没实施过，大致意思就是，容器启动时注册自己的ip+port到consul，然后consul自己做健康检查，最终将其发往fabio，fabio是个大路由，前端统一反代到fabio。
etcd：大体实现方式，就是写脚本通过etcd的api注册服务，然后再写一个service discover的脚本循环查询注册进去的service，对比template中的内容，然后生成新的配置文件进行更新。</p>

<h4 id="kubernetes-service-暴露">kubernetes service 暴露</h4>

<p>目前kubernetes service 暴露的方式有如下几种</p>

<ul>
  <li>ClusterIP 只提供kubernetes集群内部的服务发现</li>
  <li>NodePort 在每个节点上提供端口暴露服务</li>
  <li>LoadBlancer 只能在云平台上使用，使用云平台提供的LB来暴露服务</li>
  <li>ExternalName 与另一个域名绑定，通过该service访问另一个服务</li>
  <li>ingress/traefik 使用第三方插件将pod暴露出来</li>
</ul>

<p>而无论是ClusterIP 还是 NodePort 都是通过kube-proxy来对service进行实现的，而kube-proxy又有两种方式来实现负载，userspace和iptables，下面我来说一下kubernetes默认的iptables方式的kube-proxy</p>

<p>首先，我们来新建一个NodePort类型的服务</p>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">myweb-service</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">type</span><span class="pi">:</span> <span class="s">NodePort</span>
  <span class="s">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">port</span><span class="pi">:</span> <span class="s">8080</span>
    <span class="s">nodePort</span><span class="pi">:</span> <span class="s">30001</span>
  <span class="s">selector</span><span class="pi">:</span>
    <span class="s">app</span><span class="pi">:</span> <span class="s">myweb</span>         
</code></pre>
</div>

<p>myweb-service 代理了后端的一个pod，ip为10.233.88.53，看看iptables</p>

<p>下面来逐条分析</p>

<p>如果是通过node的30001来访问，则会进入如下的链</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 看看和NodePort  30001有关的iptables</span>

<span class="gp">$ </span>iptables -S -t nat |grep 30001
-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="s2">"default/myweb-service:"</span> -m tcp --dport 30001 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="s2">"default/myweb-service:"</span> -m tcp --dport 30001 -j KUBE-SVC-KINM4OXG42E5QTAT
</code></pre>
</div>

<p>然后进一步跳转到<code class="highlighter-rouge">KUBE-SVC-KINM4OXG42E5QTAT</code>的链</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>-A KUBE-SVC-KINM4OXG42E5QTAT -m comment --comment <span class="s2">"default/myweb-service:"</span> -j KUBE-SEP-I4OJ7A6SXM5YG2QP
</code></pre>
</div>

<p>然后会跳转到<code class="highlighter-rouge">KUBE-SEP-I4OJ7A6SXM5YG2QP</code>链，最终将请求转发到10.233.88.53的pod上</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>-A KUBE-SEP-I4OJ7A6SXM5YG2QP -p tcp -m comment --comment <span class="s2">"default/myweb-service:"</span> -m tcp -j DNAT --to-destination 10.233.88.53:8080
</code></pre>
</div>

<blockquote>
  <p><strong>注意：如果service代理了多个pod的话，会利用iptables的–probability特性，按一定的比例转发，如下</strong></p>
</blockquote>

<p>假若我们的myweb-service代理了3个pod</p>

<p>如果是通过node的30001来访问，则会进入如下的链</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 看看和NodePort  30001有关的iptables</span>

<span class="gp">$ </span>iptables -S -t nat |grep 30001
-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="s2">"default/myweb-service:"</span> -m tcp --dport 30001 -j KUBE-MARK-MASQ
-A KUBE-NODEPORTS -p tcp -m comment --comment <span class="s2">"default/myweb-service:"</span> -m tcp --dport 30001 -j KUBE-SVC-KINM4OXG42E5QTAT
</code></pre>
</div>

<p>然后进一步跳转到<code class="highlighter-rouge">KUBE-SEP-5I5KUCBAI2CKFMN2</code>、<code class="highlighter-rouge">KUBE-SEP-I4OJ7A6SXM5YG2QP</code>、<code class="highlighter-rouge">KUBE-SEP-FDQHGZ7N6PHRDJRL</code>的链</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 有分为30%，50%，20%的几率</span>
-A KUBE-SVC-KINM4OXG42E5QTAT -m comment --comment <span class="s2">"default/myweb-service:"</span> -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-5I5KUCBAI2CKFMN2

-A KUBE-SVC-KINM4OXG42E5QTAT -m comment --comment <span class="s2">"default/myweb-service:"</span> -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-I4OJ7A6SXM5YG2QP

-A KUBE-SVC-KINM4OXG42E5QTAT -m comment --comment <span class="s2">"default/myweb-service:"</span> -j KUBE-SEP-FDQHGZ7N6PHRDJRL
</code></pre>
</div>

<p>然后会分别跳到上面每个链下对应的链，最终转发到对应的pod上</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># KUBE-SEP-5I5KUCBAI2CKFMN2</span>
-A KUBE-SEP-5I5KUCBAI2CKFMN2 -p tcp -m comment --comment <span class="s2">"default/myweb-service:"</span> -m tcp -j DNAT --to-destination 10.233.87.108:8080

<span class="c"># KUBE-SEP-I4OJ7A6SXM5YG2QP</span>
-A KUBE-SEP-I4OJ7A6SXM5YG2QP -p tcp -m comment --comment <span class="s2">"default/myweb-service:"</span> -m tcp -j DNAT --to-destination 10.233.88.53:8080

<span class="c"># KUBE-SEP-FDQHGZ7N6PHRDJRL</span>
-A KUBE-SEP-FDQHGZ7N6PHRDJRL -p tcp -m comment --comment <span class="s2">"default/myweb-service:"</span> -m tcp -j DNAT --to-destination 10.233.96.254:8080
</code></pre>
</div>

<p>好了，说完了NodePort，然后我们再说ClusterIP</p>

<p>继续从上面看到尾就行了。</p>

<p>ingress和traefik留到后面单独留个篇幅来说吧，这里就不说了。</p>

<h3 id="volume">Volume</h3>

<p>kubernetes 的volume要说的内容就很多了。</p>

<p>首先volume是一个在pod中能被多个容器访问的共享目录，它是定义在pod上，然后挂载到容器下，而且它的生命周期只和pod有关。常见的存储类型有如下几种</p>

<ul>
  <li>emptyDir</li>
  <li>hostPath</li>
  <li>GCE Persistent Disks</li>
  <li>NFS</li>
  <li>RBD</li>
  <li>ISCSCI</li>
  <li>AWS ElasticBlockStore</li>
  <li>GlusterFS</li>
  <li>secret</li>
  <li>PersistentVolume</li>
  <li>downwardAPI</li>
  <li>projected</li>
  <li>configmap</li>
  <li>local</li>
</ul>

<p><strong>注意：gitRepo实际上也是挂载一个空目录，从GIT仓库中clone内容下来供pod使用，所以它的数据也无法永久保存</strong></p>

<h4 id="emptydir">emptyDir</h4>

<p>正如它的名字一样，这是一个空的目录，它是在Pod被分配到node节点上的时候被创建的，无需在宿主机node上指定对应的目录文件。而且当pod从节点上被删除之后，emptyDir中的数据也会被删除，emptyDir 一般用在如下几个地方</p>

<ul>
  <li>暂存空间，例如用于基于磁盘的归并排序或长计算的检查点的暂存空间</li>
  <li>临时目录，临时储存那些无需持久化的数据</li>
</ul>

<p>默认情况下，<code class="highlighter-rouge">emptyDir</code> 数据存储在SSD或者网络存储上。但是，你也可以设置<code class="highlighter-rouge">emptyDir.medium</code>为<code class="highlighter-rouge">Memory</code>来启用<code class="highlighter-rouge">tmpfs</code>,tmpfs会将数据写入到内存中，因此，当机器重启之后，数据也会永久删除，并且，因为是存放在内存中，这些数据会占用你容器内存的limit指标。</p>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">test-pd</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">test-container</span>
    <span class="s">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/cache</span>
      <span class="s">name</span><span class="pi">:</span> <span class="s">cache-volume</span>
  <span class="s">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">cache-volume</span>
    <span class="s">emptyDir</span><span class="pi">:</span>
      <span class="s">medium</span><span class="pi">:</span> <span class="s">Memory</span>
</code></pre>
</div>

<h4 id="secret">secret</h4>

<p>secret volume 使用来传递敏感信息，比如说密码之类的。我们可以将在kubernetes中定义的secret直接挂载为文件让pod访问。secret volume实际是通过tmpfs(内存文件系统)来实现的，所以这些信息不会持久保存。</p>

<h4 id="downwardapi">downwardAPI</h4>

<p>如果你想将pod或container里面的字段暴露给其他正在运行的容器，那么downwardAPI正是你所需要的，它会将pod或container里面的字段以文件的形式存储下来，然后挂载到对应的容器中。</p>

<p>目前能暴露的字段</p>
<ul>
  <li>node’s name</li>
  <li>pod’s name</li>
  <li>pod’s namespace</li>
  <li>pod’s ip</li>
  <li>pod’s serviceAccount name</li>
  <li>pod’s UID</li>
  <li>pod’s labels</li>
  <li>pod’s annotations</li>
  <li>容器 CPU limit</li>
  <li>容器 CPU request</li>
  <li>容器 memory limit</li>
  <li>容器 memory request</li>
</ul>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">kubernetes-downwardapi-volume-example</span>
  <span class="s">labels</span><span class="pi">:</span>
    <span class="s">zone</span><span class="pi">:</span> <span class="s">us-est-coast</span>
    <span class="s">cluster</span><span class="pi">:</span> <span class="s">test-cluster1</span>
    <span class="s">rack</span><span class="pi">:</span> <span class="s">rack-22</span>
  <span class="s">annotations</span><span class="pi">:</span>
    <span class="s">build</span><span class="pi">:</span> <span class="s">two</span>
    <span class="s">builder</span><span class="pi">:</span> <span class="s">john-doe</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">client-container</span>
      <span class="s">image</span><span class="pi">:</span> <span class="s">gcr.io/google_containers/busybox</span>
      <span class="s">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">sh"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">-c"</span><span class="pi">]</span>
      <span class="s">args</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">while true; do</span>
          <span class="s">if [[ -e /etc/labels ]]; then</span>
            <span class="s">echo -en '\n\n'; cat /etc/labels; fi;</span>
          <span class="s">if [[ -e /etc/annotations ]]; then</span>
            <span class="s">echo -en '\n\n'; cat /etc/annotations; fi;</span>
          <span class="s">sleep 5;</span>
        <span class="s">done;</span>
      <span class="s">volumeMounts</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">podinfo</span>
          <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/etc</span>
          <span class="s">readOnly</span><span class="pi">:</span> <span class="s">false</span>
  <span class="s">volumes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">podinfo</span>
      <span class="s">downwardAPI</span><span class="pi">:</span>
        <span class="s">items</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">labels"</span>
            <span class="s">fieldRef</span><span class="pi">:</span>
              <span class="s">fieldPath</span><span class="pi">:</span> <span class="s">metadata.labels</span>
          <span class="pi">-</span> <span class="s">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">annotations"</span>
            <span class="s">fieldRef</span><span class="pi">:</span>
              <span class="s">fieldPath</span><span class="pi">:</span> <span class="s">metadata.annotations</span>
</code></pre>
</div>

<h4 id="projected">projected</h4>

<p>projected volume 可以将几个volume内容隐射到同样的目录中，当前只支持secret、configmap、downwardAPI</p>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">volume-test</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">container-test</span>
    <span class="s">image</span><span class="pi">:</span> <span class="s">busybox</span>
    <span class="s">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">all-in-one</span>
      <span class="s">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/projected-volume"</span>
      <span class="s">readOnly</span><span class="pi">:</span> <span class="s">true</span>
  <span class="s">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">all-in-one</span>
    <span class="s">projected</span><span class="pi">:</span>
      <span class="s">sources</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">secret</span><span class="pi">:</span>
          <span class="s">name</span><span class="pi">:</span> <span class="s">mysecret</span>
          <span class="s">items</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">key</span><span class="pi">:</span> <span class="s">username</span>
              <span class="s">path</span><span class="pi">:</span> <span class="s">my-group/my-username</span>
      <span class="pi">-</span> <span class="s">downwardAPI</span><span class="pi">:</span>
          <span class="s">items</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">labels"</span>
              <span class="s">fieldRef</span><span class="pi">:</span>
                <span class="s">fieldPath</span><span class="pi">:</span> <span class="s">metadata.labels</span>
            <span class="pi">-</span> <span class="s">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">cpu_limit"</span>
              <span class="s">resourceFieldRef</span><span class="pi">:</span>
                <span class="s">containerName</span><span class="pi">:</span> <span class="s">container-test</span>
                <span class="s">resource</span><span class="pi">:</span> <span class="s">limits.cpu</span>
      <span class="pi">-</span> <span class="s">configMap</span><span class="pi">:</span>
          <span class="s">name</span><span class="pi">:</span> <span class="s">myconfigmap</span>
          <span class="s">items</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="s">key</span><span class="pi">:</span> <span class="s">config</span>
              <span class="s">path</span><span class="pi">:</span> <span class="s">my-group/my-config</span>
</code></pre>
</div>

<h4 id="local">local</h4>

<p>local volume在1.7中目前还是alpha版本，主要是用来将本地的disk，分区或者目录进行挂载。local volume只能以静态创建的PV使用。相对于HostPath，localhost可以直接以持久化的方式使用（它总是通过NodeAffinity调度在某个指定的节点上），而hostpath是无法直接以pv来使用的。</p>

<h4 id="hostpath">hostPath</h4>

<p>hostPath 就是将宿主机上的目录或文件挂在到pod里。比如我们常用到<code class="highlighter-rouge">hostPath</code>的几个例子</p>

<ul>
  <li>容器应用程序的日志，需要永久保存时，可以使用hostPath隐射宿主机上的高速文件存储</li>
  <li>运行的容器需要访问Docker内部结构：使用hostPath映射/var/lib/docker</li>
  <li>在容器中运行cAdvisor，使用hostPath映射/dev/cgroups</li>
</ul>

<p>而当我们在使用hostPath的时候需要注意以下几点</p>

<ul>
  <li>在不同Node上具有相同配置的pod(通过podTemplate创建的)，可能会因为宿主机上的目录和文件不同而导致volume上目录和文件的访问结果不一致</li>
  <li>如果使用资源配额，无法将hostPath在宿主机上使用的资源纳入管理</li>
  <li>如果宿主机上的目录是root权限，那么你也必须以root身份来运行你的进程，或者，修改你的目录权限以便于能让hostPath卷有写权限</li>
</ul>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">test-pd</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">test-container</span>
    <span class="s">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/test-pd</span>
      <span class="s">name</span><span class="pi">:</span> <span class="s">test-volume</span>
  <span class="s">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">test-volume</span>
    <span class="s">hostPath</span><span class="pi">:</span>
      <span class="s">path</span><span class="pi">:</span> <span class="s">/data</span>
</code></pre>
</div>

<p>其实hostPath也能算是持久存储的一种，只不过局限性太大了。这里我们详细的讲讲外部存储和分布式存储在kubernetes中的使用(aws和gce的就不讲了，没环境)。</p>

<p><a href="https://github.com/kubernetes/kubernetes/tree/master/examples/volumes">详细的例子</a>大家可以参考官方的例子</p>

<h4 id="nfs">NFS</h4>

<p>NFS是Network File System的缩写，即网络文件系统。kubernetes中通过简单的配置就可以挂载NFS到Pod中，而NFS中的数据是可以永久保存的，同时NFS支持同时写操作。</p>

<p>首先，你得有个已经搭建好的NFS服务</p>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">test-pd</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">test-container</span>
    <span class="s">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/test-pd</span>
      <span class="s">name</span><span class="pi">:</span> <span class="s">test-volume</span>
  <span class="s">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">test-volume</span>
    <span class="s">nfs</span><span class="pi">:</span>
      <span class="s">server</span><span class="pi">:</span> <span class="s">163.xx.xx.xx</span>
      <span class="s">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/"</span>
</code></pre>
</div>

<h4 id="persistent-volume">persistent volume</h4>

<p>事实上，我们可以单独在Pod中指定外部存储，也可以将这些外部存使用PersistentVolume资源化。</p>

<p>之前我们提到的volume都是定义在pod上的，和 pod 是一种静态绑定关系，属于”计算资源”的一部分，而实际上，“网络存储”是独立于“计算资源”之外而存在的一种实体资源。比如我们在使用虚机的情况下，我们通常会定义一个网络存储，然后从中划出一定的空间连接到虚拟机。而persistent volume和与之关联的persistent volume claim就起到了类似的作用。</p>

<p>PersistentVolume(PV)是集群中的一块网络存储，用来提供网络存储资源 。跟Node一样，也是集群的资源。PV 跟volume类似，不过会有独立于Pod的生命周期。
PersistentVolumeClaim(PVC)是对PV的请求。PVC有点类似于Pod，pod消耗node的资源，而PVC消耗PV的资源。Pod请求CPU和内存，而PVC请求特定大小和访问模式的数据卷。</p>

<p>PV的访问模式有三种：</p>

<ul>
  <li>ReadWriteOnce(RWO):最基本的访问方式，可读可写，但只支持被耽搁pod挂载</li>
  <li>ReadOnlyMany(ROX):可以以只读的方式被多个pod挂载</li>
  <li>ReadWriteMany(RWX):以读写的方式被多个POD挂载。</li>
</ul>

<p>并不是所有存储都支持这三种方式，像共享方式，目前支持的还比较少，比较常用的是NFS。在PVC绑定PV时通常根据两个条件来绑定，一个实存储大小，一个是访问方式。
<strong>需要注意的是，虽然PV支持三种访问模式，但它同时只支持一种方式来访问PV</strong></p>

<h5 id="创建pv的方式有两种">创建PV的方式有两种</h5>

<p>有两种创建PV的方式：静态和动态</p>

<h6 id="静态">静态</h6>

<p>所谓静态，就是管理员手动创建一堆PV，组成一个PV池，供PVC来绑定。</p>

<h6 id="动态">动态</h6>
<p>经过API抽象，用户可以通过PVC使用存储资源，通常用户还会关心PV的很多属性，例如对不同的应用场景需要不同的性能，仅仅提供存储大小和访问模式不能满足要求。集群管理员一方面要提供不同PV的多种属性，一方面要隐藏底层的细节，还有一点是不再需要管理员手动去创建PV,这就引入了<code class="highlighter-rouge">StorageClass</code>资源。管理员用存储级别StorageClass描述存储的分类，不同的分类可以对应不同的质量服务Qos等级、备份策略和其他自定义的策略。kubernetes本身不参与存储级别的划分，StorageClass概念在有的存储系统里被称为”profiles”</p>

<p>所谓动态，就是当所有的静态PV都不匹配用户的PVC时，集群通过storageClass的对象由存储系统根据PVC的要求自动创建。这种基于<code class="highlighter-rouge">StorageClass</code>的PV，管理员必须事先创建和配置这样的storage class。请求等级配置为<code class="highlighter-rouge">" "</code>的PVC，有效地禁用了它自身的动态供给功能。</p>

<h6 id="class">class</h6>

<p>我们可以根据不同的需求创建不同类型的PV(这种PV是基于<code class="highlighter-rouge">StorageClass</code>，是自动创建的)，然后我们可以通过为PVC指定<code class="highlighter-rouge">storageClassName</code>来请求PV，如果集群中有这个class，那么当用户请求的时候，kubernetes会自动的创建PV，如果集群中有默认的storageclass，那么你只需要创建PVC即可，无需指定<code class="highlighter-rouge">storageClassName</code>,剩下的都有默认的动态配置来搞定。</p>

<p>举个例子：比如我现在需要两种类型的存储，一种是SSD，一种是普通的硬盘，那么这时候，我就可以创建两种class的<code class="highlighter-rouge">StorageClass</code>,然后在创建PVC时，指定不同的<code class="highlighter-rouge">storageClassName</code>即可，如下：</p>

<p>每个<code class="highlighter-rouge">StorageClass</code>都包含<code class="highlighter-rouge">provisioner</code>和<code class="highlighter-rouge">parameters</code>这个两个字段，具体怎么配置这些storageclass，有哪些存储支持storageclass,请参考<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">官网</a></p>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="s">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">slow</span>
<span class="s">provisioner</span><span class="pi">:</span> <span class="s">kubernetes.io/gce-pd</span>
<span class="s">parameters</span><span class="pi">:</span>
  <span class="s">type</span><span class="pi">:</span> <span class="s">pd-standard</span>
</code></pre>
</div>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">kind</span><span class="pi">:</span> <span class="s">StorageClass</span>
<span class="s">apiVersion</span><span class="pi">:</span> <span class="s">storage.k8s.io/v1</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">fast</span>
<span class="s">provisioner</span><span class="pi">:</span> <span class="s">kubernetes.io/gce-pd</span>
<span class="s">parameters</span><span class="pi">:</span>
  <span class="s">type</span><span class="pi">:</span> <span class="s">pd-ssd</span>
</code></pre>
</div>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">pvc-fast</span>
  <span class="s">storageClassName</span><span class="pi">:</span> <span class="s">fast</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadOnlyMany</span>
  <span class="s">resources</span><span class="pi">:</span>
    <span class="s">requests</span><span class="pi">:</span>
      <span class="s">storage</span><span class="pi">:</span> <span class="s">2Gi</span>
</code></pre>
</div>
<p>当用户请求资源的时候，如果是通过pvc-fast请求，这时候就会绑定到一个SSD，而不会绑定到普通硬盘。</p>

<h6 id="绑定">绑定</h6>

<p>在动态配置的情况下，当用户创建或者之前就已经创建了具有特定数量或具有某些访问模式的PVC的时候。master中的loop control会监视新的PVC，找到匹配的PV，然后将它们绑定到一起。如果这个PV是通过动态提供给PVC，那么loop control会始终绑定这个PV给这个PVC。否则，用户可能会一直请求PV，但是实际上又没有那么多PV资源。还需要注意的事，一旦绑定了PVC，就不能再绑定其他。</p>

<h6 id="使用">使用</h6>

<p>Pod使用PVC和使用Volume一样。集群检查PVC然后找到绑定的pv，然后隐射给pod使用。
一旦用户拥有了一个PVC，并且PVC被绑定，那么只要用户还需要，PV就一直属于这个用户。</p>

<h6 id="回收">回收</h6>

<p>当用户不在需要PV时，我们可以删除PVC来回收PV，<code class="highlighter-rouge">PersistentVolume</code>中的回收策略会告诉kubernetes当PVC被释放后该怎么做，目前，支持的策略如下</p>

<ul>
  <li>Retained(保留)</li>
</ul>

<p>当PVC被删除后，PV仍然被保留下来，并且会变成<code class="highlighter-rouge">released</code>。但是它还不能被其他PVC使用，因为现在PV上仍然有上一个PVC所请求的数据。</p>

<ul>
  <li>Recycled(再利用)</li>
</ul>

<p>当PVC被删除后，kubernetes会将PV里的数据删除，然后把PV变成Available，然后又可以被新的PVC绑定</p>

<p>这个原理实际上是：在删除PVC之后，会运行一个POD来执行一个(<code class="highlighter-rouge">rm -rf /thevolume/*</code>)的操作，删除pv下的所有数据</p>

<p><strong>默认回收运行的POD用的image是gcr上的busybox，而且image策略是always，因为这个原因，你可能始终无法回收PV，这时候，就需要去重新配置回收POD的模板了，模板内容如下</strong></p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">recycler-for-nfs</span>
  <span class="s">namespace</span><span class="pi">:</span> <span class="s">default</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">restartPolicy</span><span class="pi">:</span> <span class="s">Never</span>
  <span class="s">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">vol</span>
    <span class="s">nfs</span><span class="pi">:</span>
      <span class="s">path</span><span class="pi">:</span> <span class="s">/</span>
      <span class="s">server</span><span class="pi">:</span> <span class="s">163.xx.xx.xx</span>
  <span class="s">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">pv-recycler</span>
    <span class="s">image</span><span class="pi">:</span> <span class="s2">"</span><span class="s">gcr.io/google_containers/busybox"</span>
    <span class="s">imagePullPolicy</span><span class="pi">:</span> <span class="s">IfNotPresent</span>
    <span class="s">command</span><span class="pi">:</span> <span class="pi">[</span><span class="s2">"</span><span class="s">/bin/sh"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">-c"</span><span class="pi">,</span> <span class="s2">"</span><span class="s">test</span><span class="nv"> </span><span class="s">-e</span><span class="nv"> </span><span class="s">/scrub</span><span class="nv"> </span><span class="s">&amp;&amp;</span><span class="nv"> </span><span class="s">rm</span><span class="nv"> </span><span class="s">-rf</span><span class="nv"> </span><span class="s">/scrub/..?*</span><span class="nv"> </span><span class="s">/scrub/.[!.]*</span><span class="nv"> </span><span class="s">/scrub/*</span><span class="nv">  </span><span class="s">&amp;&amp;</span><span class="nv"> </span><span class="s">test</span><span class="nv"> </span><span class="s">-z</span><span class="nv"> </span><span class="se">\"</span><span class="s">$(ls</span><span class="nv"> </span><span class="s">-A</span><span class="nv"> </span><span class="s">/scrub)</span><span class="se">\"</span><span class="nv"> </span><span class="s">||</span><span class="nv"> </span><span class="s">exit</span><span class="nv"> </span><span class="s">1"</span><span class="pi">]</span>
    <span class="s">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">vol</span>
      <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/scrub</span>
</code></pre>
</div>

<p>写好模板了，还需要在<code class="highlighter-rouge">kube-controller-manager</code>中去配置模板，<a href="https://kubernetes.io/docs/admin/kube-controller-manager/">更多详细信息</a>可参考官网</p>

<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="c1"># 指定nfs回收模板的位置</span>
<span class="pi">-</span> <span class="s">--pv-recycler-pod-template-filepath-nfs=/etc/kubernetes/recycler-for-nfs.yaml</span>

<span class="c1"># 在容器下挂载模板文件</span>
<span class="s">volumeMounts</span><span class="pi">:</span>
<span class="pi">-</span> <span class="s">mountPath</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/etc/kubernetes/recycler-for-nfs.yaml"</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">recycler-nfs</span>

<span class="c1"># 将宿主机上的模板文件挂载到pod</span>
<span class="s">volumes</span><span class="pi">:</span>
<span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">recycler-nfs</span>
  <span class="s">hostPath</span><span class="pi">:</span>
    <span class="s">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/etc/kubernetes/recycler-for-nfs.yaml"</span>
</code></pre>
</div>

<p>下面我们来新建PV和PVC来测试一下</p>

<p>nfs-pv.yaml</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">nfs</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">capacity</span><span class="pi">:</span>
    <span class="s">storage</span><span class="pi">:</span> <span class="s">5Gi</span>
  <span class="s">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="s">persistentVolumeReclaimPolicy</span><span class="pi">:</span> <span class="s">Recycle</span>
  <span class="s">nfs</span><span class="pi">:</span>
    <span class="s">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/"</span>
    <span class="s">server</span><span class="pi">:</span> <span class="s">163.44.165.142</span>
</code></pre>
</div>

<p>nfs-pvc.yaml</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">nfs</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
  <span class="s">resources</span><span class="pi">:</span>
    <span class="s">requests</span><span class="pi">:</span>
      <span class="s">storage</span><span class="pi">:</span> <span class="s">5Gi</span>
</code></pre>
</div>

<p>nfs-nginx.yaml</p>
<div class="language-yaml highlighter-rouge"><pre class="highlight"><code><span class="s">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="s">kind</span><span class="pi">:</span> <span class="s">Pod</span>
<span class="s">metadata</span><span class="pi">:</span>
  <span class="s">name</span><span class="pi">:</span> <span class="s">test-pd</span>
<span class="s">spec</span><span class="pi">:</span>
  <span class="s">containers</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">image</span><span class="pi">:</span> <span class="s">nginx</span>
    <span class="s">name</span><span class="pi">:</span> <span class="s">test-container</span>
    <span class="s">volumeMounts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">mountPath</span><span class="pi">:</span> <span class="s">/test-pd</span>
      <span class="s">name</span><span class="pi">:</span> <span class="s">test-volume</span>
  <span class="s">volumes</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s">name</span><span class="pi">:</span> <span class="s">test-volume</span>
    <span class="s">persistentVolumeClaim</span><span class="pi">:</span>
      <span class="s">claimName</span><span class="pi">:</span> <span class="s">nfs</span>

</code></pre>
</div>

<p>命令如下：</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 创建PV,PVC以及关联的POD</span>
kubectl create -f nfs-pv.yaml
kubectl create -f nfs-pvc.yaml
kubectl create -f nfs-nginx.yaml

<span class="c"># 查看创建的PV</span>
<span class="gp">$ </span>kubectl get pv
NAME      CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM         STORAGECLASS   REASON    AGE
nfs       5Gi        RWO           Recycle         Bound     default/nfs                            1h

<span class="c"># 查看创建的PVC</span>
<span class="gp">$ </span>kubectl get pvc
NAME      STATUS    VOLUME    CAPACITY   ACCESSMODES   STORAGECLASS   AGE
nfs       Bound     nfs       5Gi        RWO                          8m

<span class="c"># 然后我们进入pod中新建一个文件recycle-file</span>
<span class="gp">$ </span>kubectl <span class="nb">exec</span> -ti <span class="nb">test</span>-pd /bin/bash
<span class="gp">$ </span>touch recycle-file
</code></pre>
</div>

<p>我们发现nfs的回收策略是<code class="highlighter-rouge">Recycle</code>，当前状态是<code class="highlighter-rouge">Bound</code>，那么假如我现在将PVC删除掉呢，下面我们来操作试试看</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code>kubectl delete -f nfs-nginx.yaml
kubectl delete -f nfs-pvc.yaml

<span class="c"># 查看现在的pv</span>
<span class="gp">$ </span>kubectl get pv
NAME      CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS     CLAIM         STORAGECLASS   REASON    AGE
nfs       5Gi        RWO           Recycle         Released   default/nfs                            1h

<span class="c"># 过了一会儿，再看pv</span>
<span class="gp">$ </span>kubectl get pv
NAME      CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE
nfs       5Gi        RWO           Recycle         Available                                      1h

<span class="c"># 再重新使用PVC关联POD</span>
kubectl create -f nfs-pvc.yaml
kubectl create -f nfs-nginx.yaml

<span class="c"># 进入pod中看看数据是否还存在</span>
<span class="gp">$ </span>kubectl <span class="nb">exec</span> -ti <span class="nb">test</span>-pd ls /test-pd
</code></pre>
</div>

<p>我们发现当我们的回收策略是<code class="highlighter-rouge">Recycle</code>时，删除PVC之后，PV的状态先是<code class="highlighter-rouge">Released</code>，然后过一会儿之后，会变成<code class="highlighter-rouge">Available</code>状态，而且PV上的数据也已经被删除了，这时候就可以再次被其他的PVC使用了。</p>

<p><strong>注意：当前只有NFS和HostPath支持回收利用操作</strong></p>

<ul>
  <li>Delete(删除)</li>
</ul>

<p>当PVC被删除后，kubernetes会删除PV及里面的数据。</p>

<p><strong>注意：当前只有AWS EBS,GCE PD,AZURE DISK,OPENSTACK CINDER卷支持删除操作</strong></p>

<p><strong>注意：动态PV，总会在PVC被删除后被删除</strong></p>

<h6 id="pv阶段状态">PV阶段状态</h6>

<p>一个volume会处于下面的几个状态之一</p>

<ul>
  <li>Avaliable 尚未绑定到PVC上的可用资源</li>
  <li>Bound 已经被绑定到PVC</li>
  <li>Released PVC已被删除，但是资源尚未回收</li>
  <li>Failed 自动回收失败</li>
</ul>

<h6 id="capacity">Capacity</h6>

<p>通常，我们在创建PV的时候，会从存储上给它划定一定大小的容量，这就使用capacity来指定即可。</p>

<h6 id="resource">resource</h6>

<p>pvc，就像pod一样，可以指定request资源的大小</p>

<h6 id="selector">selector</h6>

<p>PVC也可以指定标签选择器进行深度过滤PV，只有匹配了selector的PV才能绑定给PVC</p>

<ul>
  <li>matchLabels 单个匹配</li>
  <li>matchExpressions 表达式匹配</li>
</ul>

<h3 id="namespace">Namespace</h3>

<p>Namespace一般用于实现多租户的资源隔离。namespace 通过将集群内部的资源分配到不同的Namespace中，形成逻辑上不同项目、小组或环境的隔离，同时利用resource quota实现资源的管控限制，而随着kubernetes访问控制的深入，namespace开始与kubernetes的认证和授权机制结合。</p>

<h3 id="annotation">Annotation</h3>

<p>annotation 和label类似，也是使用 key/value的形式进行定义。不同的是label具有严格的命名规则，它定义的是kubernetes对象的元数据，并且用于label selector，而annotation测试用户任意定义的”附加”信息，以便于外部工具进行查找，很多时候，kubernetes的模块会通过annotation的方式标记资源的特殊信息。</p>

    </article>
    <div class="share">
      <div class="share-component"></div>
    </div>
    <div class="comment">
      

  

  
      
        
        <!-- Disqus Protection, see https://github.com/mzlogin/mzlogin.github.io/issues/2 -->
        
        
          <div id="disqus_thread"></div>
          <script>
            var disqus_config = function () {
              this.page.url = 'https://kevinguo.me/2017/09/01/kubernetes-one-section/';
              this.page.identifier = '/2017/09/01/kubernetes-one-section/';
              this.page.title = 'kubernetes 入门';
            };
            (function() { // DON'T EDIT BELOW THIS LINE
              var d = document, s = d.createElement('script');

              s.type = 'text/javascript';
              s.async = true;
              var shortname = 'kevinguo';

              s.src = '//' + shortname + '.disqus.com/embed.js';

              s.setAttribute('data-timestamp', +new Date());
              (d.head || d.body).appendChild(s);
            })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
        
      
    


    </div>
  </div>
  <div class="column one-fourth">
    
<h3>Search</h3>
<div id="site_search">
    <input type="text" id="search_box" placeholder="Search">
    <button class="btn btn-default" id="site_search_do"><span class="octicon octicon-search"></span></button>
</div>

<ul id="search_results"></ul>

<link rel="stylesheet" type="text/css" href="/assets/css/modules/sidebar-search.css">
<script src="/assets/js/lunr.min.js"></script>
<script src="/assets/js/search.js"></script>


    

    
<h3 class="post-directory-title mobile-hidden">Table of Contents</h3>
<div id="post-directory-module" class="mobile-hidden">
  <section class="post-directory">
  <!-- Links that trigger the jumping -->
  <!-- Added by javascript below -->
  <dl></dl>
  </section>
</div>

<script src="/assets/js/jquery.toc.js"></script>

  </div>
</div>
</section>
<!-- /section.content -->

    <footer class="container">
        <div class="site-footer" role="contentinfo">
            <div class="copyright left mobile-block">
                    © 2015
                    <span title="KevinGuo">KevinGuo</span>
                    <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
            </div>

            <ul class="site-footer-links right mobile-hidden">
                <li>
                    <a href="javascript:window.scrollTo(0,0)" >TOP</a>
                </li>
            </ul>
            <a href="http://github.com/chinakevinguo/chinakevinguo.github.io" target="_blank" aria-label="view source code">
                <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
            </a>
            <ul class="site-footer-links mobile-hidden">
                
                <li>
                    <a href="/" title="首页" target="">首页</a>
                </li>
                
                <li>
                    <a href="/categories/" title="分类" target="">分类</a>
                </li>
                
                <li>
                    <a href="/wiki/" title="维基" target="">维基</a>
                </li>
                
                <li>
                    <a href="/open-source/" title="开源" target="">开源</a>
                </li>
                
                <li>
                    <a href="/links/" title="链接" target="">链接</a>
                </li>
                
                <li>
                    <a href="/about/" title="关于" target="">关于</a>
                </li>
                
                <li><a href="/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li>
            </ul>

        </div>
    </footer>
    <!-- / footer -->
    <script src="/assets/vendor/share.js/dist/js/share.min.js"></script>
    <script src="/assets/js/geopattern.js"></script>
    <script src="/assets/js/prism.js"></script>
    <link rel="stylesheet" href="/assets/css/globals/prism.css">
    <script>
      jQuery(document).ready(function($) {
        // geopattern
        $('.geopattern').each(function(){
          $(this).geopattern($(this).data('pattern-id'));
        });
       // hljs.initHighlightingOnLoad();
      });
    </script>
    
    <div style="display:none">
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-80669434-1', 'auto');
        ga('send', 'pageview');

      </script>
    </div>
    
</body>
</html>
