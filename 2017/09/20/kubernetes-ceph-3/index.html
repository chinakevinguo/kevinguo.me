<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>kubernetes ceph 笔记 3 &mdash; KevinGuo</title>
    <link rel="stylesheet" href="/assets/vendor/primer-css/css/primer.css">
    <link rel="stylesheet" href="/assets/vendor/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="/assets/vendor/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="/assets/css/components/collection.css">
    <link rel="stylesheet" href="/assets/css/components/repo-card.css">
    <link rel="stylesheet" href="/assets/css/sections/repo-list.css">
    <link rel="stylesheet" href="/assets/css/sections/mini-repo-list.css">
    <link rel="stylesheet" href="/assets/css/components/boxed-group.css">
    <link rel="stylesheet" href="/assets/css/globals/common.css">
    <link rel="stylesheet" href="/assets/vendor/share.js/dist/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/globals/responsive.css">
    <link rel="stylesheet" href="/assets/css/posts/index.css">
    <!-- Latest compiled and minified CSS -->
    

    
    <link rel="canonical" href="https://kevinguo.me/2017/09/20/kubernetes-ceph-3/">
    <link rel="alternate" type="application/atom+xml" title="KevinGuo" href="/feed.xml">
    <link rel="shortcut icon" href="/favicon.ico">
    
    <meta property="og:title" content="kubernetes ceph 笔记 3">
      
    <meta name="keywords" content="kubernetes,ceph">
    <meta name="og:keywords" content="kubernetes,ceph">
      
    <meta name="description" content="  前面花了两章的时间介绍了ceph存储集群，简单的讲了ceph的组件、架构、寻址过程以及关于rbd,cephfs,cephGW,rados,osd,mon,mds,pool,pg,object等的操作过程，这一章主要记录下kubernetes使用ceph的相关配置过程。通过官网，我们发现在kubernetes中使用的是ceph的RBD部署集群具体的集群部署这里就不在赘述，请参考kubernetes ceph 笔记 1、kubernetes ceph 笔记 2，这里我们只是额外添加一组实验所需的osd，命令如下添加osd为每台机器添加一个sdc的硬盘，我这里用目录代替硬盘# 在每台osd节点上执行sudo mkdir -p /data/sdcchown ceph.ceph /data/sdc#在管理节点上执行`ceph-deploy`来准备OSDceph-deploy osd prepare k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc# 激活OSDceph-deploy osd activate k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc# 检测集群状态ceph halth创建RBD# 创建存储池rados mkpool data# 创建imagerbd create data --size 10G -p data# 关闭不支持特性rbd feature disable data exclusive-lock, object-map, fast-diff, deep-flatten -p data# 映射image到块设备(每个节点都需要隐射)rbd map data --name client.admin -p data# 格式化块设备mkfs.xfs /dev/rbd0Kubernetes 使用CephPV &amp; PVC方式传统使用分布式存储的方式一般为 PV &amp; PVC 的方式，也就是说管理员必须预先创建好PV 和 PVC ，然后对应的deployment或者replication挂载PVC来使用创建secret# 获取管理 key 并进行 base64 编码ceph auth get-key client.admin | base64# 创建一个secret 配置(key 为上一条命令生成)vim ceph-secret.ymlapiVersion: v1kind: Secretmetadata:  name: ceph-secretdata:  key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ==# 创建secretkubectl create -f ceph-secret.yml创建PV#创建ceph-pv文件vim ceph-pv.ymlapiVersion: v1kind: PersistentVolumemetadata:  name: ceph-pvspec:  capacity:    storage: 2Gi  accessModes:    - ReadWriteMany  rbd:    monitors:      - 172.30.33.90:6789      - 172.30.33.91:6789      - 172.30.33.92:6789    pool: data    image: data    user: admin    secretRef:      name: ceph-secret    fsType: xfs    readOnly: false  persistentVolumeReclaimPolicy: Recycle# 创建PVkubectl create -f ceph-pv.yml创建PVC# 新建ceph-pvc.yml文件vim ceph-pvc.ymlapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: ceph-pvcspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 2Gi# 创建PVCkubectl create -f ceph-pvc.yml创建一个测试的deployment来挂载# 新建nginx.ymlapiVersion: apps/v1beta1kind: Deploymentmetadata:  name: ceph-nginxspec:  replicas: 3  template:    metadata:      labels:        app: ceph-nginx    spec:      containers:      - name: ceph-nginx        image: nginx        ports:        - containerPort: 80        volumeMounts:          - mountPath: "/data"            name: data      volumes:        - name: data          persistentVolumeClaim:            claimName: ceph-pvc# 创建nginx deploymentkubectl create -f ceph-nginx.yml配置k8s node我们创建好PV 和 PVC之后，进行查看时可能会出现with: rbd: failed to modprobe rbd error:exit status 1的报错，所以这时候我们需要对所有k8s-node进行如下配置# 在所有k8s node上安装ceph-commonyum install -y ceph-common# 拷贝ceph.conf和ceph.client.admin.keyring到/etc/ceph/目录下# 配置kubelet有关ceph的参数，增加如下内容vim /usr/local/bin/kubelet-v /sbin/modprobe:/sbin/modprobe:ro \-v /lib/modules:/lib/modules:ro \-v /etc/ceph:/etc/ceph:ro \# 重启kubeletsystemctl restart kubelet# 查看pod是否启动成功kubectl get podsNAME                          READY     STATUS    RESTARTS   AGEceph-nginx-2497831062-569lw   1/1       Running   0          15mceph-nginx-2497831062-589j9   1/1       Running   0          59mceph-nginx-2497831062-5t01s   1/1       Running   0          12m# 然后进入其中一个pod，写入一个1G的文件kubectl exec -ti ceph-nginx-2497831062-569lw  /bin/bashdd if=/dev/zero of=test-file bs=1G count=1# 然后查看是否已经占用了rbd中的空间呢ceph dfGLOBAL:    SIZE     AVAIL     RAW USED     %RAW USED    575G      344G         230G         40.09POOLS:    NAME                      ID     USED      %USED     MAX AVAIL     OBJECTS    data                      14     1038M      1.50        68277M         280# 然后我们删除这个podkubectl delete pods ceph-nginx-2497831062-569lw# 查看新的pod,发现文件依旧在kubectl exec -ti ceph-nginx-2497831062-rgkcl ls /datatest-fileStorageClass方式之StatefulSet重头戏来了，洋洋洒洒写了近3篇文章，最终就是要使用这个StorageClass这个东西；这个东西在前面的kubernetes入门有简单的提到过，就是说动态创建PV，不用再事先固定PV的大小，直接创建PVC即可分配使用。创建secret# 获取管理 key 并进行 base64 编码ceph auth get-key client.admin | base64# 创建一个secret 配置(key 为上一条命令生成)vim ceph-storageclass-secret.ymlapiVersion: v1kind: Secretmetadata:  name: ceph-storageclass-secretdata:  key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ==type: kubernetes.io/rbd# 创建一个namespace的secretvim ceph-storageclass-secret-system.ymlapiVersion: v1kind: Secretmetadata:  name: ceph-storageclass-secret  namespace: kube-systemdata:  key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ==type: kubernetes.io/rbd# 创建secretkubectl create -f ceph-storageclass-secret.ymlkubectl create -f ceph-storageclass-secret-system.yml创建一个storageclass# 新建ceph-storageclass.yml文件vim ceph-storageclass.ymlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: ceph-storageclassprovisioner: kubernetes.io/rbdparameters:  monitors: 172.30.33.90:6789,172.30.33.91:6789,172.30.33.92:6789  adminId: admin  adminSecretName: ceph-storageclass-secret  adminSecretNamespace: kube-system  pool: data  userId: admin  userSecretName: ceph-storageclass-secret# 新建storageclasskubectl create -f ceph-storageclass.yml创建statefulset我们在使用StorageClass的时候，可以自己手动创建PVC，然后所有pods共享一个pvc；也可以定义volumeClaimTemplates来为自动为每个pod创建一个单独的pvc，如下所示# 新建ceph-storageclass-nginx.ymlvim ceph-storageclass-nginx.ymlapiVersion: apps/v1beta1kind: StatefulSetmetadata:  name: ceph-storageclass-nginxspec:  serviceName: "ceph-storageclass-nginx-service"  replicas: 3  template:    metadata:      labels:        app: ceph-storageclass-nginx    spec:      containers:      - name: ceph-storageclass-nginx        image: nginx        ports:        - containerPort: 80        volumeMounts:          - mountPath: "/data"            name: data  volumeClaimTemplates:  - metadata:      name: data      annotations:        volume.beta.kubernetes.io/storage-class: ceph-storageclass-pvc    spec:      accessModes: ["ReadWriteOnce"]      resources:        requests:          storage: 5Gi# 新建ceph-storageclass-nginx-service.ymlvim ce ph-storageclass-nginx-service.ymlapiVersion: v1kind: Servicemetadata:  name: ceph-storageclass-nginx-service  labels:    app: ceph-storageclass-nginx-servicespec:  ports:  - port: 80    name: web  clusterIP: None  selector:    app: ceph-storageclass-nginx# 创建statefulSetkubectl create -f ceph-storageclass-nginx.ymlkubectl create -f ceph-storageclass-nginx-service.yml# 查看podskubectl get podsNAME                          READY     STATUS    RESTARTS   AGEceph-storageclass-nginx-0     1/1       Running   0          11mceph-storageclass-nginx-1     1/1       Running   0          11mceph-storageclass-nginx-2     1/1       Running   0          11m# 查看自动创建的pv和pvckubectl get pvNAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                                    STORAGECLASS        REASON    AGEpvc-1af65cba-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-0   ceph-storageclass             15mpvc-934ed5ad-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-1   ceph-storageclass             12mpvc-9ff7359d-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-2   ceph-storageclass             11mkubectl get pvcNAME                             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGEdata-ceph-storageclass-nginx-0   Bound     pvc-1af65cba-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   15mdata-ceph-storageclass-nginx-1   Bound     pvc-934ed5ad-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   12mdata-ceph-storageclass-nginx-2   Bound     pvc-9ff7359d-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   11m# 进入pod查看使用情况，发现/data使用大小5G[root@k8s-master01 k8s-quark]# kubectl exec -ti ceph-storageclass-nginx-1 -- df -ThFilesystem                                                                                      Type   Size  Used Avail Use% Mounted on/dev/rbd1                                                                                       ext4   4.8G   10M  4.6G   1% /dataStorageClass方式之Deployment我们接着使用上面创建的StorageClass，只不过这个时候我们需要手动来创建一个PVC创建PVC# 新建ceph-storageclass-pvc.ymlvim ceph-storageclass-pvc.ymlkind: PersistentVolumeClaimapiVersion: v1metadata:  name: test-pvc  annotations:    volume.beta.kubernetes.io/storage-class: ceph-storageclassspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 50Gi# 创建PVCkubectl create -f ceph-storageclass-pvc.yml创建deployment# 新建ceph-storageclass-nginx-deployment.ymlvim ceph-storageclass-nginx-deployment.ymlapiVersion: apps/v1beta1kind: Deploymentmetadata:  name: ceph-nginxspec:  replicas: 3  template:    metadata:      labels:        app: ceph-nginx    spec:      containers:      - name: ceph-nginx        image: nginx        ports:        - containerPort: 80        volumeMounts:          - mountPath: "/data"            name: data      volumes:        - name: data          persistentVolumeClaim:            claimName: test-pvc# 创建deploymentkubectl create -f ceph-storageclass-nginx-deployment.yml# 查看podskubectl get podsNAME                          READY     STATUS    RESTARTS   AGEceph-nginx-3206996150-29q7j   1/1       Running   0          7mceph-nginx-3206996150-94tzk   1/1       Running   0          7mceph-nginx-3206996150-xvkzh   1/1       Running   0          7m# 查看PV和PVCkubectl get pvcNAME                             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGEtest-pvc                         Bound     pvc-76a48238-9dcf-11e7-84a9-00155d201312   50Gi       RWX           ceph-storageclass   9mkubectl get pvNAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                                    STORAGECLASS        REASON    AGEpvc-76a48238-9dcf-11e7-84a9-00155d201312   50Gi       RWX           Delete          Bound     default/test-pvc                         ceph-storageclass             10m# 进入pod 查看使用情况，看到/data总共50Gkubectl exec -ti ceph-nginx-3206996150-29q7j -- df -ThFilesystem                                                                                      Type   Size  Used Avail Use% Mounted on/dev/rbd0                                                                                       ext4    50G   52M   47G   1% /data至此kubernetes结合ceph RBD的实验基本上已经完成，我们发现，storageclass确实是个好东西，省去了创建PV的步骤，并且，可以根据PVC中定义的class来选择创建不同的PVC">
    <meta name="og:description" content="  前面花了两章的时间介绍了ceph存储集群，简单的讲了ceph的组件、架构、寻址过程以及关于rbd,cephfs,cephGW,rados,osd,mon,mds,pool,pg,object等的操作过程，这一章主要记录下kubernetes使用ceph的相关配置过程。通过官网，我们发现在kubernetes中使用的是ceph的RBD部署集群具体的集群部署这里就不在赘述，请参考kubernetes ceph 笔记 1、kubernetes ceph 笔记 2，这里我们只是额外添加一组实验所需的osd，命令如下添加osd为每台机器添加一个sdc的硬盘，我这里用目录代替硬盘# 在每台osd节点上执行sudo mkdir -p /data/sdcchown ceph.ceph /data/sdc#在管理节点上执行`ceph-deploy`来准备OSDceph-deploy osd prepare k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc# 激活OSDceph-deploy osd activate k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc# 检测集群状态ceph halth创建RBD# 创建存储池rados mkpool data# 创建imagerbd create data --size 10G -p data# 关闭不支持特性rbd feature disable data exclusive-lock, object-map, fast-diff, deep-flatten -p data# 映射image到块设备(每个节点都需要隐射)rbd map data --name client.admin -p data# 格式化块设备mkfs.xfs /dev/rbd0Kubernetes 使用CephPV &amp; PVC方式传统使用分布式存储的方式一般为 PV &amp; PVC 的方式，也就是说管理员必须预先创建好PV 和 PVC ，然后对应的deployment或者replication挂载PVC来使用创建secret# 获取管理 key 并进行 base64 编码ceph auth get-key client.admin | base64# 创建一个secret 配置(key 为上一条命令生成)vim ceph-secret.ymlapiVersion: v1kind: Secretmetadata:  name: ceph-secretdata:  key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ==# 创建secretkubectl create -f ceph-secret.yml创建PV#创建ceph-pv文件vim ceph-pv.ymlapiVersion: v1kind: PersistentVolumemetadata:  name: ceph-pvspec:  capacity:    storage: 2Gi  accessModes:    - ReadWriteMany  rbd:    monitors:      - 172.30.33.90:6789      - 172.30.33.91:6789      - 172.30.33.92:6789    pool: data    image: data    user: admin    secretRef:      name: ceph-secret    fsType: xfs    readOnly: false  persistentVolumeReclaimPolicy: Recycle# 创建PVkubectl create -f ceph-pv.yml创建PVC# 新建ceph-pvc.yml文件vim ceph-pvc.ymlapiVersion: v1kind: PersistentVolumeClaimmetadata:  name: ceph-pvcspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 2Gi# 创建PVCkubectl create -f ceph-pvc.yml创建一个测试的deployment来挂载# 新建nginx.ymlapiVersion: apps/v1beta1kind: Deploymentmetadata:  name: ceph-nginxspec:  replicas: 3  template:    metadata:      labels:        app: ceph-nginx    spec:      containers:      - name: ceph-nginx        image: nginx        ports:        - containerPort: 80        volumeMounts:          - mountPath: "/data"            name: data      volumes:        - name: data          persistentVolumeClaim:            claimName: ceph-pvc# 创建nginx deploymentkubectl create -f ceph-nginx.yml配置k8s node我们创建好PV 和 PVC之后，进行查看时可能会出现with: rbd: failed to modprobe rbd error:exit status 1的报错，所以这时候我们需要对所有k8s-node进行如下配置# 在所有k8s node上安装ceph-commonyum install -y ceph-common# 拷贝ceph.conf和ceph.client.admin.keyring到/etc/ceph/目录下# 配置kubelet有关ceph的参数，增加如下内容vim /usr/local/bin/kubelet-v /sbin/modprobe:/sbin/modprobe:ro \-v /lib/modules:/lib/modules:ro \-v /etc/ceph:/etc/ceph:ro \# 重启kubeletsystemctl restart kubelet# 查看pod是否启动成功kubectl get podsNAME                          READY     STATUS    RESTARTS   AGEceph-nginx-2497831062-569lw   1/1       Running   0          15mceph-nginx-2497831062-589j9   1/1       Running   0          59mceph-nginx-2497831062-5t01s   1/1       Running   0          12m# 然后进入其中一个pod，写入一个1G的文件kubectl exec -ti ceph-nginx-2497831062-569lw  /bin/bashdd if=/dev/zero of=test-file bs=1G count=1# 然后查看是否已经占用了rbd中的空间呢ceph dfGLOBAL:    SIZE     AVAIL     RAW USED     %RAW USED    575G      344G         230G         40.09POOLS:    NAME                      ID     USED      %USED     MAX AVAIL     OBJECTS    data                      14     1038M      1.50        68277M         280# 然后我们删除这个podkubectl delete pods ceph-nginx-2497831062-569lw# 查看新的pod,发现文件依旧在kubectl exec -ti ceph-nginx-2497831062-rgkcl ls /datatest-fileStorageClass方式之StatefulSet重头戏来了，洋洋洒洒写了近3篇文章，最终就是要使用这个StorageClass这个东西；这个东西在前面的kubernetes入门有简单的提到过，就是说动态创建PV，不用再事先固定PV的大小，直接创建PVC即可分配使用。创建secret# 获取管理 key 并进行 base64 编码ceph auth get-key client.admin | base64# 创建一个secret 配置(key 为上一条命令生成)vim ceph-storageclass-secret.ymlapiVersion: v1kind: Secretmetadata:  name: ceph-storageclass-secretdata:  key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ==type: kubernetes.io/rbd# 创建一个namespace的secretvim ceph-storageclass-secret-system.ymlapiVersion: v1kind: Secretmetadata:  name: ceph-storageclass-secret  namespace: kube-systemdata:  key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ==type: kubernetes.io/rbd# 创建secretkubectl create -f ceph-storageclass-secret.ymlkubectl create -f ceph-storageclass-secret-system.yml创建一个storageclass# 新建ceph-storageclass.yml文件vim ceph-storageclass.ymlapiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: ceph-storageclassprovisioner: kubernetes.io/rbdparameters:  monitors: 172.30.33.90:6789,172.30.33.91:6789,172.30.33.92:6789  adminId: admin  adminSecretName: ceph-storageclass-secret  adminSecretNamespace: kube-system  pool: data  userId: admin  userSecretName: ceph-storageclass-secret# 新建storageclasskubectl create -f ceph-storageclass.yml创建statefulset我们在使用StorageClass的时候，可以自己手动创建PVC，然后所有pods共享一个pvc；也可以定义volumeClaimTemplates来为自动为每个pod创建一个单独的pvc，如下所示# 新建ceph-storageclass-nginx.ymlvim ceph-storageclass-nginx.ymlapiVersion: apps/v1beta1kind: StatefulSetmetadata:  name: ceph-storageclass-nginxspec:  serviceName: "ceph-storageclass-nginx-service"  replicas: 3  template:    metadata:      labels:        app: ceph-storageclass-nginx    spec:      containers:      - name: ceph-storageclass-nginx        image: nginx        ports:        - containerPort: 80        volumeMounts:          - mountPath: "/data"            name: data  volumeClaimTemplates:  - metadata:      name: data      annotations:        volume.beta.kubernetes.io/storage-class: ceph-storageclass-pvc    spec:      accessModes: ["ReadWriteOnce"]      resources:        requests:          storage: 5Gi# 新建ceph-storageclass-nginx-service.ymlvim ce ph-storageclass-nginx-service.ymlapiVersion: v1kind: Servicemetadata:  name: ceph-storageclass-nginx-service  labels:    app: ceph-storageclass-nginx-servicespec:  ports:  - port: 80    name: web  clusterIP: None  selector:    app: ceph-storageclass-nginx# 创建statefulSetkubectl create -f ceph-storageclass-nginx.ymlkubectl create -f ceph-storageclass-nginx-service.yml# 查看podskubectl get podsNAME                          READY     STATUS    RESTARTS   AGEceph-storageclass-nginx-0     1/1       Running   0          11mceph-storageclass-nginx-1     1/1       Running   0          11mceph-storageclass-nginx-2     1/1       Running   0          11m# 查看自动创建的pv和pvckubectl get pvNAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                                    STORAGECLASS        REASON    AGEpvc-1af65cba-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-0   ceph-storageclass             15mpvc-934ed5ad-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-1   ceph-storageclass             12mpvc-9ff7359d-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-2   ceph-storageclass             11mkubectl get pvcNAME                             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGEdata-ceph-storageclass-nginx-0   Bound     pvc-1af65cba-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   15mdata-ceph-storageclass-nginx-1   Bound     pvc-934ed5ad-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   12mdata-ceph-storageclass-nginx-2   Bound     pvc-9ff7359d-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   11m# 进入pod查看使用情况，发现/data使用大小5G[root@k8s-master01 k8s-quark]# kubectl exec -ti ceph-storageclass-nginx-1 -- df -ThFilesystem                                                                                      Type   Size  Used Avail Use% Mounted on/dev/rbd1                                                                                       ext4   4.8G   10M  4.6G   1% /dataStorageClass方式之Deployment我们接着使用上面创建的StorageClass，只不过这个时候我们需要手动来创建一个PVC创建PVC# 新建ceph-storageclass-pvc.ymlvim ceph-storageclass-pvc.ymlkind: PersistentVolumeClaimapiVersion: v1metadata:  name: test-pvc  annotations:    volume.beta.kubernetes.io/storage-class: ceph-storageclassspec:  accessModes:    - ReadWriteMany  resources:    requests:      storage: 50Gi# 创建PVCkubectl create -f ceph-storageclass-pvc.yml创建deployment# 新建ceph-storageclass-nginx-deployment.ymlvim ceph-storageclass-nginx-deployment.ymlapiVersion: apps/v1beta1kind: Deploymentmetadata:  name: ceph-nginxspec:  replicas: 3  template:    metadata:      labels:        app: ceph-nginx    spec:      containers:      - name: ceph-nginx        image: nginx        ports:        - containerPort: 80        volumeMounts:          - mountPath: "/data"            name: data      volumes:        - name: data          persistentVolumeClaim:            claimName: test-pvc# 创建deploymentkubectl create -f ceph-storageclass-nginx-deployment.yml# 查看podskubectl get podsNAME                          READY     STATUS    RESTARTS   AGEceph-nginx-3206996150-29q7j   1/1       Running   0          7mceph-nginx-3206996150-94tzk   1/1       Running   0          7mceph-nginx-3206996150-xvkzh   1/1       Running   0          7m# 查看PV和PVCkubectl get pvcNAME                             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGEtest-pvc                         Bound     pvc-76a48238-9dcf-11e7-84a9-00155d201312   50Gi       RWX           ceph-storageclass   9mkubectl get pvNAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                                    STORAGECLASS        REASON    AGEpvc-76a48238-9dcf-11e7-84a9-00155d201312   50Gi       RWX           Delete          Bound     default/test-pvc                         ceph-storageclass             10m# 进入pod 查看使用情况，看到/data总共50Gkubectl exec -ti ceph-nginx-3206996150-29q7j -- df -ThFilesystem                                                                                      Type   Size  Used Avail Use% Mounted on/dev/rbd0                                                                                       ext4    50G   52M   47G   1% /data至此kubernetes结合ceph RBD的实验基本上已经完成，我们发现，storageclass确实是个好东西，省去了创建PV的步骤，并且，可以根据PVC中定义的class来选择创建不同的PVC">
      
    
    
        
    
    <meta property="og:url" content="https://kevinguo.me/2017/09/20/kubernetes-ceph-3/">
    <meta property="og:site_name" content="KevinGuo">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh_CN" />
    
    <meta property="article:published_time" content="2017-09-20">
    
    <script src="/assets/vendor/jquery/dist/jquery.min.js"></script>
    <script src="/assets/js/jquery-ui.js"></script>
    <script type="text/javascript">
    function toggleMenu() {
        var nav = document.getElementsByClassName("site-header-nav")[0];
        if (nav.style.display == "inline-flex") {
          nav.style.display = "none";
        } else {
          nav.style.display = "inline-flex";
        }
    }
    </script>
</head>
<body class="" data-mz="">
    <header class="site-header">
        <div class="container">
            <h1><a href="/" title="KevinGuo"><span class="octicon octicon-mark-github"></span> KevinGuo</a></h1>
            <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <nav class="site-header-nav" role="navigation">
                
                <a href="/" class=" site-header-nav-item" target="" title="首页">首页</a>
                
                <a href="/categories/" class=" site-header-nav-item" target="" title="分类">分类</a>
                
                <a href="/wiki/" class=" site-header-nav-item" target="" title="维基">维基</a>
                
                <a href="/open-source/" class=" site-header-nav-item" target="" title="开源">开源</a>
                
                <a href="/links/" class=" site-header-nav-item" target="" title="链接">链接</a>
                
                <a href="/about/" class=" site-header-nav-item" target="" title="关于">关于</a>
                
            </nav>
        </div>
    </header>
    <!-- / header -->

    <section class="collection-head small geopattern" data-pattern-id="kubernetes ceph">
<div class="container">
  <div class="columns">
    <div class="column three-fourths">
      <div class="collection-title">
        <h1 class="collection-header">kubernetes ceph 笔记 3</h1>
        <div class="collection-info">
          
          <span class="meta-info">
            <span class="octicon octicon-calendar"></span> 2017/09/20
          </span>
          
          
          <span class="meta-info">
            <span class="octicon octicon-file-directory"></span>
            <a href="/categories/#kubernetes" title="kubernetes">kubernetes</a>
          </span>
          
          <span class="meta-info">
            <span class="octicon octicon-file-directory"></span>
            <a href="/categories/#ceph" title="ceph">ceph</a>
          </span>
          
        </div>
      </div>
    </div>
  </div>
</div>
</section>
<!-- / .banner -->
<section class="container content">
<div class="columns">
  <div class="column three-fourths" >
    <article class="article-content markdown-body">
    <blockquote>
  <p>前面花了两章的时间介绍了ceph存储集群，简单的讲了ceph的组件、架构、寻址过程以及关于rbd,cephfs,cephGW,rados,osd,mon,mds,pool,pg,object等的操作过程，这一章主要记录下kubernetes使用ceph的相关配置过程。</p>
</blockquote>

<p>通过<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#ceph-rbd">官网</a>，我们发现在kubernetes中使用的是ceph的RBD</p>

<h3 id="部署集群">部署集群</h3>

<p>具体的集群部署这里就不在赘述，请参考<a href="https://kevinguo.me/2017/09/06/kubernetes-ceph-1/">kubernetes ceph 笔记 1</a>、<a href="https://kevinguo.me/2017/09/12/kubernetes-ceph-2/">kubernetes ceph 笔记 2</a>，这里我们只是额外添加一组实验所需的osd，命令如下</p>

<h5 id="添加osd">添加osd</h5>

<p>为每台机器添加一个sdc的硬盘，我这里用目录代替硬盘</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 在每台osd节点上执行</span>
sudo mkdir -p /data/sdc
chown ceph.ceph /data/sdc

<span class="c">#在管理节点上执行`ceph-deploy`来准备OSD</span>
ceph-deploy osd prepare k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc

<span class="c"># 激活OSD</span>
ceph-deploy osd activate k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc

<span class="c"># 检测集群状态</span>
ceph halth
</code></pre>
</div>

<h5 id="创建rbd">创建RBD</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 创建存储池</span>
rados mkpool data

<span class="c"># 创建image</span>
rbd create data --size 10G -p data

<span class="c"># 关闭不支持特性</span>
rbd feature disable data exclusive-lock, object-map, fast-diff, deep-flatten -p data

<span class="c"># 映射image到块设备(每个节点都需要隐射)</span>
rbd map data --name client.admin -p data

<span class="c"># 格式化块设备</span>
mkfs.xfs /dev/rbd0
</code></pre>
</div>

<h3 id="kubernetes-使用ceph">Kubernetes 使用Ceph</h3>

<h4 id="pv--pvc方式">PV &amp; PVC方式</h4>

<p>传统使用分布式存储的方式一般为 <code class="highlighter-rouge">PV &amp; PVC</code> 的方式，也就是说管理员必须预先创建好PV 和 PVC ，然后对应的deployment或者replication挂载PVC来使用</p>

<h5 id="创建secret">创建secret</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 获取管理 key 并进行 base64 编码</span>
ceph auth get-key client.admin | base64

<span class="c"># 创建一个secret 配置(key 为上一条命令生成)</span>
vim ceph-secret.yml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: <span class="nv">QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ</span><span class="o">==</span>

<span class="c"># 创建secret</span>
kubectl create -f ceph-secret.yml
</code></pre>
</div>

<h5 id="创建pv">创建PV</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c">#创建ceph-pv文件</span>
vim ceph-pv.yml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: ceph-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteMany
  rbd:
    monitors:
      - 172.30.33.90:6789
      - 172.30.33.91:6789
      - 172.30.33.92:6789
    pool: data
    image: data
    user: admin
    secretRef:
      name: ceph-secret
    fsType: xfs
    readOnly: <span class="nb">false
  </span>persistentVolumeReclaimPolicy: Recycle

<span class="c"># 创建PV</span>
kubectl create -f ceph-pv.yml
</code></pre>
</div>

<h5 id="创建pvc">创建PVC</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 新建ceph-pvc.yml文件</span>
vim ceph-pvc.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi

<span class="c"># 创建PVC</span>
kubectl create -f ceph-pvc.yml
</code></pre>
</div>

<h5 id="创建一个测试的deployment来挂载">创建一个测试的deployment来挂载</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 新建nginx.yml</span>
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: ceph-nginx
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: ceph-nginx
    spec:
      containers:
      - name: ceph-nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - mountPath: <span class="s2">"/data"</span>
            name: data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: ceph-pvc

<span class="c"># 创建nginx deployment</span>
kubectl create -f ceph-nginx.yml
</code></pre>
</div>

<h5 id="配置k8s-node">配置k8s node</h5>

<p>我们创建好PV 和 PVC之后，进行查看时可能会出现<code class="highlighter-rouge">with: rbd: failed to modprobe rbd error:exit status 1</code>的报错，所以这时候我们需要对所有k8s-node进行如下配置</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 在所有k8s node上安装ceph-common</span>
yum install -y ceph-common

<span class="c"># 拷贝ceph.conf和ceph.client.admin.keyring到/etc/ceph/目录下</span>

<span class="c"># 配置kubelet有关ceph的参数，增加如下内容</span>
vim /usr/local/bin/kubelet

-v /sbin/modprobe:/sbin/modprobe:ro <span class="se">\</span>
-v /lib/modules:/lib/modules:ro <span class="se">\</span>
-v /etc/ceph:/etc/ceph:ro <span class="se">\</span>

<span class="c"># 重启kubelet</span>
systemctl restart kubelet

<span class="c"># 查看pod是否启动成功</span>
kubectl get pods
NAME                          READY     STATUS    RESTARTS   AGE
ceph-nginx-2497831062-569lw   1/1       Running   0          15m
ceph-nginx-2497831062-589j9   1/1       Running   0          59m
ceph-nginx-2497831062-5t01s   1/1       Running   0          12m

<span class="c"># 然后进入其中一个pod，写入一个1G的文件</span>
kubectl <span class="nb">exec</span> -ti ceph-nginx-2497831062-569lw  /bin/bash

dd <span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span><span class="nb">test</span>-file <span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1

<span class="c"># 然后查看是否已经占用了rbd中的空间呢</span>
ceph df
GLOBAL:
    SIZE     AVAIL     RAW USED     %RAW USED
    575G      344G         230G         40.09
POOLS:
    NAME                      ID     USED      %USED     MAX AVAIL     OBJECTS
    data                      14     1038M      1.50        68277M         280

<span class="c"># 然后我们删除这个pod</span>
kubectl delete pods ceph-nginx-2497831062-569lw

<span class="c"># 查看新的pod,发现文件依旧在</span>
kubectl <span class="nb">exec</span> -ti ceph-nginx-2497831062-rgkcl ls /data
<span class="nb">test</span>-file
</code></pre>
</div>

<h4 id="storageclass方式之statefulset">StorageClass方式之StatefulSet</h4>

<p>重头戏来了，洋洋洒洒写了近3篇文章，最终就是要使用这个StorageClass这个东西；这个东西在前面的<a href="https://kevinguo.me/2017/09/01/kubernetes-one-section/#%E5%8A%A8%E6%80%81">kubernetes入门</a>有简单的提到过，就是说动态创建PV，不用再事先固定PV的大小，直接创建PVC即可分配使用。</p>

<h5 id="创建secret-1">创建secret</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 获取管理 key 并进行 base64 编码</span>
ceph auth get-key client.admin | base64

<span class="c"># 创建一个secret 配置(key 为上一条命令生成)</span>
vim ceph-storageclass-secret.yml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-storageclass-secret
data:
  key: <span class="nv">QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ</span><span class="o">==</span>
<span class="nb">type</span>: kubernetes.io/rbd

<span class="c"># 创建一个namespace的secret</span>
vim ceph-storageclass-secret-system.yml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-storageclass-secret
  namespace: kube-system
data:
  key: <span class="nv">QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ</span><span class="o">==</span>
<span class="nb">type</span>: kubernetes.io/rbd

<span class="c"># 创建secret</span>
kubectl create -f ceph-storageclass-secret.yml
kubectl create -f ceph-storageclass-secret-system.yml
</code></pre>
</div>

<h5 id="创建一个storageclass">创建一个storageclass</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 新建ceph-storageclass.yml文件</span>
vim ceph-storageclass.yml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-storageclass
provisioner: kubernetes.io/rbd
parameters:
  monitors: 172.30.33.90:6789,172.30.33.91:6789,172.30.33.92:6789
  adminId: admin
  adminSecretName: ceph-storageclass-secret
  adminSecretNamespace: kube-system
  pool: data
  userId: admin
  userSecretName: ceph-storageclass-secret

<span class="c"># 新建storageclass</span>
kubectl create -f ceph-storageclass.yml
</code></pre>
</div>

<h5 id="创建statefulset">创建statefulset</h5>

<p>我们在使用StorageClass的时候，可以自己手动创建PVC，然后所有pods共享一个pvc；也可以定义<code class="highlighter-rouge">volumeClaimTemplates</code>来为自动为每个pod创建一个单独的pvc，如下所示</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 新建ceph-storageclass-nginx.yml</span>
vim ceph-storageclass-nginx.yml

apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: ceph-storageclass-nginx
spec:
  serviceName: <span class="s2">"ceph-storageclass-nginx-service"</span>
  replicas: 3
  template:
    metadata:
      labels:
        app: ceph-storageclass-nginx
    spec:
      containers:
      - name: ceph-storageclass-nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - mountPath: <span class="s2">"/data"</span>
            name: data
  volumeClaimTemplates:
  - metadata:
      name: data
      annotations:
        volume.beta.kubernetes.io/storage-class: ceph-storageclass-pvc
    spec:
      accessModes: <span class="o">[</span><span class="s2">"ReadWriteOnce"</span><span class="o">]</span>
      resources:
        requests:
          storage: 5Gi

<span class="c"># 新建ceph-storageclass-nginx-service.yml</span>
vim ce ph-storageclass-nginx-service.yml

apiVersion: v1
kind: Service
metadata:
  name: ceph-storageclass-nginx-service
  labels:
    app: ceph-storageclass-nginx-service
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: ceph-storageclass-nginx

<span class="c"># 创建statefulSet</span>
kubectl create -f ceph-storageclass-nginx.yml
kubectl create -f ceph-storageclass-nginx-service.yml

<span class="c"># 查看pods</span>
kubectl get pods
NAME                          READY     STATUS    RESTARTS   AGE
ceph-storageclass-nginx-0     1/1       Running   0          11m
ceph-storageclass-nginx-1     1/1       Running   0          11m
ceph-storageclass-nginx-2     1/1       Running   0          11m

<span class="c"># 查看自动创建的pv和pvc</span>
kubectl get pv
NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                                    STORAGECLASS        REASON    AGE
pvc-1af65cba-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-0   ceph-storageclass             15m
pvc-934ed5ad-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-1   ceph-storageclass             12m
pvc-9ff7359d-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-2   ceph-storageclass             11m


kubectl get pvc
NAME                             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGE
data-ceph-storageclass-nginx-0   Bound     pvc-1af65cba-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   15m
data-ceph-storageclass-nginx-1   Bound     pvc-934ed5ad-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   12m
data-ceph-storageclass-nginx-2   Bound     pvc-9ff7359d-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   11m

<span class="c"># 进入pod查看使用情况，发现/data使用大小5G</span>
<span class="o">[</span>root@k8s-master01 k8s-quark]# kubectl <span class="nb">exec</span> -ti ceph-storageclass-nginx-1 -- df -Th
Filesystem                                                                                      Type   Size  Used Avail Use% Mounted on
/dev/rbd1                                                                                       ext4   4.8G   10M  4.6G   1% /data
</code></pre>
</div>

<h4 id="storageclass方式之deployment">StorageClass方式之Deployment</h4>

<p>我们接着使用上面创建的StorageClass，只不过这个时候我们需要手动来创建一个PVC</p>

<h5 id="创建pvc-1">创建PVC</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 新建ceph-storageclass-pvc.yml</span>
vim ceph-storageclass-pvc.yml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: <span class="nb">test</span>-pvc
  annotations:
    volume.beta.kubernetes.io/storage-class: ceph-storageclass
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi

<span class="c"># 创建PVC</span>
kubectl create -f ceph-storageclass-pvc.yml
</code></pre>
</div>

<h5 id="创建deployment">创建deployment</h5>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="c"># 新建ceph-storageclass-nginx-deployment.yml</span>
vim ceph-storageclass-nginx-deployment.yml

apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: ceph-nginx
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: ceph-nginx
    spec:
      containers:
      - name: ceph-nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - mountPath: <span class="s2">"/data"</span>
            name: data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: <span class="nb">test</span>-pvc

<span class="c"># 创建deployment</span>
kubectl create -f ceph-storageclass-nginx-deployment.yml

<span class="c"># 查看pods</span>
kubectl get pods
NAME                          READY     STATUS    RESTARTS   AGE
ceph-nginx-3206996150-29q7j   1/1       Running   0          7m
ceph-nginx-3206996150-94tzk   1/1       Running   0          7m
ceph-nginx-3206996150-xvkzh   1/1       Running   0          7m

<span class="c"># 查看PV和PVC</span>
kubectl get pvc
NAME                             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGE
<span class="nb">test</span>-pvc                         Bound     pvc-76a48238-9dcf-11e7-84a9-00155d201312   50Gi       RWX           ceph-storageclass   9m

kubectl get pv
NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                                    STORAGECLASS        REASON    AGE
pvc-76a48238-9dcf-11e7-84a9-00155d201312   50Gi       RWX           Delete          Bound     default/test-pvc                         ceph-storageclass             10m

<span class="c"># 进入pod 查看使用情况，看到/data总共50G</span>
kubectl <span class="nb">exec</span> -ti ceph-nginx-3206996150-29q7j -- df -Th
Filesystem                                                                                      Type   Size  Used Avail Use% Mounted on
/dev/rbd0                                                                                       ext4    50G   52M   47G   1% /data
</code></pre>
</div>

<p>至此kubernetes结合ceph RBD的实验基本上已经完成，我们发现，storageclass确实是个好东西，省去了创建PV的步骤，并且，可以根据PVC中定义的class来选择创建不同的PVC</p>

    </article>
    <div class="share">
      <div class="share-component"></div>
    </div>
    <div class="comment">
      

  

  
      
        
        <!-- Disqus Protection, see https://github.com/mzlogin/mzlogin.github.io/issues/2 -->
        
        
          <div id="disqus_thread"></div>
          <script>
            var disqus_config = function () {
              this.page.url = 'https://kevinguo.me/2017/09/20/kubernetes-ceph-3/';
              this.page.identifier = '/2017/09/20/kubernetes-ceph-3/';
              this.page.title = 'kubernetes ceph 笔记 3';
            };
            (function() { // DON'T EDIT BELOW THIS LINE
              var d = document, s = d.createElement('script');

              s.type = 'text/javascript';
              s.async = true;
              var shortname = 'kevinguo';

              s.src = '//' + shortname + '.disqus.com/embed.js';

              s.setAttribute('data-timestamp', +new Date());
              (d.head || d.body).appendChild(s);
            })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
        
      
    


    </div>
  </div>
  <div class="column one-fourth">
    
<h3>Search</h3>
<div id="site_search">
    <input type="text" id="search_box" placeholder="Search">
    <button class="btn btn-default" id="site_search_do"><span class="octicon octicon-search"></span></button>
</div>

<ul id="search_results"></ul>

<link rel="stylesheet" type="text/css" href="/assets/css/modules/sidebar-search.css">
<script src="/assets/js/lunr.min.js"></script>
<script src="/assets/js/search.js"></script>


    

    
<h3 class="post-directory-title mobile-hidden">Table of Contents</h3>
<div id="post-directory-module" class="mobile-hidden">
  <section class="post-directory">
  <!-- Links that trigger the jumping -->
  <!-- Added by javascript below -->
  <dl></dl>
  </section>
</div>

<script src="/assets/js/jquery.toc.js"></script>

  </div>
</div>
</section>
<!-- /section.content -->

    <footer class="container">
        <div class="site-footer" role="contentinfo">
            <div class="copyright left mobile-block">
                    © 2015
                    <span title="KevinGuo">KevinGuo</span>
                    <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
            </div>

            <ul class="site-footer-links right mobile-hidden">
                <li>
                    <a href="javascript:window.scrollTo(0,0)" >TOP</a>
                </li>
            </ul>
            <a href="http://github.com/chinakevinguo/chinakevinguo.github.io" target="_blank" aria-label="view source code">
                <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
            </a>
            <ul class="site-footer-links mobile-hidden">
                
                <li>
                    <a href="/" title="首页" target="">首页</a>
                </li>
                
                <li>
                    <a href="/categories/" title="分类" target="">分类</a>
                </li>
                
                <li>
                    <a href="/wiki/" title="维基" target="">维基</a>
                </li>
                
                <li>
                    <a href="/open-source/" title="开源" target="">开源</a>
                </li>
                
                <li>
                    <a href="/links/" title="链接" target="">链接</a>
                </li>
                
                <li>
                    <a href="/about/" title="关于" target="">关于</a>
                </li>
                
                <li><a href="/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li>
            </ul>

        </div>
    </footer>
    <!-- / footer -->
    <script src="/assets/vendor/share.js/dist/js/share.min.js"></script>
    <script src="/assets/js/geopattern.js"></script>
    <script src="/assets/js/prism.js"></script>
    <link rel="stylesheet" href="/assets/css/globals/prism.css">
    <script>
      jQuery(document).ready(function($) {
        // geopattern
        $('.geopattern').each(function(){
          $(this).geopattern($(this).data('pattern-id'));
        });
       // hljs.initHighlightingOnLoad();
      });
    </script>
    
    <div style="display:none">
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-80669434-1', 'auto');
        ga('send', 'pageview');

      </script>
    </div>
    
</body>
</html>
