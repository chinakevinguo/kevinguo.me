<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>kubernetes ceph 笔记 1 &mdash; KevinGuo</title>
    <link rel="stylesheet" href="/assets/vendor/primer-css/css/primer.css">
    <link rel="stylesheet" href="/assets/vendor/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="/assets/vendor/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="/assets/css/components/collection.css">
    <link rel="stylesheet" href="/assets/css/components/repo-card.css">
    <link rel="stylesheet" href="/assets/css/sections/repo-list.css">
    <link rel="stylesheet" href="/assets/css/sections/mini-repo-list.css">
    <link rel="stylesheet" href="/assets/css/components/boxed-group.css">
    <link rel="stylesheet" href="/assets/css/globals/common.css">
    <link rel="stylesheet" href="/assets/vendor/share.js/dist/css/share.min.css">
    <link rel="stylesheet" href="/assets/css/globals/responsive.css">
    <link rel="stylesheet" href="/assets/css/posts/index.css">
    <!-- Latest compiled and minified CSS -->
    

    
    <link rel="canonical" href="https://kevinguo.me/2017/09/06/kubernetes-ceph-1/">
    <link rel="alternate" type="application/atom+xml" title="KevinGuo" href="/feed.xml">
    <link rel="shortcut icon" href="/favicon.ico">
    
    <meta property="og:title" content="kubernetes ceph 笔记 1">
      
    <meta name="keywords" content="kubernetes,ceph">
    <meta name="og:keywords" content="kubernetes,ceph">
      
    <meta name="description" content="  该教程主要是为statefulset有状态服务集群提供持久化存储提供基础，在讲statefulset之前，我们先搭建我们的ceph集群；具备了极好的可靠性、统一性；经过近几年的发展，ceph开辟了一个全新的数据存储途径。ceph具备了企业级存储的分布式、可大规模扩展、没有当节点故障等特点，越来越受人们的青睐。简介Ceph是一个符合POSIX、开源的分布式存储系统，不论你是想提供ceph 对象存储或ceph 块设备，还是想部署一个ceph文件系统或者把ceph作为他用，所有 Ceph存储集群 的部署都始于部署一个个 ceph节点、网络和ceph存储集群，ceph 存储集群至少需要一个 ceph monitor和两个osd守护进程。而运行ceph文件系统客户端，则必须需要MDS(元数据服务器)。基础组件:      Ceph OSDs: Ceph OSD 守护进程的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当Ceph 存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能到到active+clean状态(Ceph 默认有3个副本，你可以调整osd poll default size)        Ceph Monitors: Ceph Monitor基于PAXOS算法维护着 展示集群状态的各种图表，包括监视器图、OSD图、PG图、CRUSH图。Ceph 保存着发生在Monitors、OSD和PG上的每一次状态变更的历史信息(称为 epoch)        MDSs：Ceph MDS为 Ceph 文件系统存储元数据(也就是说，Ceph块设备和Ceph对象存储是不是用MDS)。MDS使得POSIX文件系统的用户们，可以在部队Ceph存储集群造成负担的前提下，执行诸如 ls、find等基本命令  下图展示了Ceph的基础架构图1.基础存储系统RADOSRADOS (Reliable Autonomic Distributed Object Store),这一层本身就是一个完整的对象存储系统，包括Ceph的基础服务(OSD,MON,MDS)，所有存储在ceph中的用户数据实际上最终都是由这一层来存储的。而Ceph的高可用，高扩展性，高自动化等特性本质上也是有这一层所提供的，因此，RADOS是ceph的核心精华部分。2.基础库LibRados这一层是对RADOS进行抽象和封装，并向上层提供不同的API，这样上层的RBD、RGW、CephFS才能访问RADOS，RADOS所提供的原生librados API包括C和C++两种。3.高层存储应用接口这一层包含了RGW、RBD和CephFS这几个部分，其作用是在librados库的基础上提供抽象层次更高，更便于应用和用户端使用的上层接口。      RGW: 是一个提供与S3和Swift兼容的RESTful API的gateway，以供对应的对象存储应用开发使用。通过RGW可以将RADOS响应转化为HTTP响应，同样也可以将外部的HTTP响应状花为RADOS调用。        RBD：提供一个标准的块设备接口服务。        CephFS: 提供一个POSIX兼容的分布式文件系统。  4.client层这一层其实就是不同场景下对于ceph各个应用接口的各种应用方式，例如基于librados直接开发的对象存储应用，基于RGW开发的对象存储应用，基于RBD实现的云硬盘等等。5.其他一个Cluster逻辑上可以划分为多个Pool，一个Pool由若干个逻辑PG组成。一个文件会被切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD，其中第一个OSD（Primary OSD）为主，其余是备，OSD间通过心跳来互相监控存活状态。CRUSH： CRUSH是ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。一.快速安装1.1 安装前准备所需机器如下            IP      HostName      OS      role                  172.30.33.31      deploy-node      centos7.3.1611      deploy node              172.30.33.90      k8s-master01      centos7.3.1611      monitor osd node1              172.30.33.91      k8s-master02      centos7.3.1611      monitor osd node2              172.30.33.92      k8s-master03      centos7.3.1611      monitor osd node3              172.30.33.89      k8s-registry      centos7.3.1611      osd mds              172.30.33.93      k8s-node01      centos7.3.1611      osd mds              172.30.33.94      k8s-node02      centos7.3.1611      osd mds      在管理节点上操作1.添加ceph源[ceph-noarch]name=Ceph noarch packagesbaseurl=https://download.ceph.com/rpm-jewel/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.asc$ yum update -y &amp;&amp; yum install ceph-deploy -y2.更新并安装ceph-deploy$ sudo yum update &amp;&amp; sudo yum install ceph-deploy2.配置从部署机器到所有其他节点的免密钥登录，具体参考这里在节点上操作1.安装epel源$ yum install yum-plugin-priorities$ yum install epel-release -y2.校对时间，由于ceph使用Paxos算法保证数据一致性，所以安装前要先保证各个节点的时间同步$ sudo yum install ntp ntpdate ntp-doc$ ntpdate 0.cn.pool.ntp.org3.开放所需端口或关闭防火墙$ system stop firewalld$ sudo firewall-cmd --zone=public --add-port=6789/tcp --permanent4.关闭selinux$ sudo setenforce 01.2 创建集群1.由于ceph-deploy工具部署集群前需要创建一些集群配置信息，其保存在ceph.conf文件中，这个文件将来会被复制到每个节点的 /etc/ceph/ceph.conf# 创建集群配置目录mkdir ceph-cluster &amp;&amp; cd ceph-cluster# 创建 monitor-nodeceph-deploy new k8s-master01# 追加 OSD 副本数量(测试虚拟机总共有3台)echo "osd pool default size = 3" &gt;&gt; ceph.conf2.创建集群使用 ceph-deploy工具在部署节点上执行即可# 安装ceph$ ceph-deploy install k8s-master01 k8s-master02 k8s-master03注意：在部署节点部署的时候，可能会因为网络原因导致无法安装ceph和ceph-radosgw，这时候，我们在各个节点上手动安装一下# 添加ceph 源[Ceph]name=Ceph packages for $basearchbaseurl=http://download.ceph.com/rpm-jewel/el7/$basearchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://download.ceph.com/rpm-jewel/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[ceph-source]name=Ceph source packagesbaseurl=http://download.ceph.com/rpm-jewel/el7/SRPMSenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1$ yum install ceph ceph-radosgw -y3.初始化monitor node 和密钥文件$ ceph-deploy mon create-initial4.在osd节点上创建一个目录作为 osd 存储，并修改其权限,千万别创建在/usr/local目录下，否则，你的osd可能会无法挂载$ ssh k8s-master01$ sudo mkdir /data/osd1$ chown -R ceph:ceph osd1$ exit$ ssh k8s-master2$ sudo mkdir /data/osd2$ chown -R ceph:ceph osd2$ exit$ ssh k8s-master3$ sudo mkdir /data/osd3$ chown -R ceph:ceph osd3$ exit5.然后，在管理节点上初始化 osd$ ceph-deploy osd prepare k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd36.最后，在管理节点上激活 osd$ ceph-deploy osd activate k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd37.在管理节点上部署 ceph cli 工具和密钥文件$ ceph-deploy admin k8s-master01 k8s-master02 k8s-master038.确保你对 ceph.client.admin.keyring有正确的操作权限，在每个节点上执行$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring9.最后，检查集群的健康状态$ ceph healthHEALTH_OK$ ceph osd treeID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY-1 0.14067 root default                                            -2 0.04689     host k8s-master01                                    0 0.04689         osd.0              up  1.00000          1.00000-3 0.04689     host k8s-master02                                    1 0.04689         osd.1              up  1.00000          1.00000-4 0.04689     host k8s-master03                                    2 0.04689         osd.2              up  1.00000          1.00000我们看到状态是OK的，而且每个节点上的osd的状态都是up的如果在某些地方碰到麻烦，想从头再来，可以用下列命令来清楚配置：$ ceph-deploy purgedata {ceph-node} [{ceph-node}]$ ceph-deploy forgetkeys用下列命令可以连Ceph安装包一起清除：$ ceph-deploy purge {ceph-node} [{ceph-node}]二.操作集群2.1 基础操作# 创建MDS$ ceph-deploy mds create {ceph-node}# 创建RGW$ ceph-deploy rgw create {ceph-node}# 添加mon$ echo "public network = 172.30.33.0/24" &gt;&gt; ceph.conf$ ceph-deploy mon add {ceph-node}# 查看仲裁$ ceph quorum_status --format json-pretty2.2 对象存储测试# 创建pool# rados mkpool {pool-name}$ rados mkpool test-pool# 创建测试文件$ dd if=/dev/zero of=testfile bs=1G count=1# 创建一个对象(这时候也将对象放入了pool中)# rados put {object-name} {file-path} --pool={pool-name}$ rados put test-object testfile --pool=test-pool# 检查存储池，确认ceph存储了此对象$ rados -p test-pool ls# 定位对象，会输出对象位置$ ceph osd map test-pool test-objectosdmap e42 pool 'test-pool' (3) object 'test-file' -&gt; pg 3.b79653d4 (3.4) -&gt; up ([1,5,2,3,4], p1) acting ([1,5,2,3,4], p1)# 删除对象$ rados -p data rm test-object# 删除存储池$ rados rmpool test-pool test-pool --yes-i-really-really-mean-it随着集群的运行，对象的位置可能会动态改变。Ceph有动态均衡机制，无需手动干预即可完成。2.3 块存储测试官方建议使用RBD的客户端最好不要和OSD在同一台物理机上(除非它们都是VM)1.确认你使用了合适的内核版本，详情参见lab_release -auname -a2.在管理节点上用 ceph-deploy安装cephceph-deploy install {rbd-client}3.在管理节点上部署ceph cli工具和密钥ceph-deploy admin {rbd-client}4.在rbd节点上创建块设备imagerbd create test-block --size 40965.映射image到块设备rbd map test-block --name client.admin在上面的map映射操作时，会出现如下报错$ rbd map test-block --name client.adminrbd: sysfs write failedRBD image feature set mismatch. You can disable features unsupported by the kernel with "rbd feature disable".In some cases useful info is found in syslog - try "dmesg | tail" or so.rbd: map failed: (6) No such device or address大致意思是说features不匹配，可以通过disable features关掉一些特性来让内核支持。这是因为在Ceph高本本进行 map image时，默认ceph在创建image(上文test-block)时，会增加很多features，这些features需要内核支持，centos7上的支持有限，所以，我们需要关掉一些我们可以用 rbd info data 看看创建的image目前有哪些features$ rbd info test-blockrbd image 'test-block':	size 4096 MB in 1024 objects	order 22 (4096 kB objects)	block_name_prefix: rbd_data.10bb238e1f29	format: 2	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten	flags:在features中，我们可以看到默认开启了很多：  layering 支持分层  exclusive-lock 支持独占锁  object-map 支持对象映射(依赖exclusive-lock)  fast-diff 快速计算差异(依赖object-map)  deep-flatten 支持快照扁平化操作而实际上在CentOS7的3.10内核中只支持layering，所以我们需要手动关闭一些features，然后重新map；如果想要一劳永逸，可以在 ceph.conf 中加入 rbd_default_features = 1 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)6.关闭不支持的特性之后重新map# 关闭不支持的features$ rbd feature disable test-block exclusive-lock, object-map, fast-diff, deep-flatten# 重新map$ rbd map test-block --name client.admin/dev/rbd07.格式化之后挂载到系统目录# 格式化$ mkfs.xfs /dev/rbd0meta-data=/dev/rbd0              isize=512    agcount=9, agsize=130048 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=1048576, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0# 挂载$ mkdir test-block$ mount /dev/rbd0 test-block# 写入测试$ dd if=/dev/zero of=test-block/test-file bs=1G count=11+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 2.96071 s, 363 MB/s$ ls test-block/test-file2.4 CephFS 测试1.创建MDS$ ceph-deploy mds create k8s-node01 k8s-node02 k8s-registry2.创建pool和fs，创建pool需要指定PG数量ceph osd pool create cephfs_data 32ceph osd pool create cephfs_metadata 32ceph fs new test-fs cephfs_metadata cephfs_dataPG 概念：  当Ceph集群接受到存储请求时，ceph会将一个文件会切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD(根据副本数)；一般来说增加PG的数量能降低OSD负载，一般每个OSD大约分配50～100PG，关于PG数量指定，一般遵循以下公式      集群PG总数 = (OSD总数 * 100)/数据最大副本数    单个存储池PG数 = (OSD总数 * 100)/数据最大副本数/存储池数  注意：PG的最终结果应当以最接近以上计算公式的2的N次幂(向上取值)；如我的虚拟机环境的每个存储池 PG数 = 6(OSD) * 100 / 5(副本数) / 4（4个存储池）= 30，向上取2的N次幂为32(即，2的5次方=32，最接近30)3.挂载CephFS有两种方式，一种是使用内核驱动挂载，一种是使用 ceph-fuse用户空间挂载内核挂载需要提取ceph管理key，方式如下：在密钥文件中找到与某用户对于的密钥$ cat /etc/ceph/ceph.client.admin.keyring[client.admin]	key = AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA==复制密钥到文件中保存，并确保其权限echo "AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA==" &gt; ceph-key创建目录挂载mkdir test-fsmount -t ceph 172.30.33.90:6789:/ /root/test-fs -o name=admin,secretfile=ceph-key#写入数据测试$ dd if=/dev/zero of=test-fs/test-fs bs=1G count=11+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 2.77355 s, 387 MB/sceph-fuse用户空间挂载的方式也比较简单，需要先安装ceph-fuse，同时也需要key# 按照前面的步骤添加ceph源$ vi /etc/yum.repos.d/ceph.repo[ceph]name=cephbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0# 安装ceph-fuseyum install ceph-fuse -y# 复制配置和ceph key到client端sudo mkdir -p /etc/cephsudo scp root@172.30.33.91:/etc/ceph/ceph.conf /etc/ceph/ceph.confsudo scp root@172.30.33.91:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring# 创建目录挂载mkdir test-fs-fuse$ sudo ceph-fuse -m 172.30.33.91:6789 test-fs-fuseceph-fuse[60551]: starting ceph client2017-09-12 14:47:24.137929 7f63d9719ec0 -1 init, newargv = 0x7f63e51d4840 newargc=11ceph-fuse[60551]: starting fuse# 写入数据测试$ sudo dd if=/dev/zero of=test-fs-fuse/test-fs-fuse bs=1G count=11+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 10.5426 s, 102 MB/s# 查看确认，发现我们上面通过内核挂载的文件也还在$ ls -lhtotal 2.0G-rw-r--r-- 1 root root 1.0G Sep 12 13:59 test-fs-rw-r--r-- 1 root root 1.0G Sep 12 14:49 test-fs-fuse2.5 Ceph对象网关1.对象网关创建# 创建RGW$ ceph-deploy rgw create k8s-node022.直接访问http://ceph-node-ip:7480返回结果如下&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName/&gt;&lt;/Owner&gt;&lt;Buckets/&gt;&lt;/ListAllMyBucketsResult&gt;这就说明网关OK了，但是因为没有读写环境，所以暂时测不了。本文参考了ceph官方文档及漠然的ceph笔记(一)部分">
    <meta name="og:description" content="  该教程主要是为statefulset有状态服务集群提供持久化存储提供基础，在讲statefulset之前，我们先搭建我们的ceph集群；具备了极好的可靠性、统一性；经过近几年的发展，ceph开辟了一个全新的数据存储途径。ceph具备了企业级存储的分布式、可大规模扩展、没有当节点故障等特点，越来越受人们的青睐。简介Ceph是一个符合POSIX、开源的分布式存储系统，不论你是想提供ceph 对象存储或ceph 块设备，还是想部署一个ceph文件系统或者把ceph作为他用，所有 Ceph存储集群 的部署都始于部署一个个 ceph节点、网络和ceph存储集群，ceph 存储集群至少需要一个 ceph monitor和两个osd守护进程。而运行ceph文件系统客户端，则必须需要MDS(元数据服务器)。基础组件:      Ceph OSDs: Ceph OSD 守护进程的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当Ceph 存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能到到active+clean状态(Ceph 默认有3个副本，你可以调整osd poll default size)        Ceph Monitors: Ceph Monitor基于PAXOS算法维护着 展示集群状态的各种图表，包括监视器图、OSD图、PG图、CRUSH图。Ceph 保存着发生在Monitors、OSD和PG上的每一次状态变更的历史信息(称为 epoch)        MDSs：Ceph MDS为 Ceph 文件系统存储元数据(也就是说，Ceph块设备和Ceph对象存储是不是用MDS)。MDS使得POSIX文件系统的用户们，可以在部队Ceph存储集群造成负担的前提下，执行诸如 ls、find等基本命令  下图展示了Ceph的基础架构图1.基础存储系统RADOSRADOS (Reliable Autonomic Distributed Object Store),这一层本身就是一个完整的对象存储系统，包括Ceph的基础服务(OSD,MON,MDS)，所有存储在ceph中的用户数据实际上最终都是由这一层来存储的。而Ceph的高可用，高扩展性，高自动化等特性本质上也是有这一层所提供的，因此，RADOS是ceph的核心精华部分。2.基础库LibRados这一层是对RADOS进行抽象和封装，并向上层提供不同的API，这样上层的RBD、RGW、CephFS才能访问RADOS，RADOS所提供的原生librados API包括C和C++两种。3.高层存储应用接口这一层包含了RGW、RBD和CephFS这几个部分，其作用是在librados库的基础上提供抽象层次更高，更便于应用和用户端使用的上层接口。      RGW: 是一个提供与S3和Swift兼容的RESTful API的gateway，以供对应的对象存储应用开发使用。通过RGW可以将RADOS响应转化为HTTP响应，同样也可以将外部的HTTP响应状花为RADOS调用。        RBD：提供一个标准的块设备接口服务。        CephFS: 提供一个POSIX兼容的分布式文件系统。  4.client层这一层其实就是不同场景下对于ceph各个应用接口的各种应用方式，例如基于librados直接开发的对象存储应用，基于RGW开发的对象存储应用，基于RBD实现的云硬盘等等。5.其他一个Cluster逻辑上可以划分为多个Pool，一个Pool由若干个逻辑PG组成。一个文件会被切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD，其中第一个OSD（Primary OSD）为主，其余是备，OSD间通过心跳来互相监控存活状态。CRUSH： CRUSH是ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。一.快速安装1.1 安装前准备所需机器如下            IP      HostName      OS      role                  172.30.33.31      deploy-node      centos7.3.1611      deploy node              172.30.33.90      k8s-master01      centos7.3.1611      monitor osd node1              172.30.33.91      k8s-master02      centos7.3.1611      monitor osd node2              172.30.33.92      k8s-master03      centos7.3.1611      monitor osd node3              172.30.33.89      k8s-registry      centos7.3.1611      osd mds              172.30.33.93      k8s-node01      centos7.3.1611      osd mds              172.30.33.94      k8s-node02      centos7.3.1611      osd mds      在管理节点上操作1.添加ceph源[ceph-noarch]name=Ceph noarch packagesbaseurl=https://download.ceph.com/rpm-jewel/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.asc$ yum update -y &amp;&amp; yum install ceph-deploy -y2.更新并安装ceph-deploy$ sudo yum update &amp;&amp; sudo yum install ceph-deploy2.配置从部署机器到所有其他节点的免密钥登录，具体参考这里在节点上操作1.安装epel源$ yum install yum-plugin-priorities$ yum install epel-release -y2.校对时间，由于ceph使用Paxos算法保证数据一致性，所以安装前要先保证各个节点的时间同步$ sudo yum install ntp ntpdate ntp-doc$ ntpdate 0.cn.pool.ntp.org3.开放所需端口或关闭防火墙$ system stop firewalld$ sudo firewall-cmd --zone=public --add-port=6789/tcp --permanent4.关闭selinux$ sudo setenforce 01.2 创建集群1.由于ceph-deploy工具部署集群前需要创建一些集群配置信息，其保存在ceph.conf文件中，这个文件将来会被复制到每个节点的 /etc/ceph/ceph.conf# 创建集群配置目录mkdir ceph-cluster &amp;&amp; cd ceph-cluster# 创建 monitor-nodeceph-deploy new k8s-master01# 追加 OSD 副本数量(测试虚拟机总共有3台)echo "osd pool default size = 3" &gt;&gt; ceph.conf2.创建集群使用 ceph-deploy工具在部署节点上执行即可# 安装ceph$ ceph-deploy install k8s-master01 k8s-master02 k8s-master03注意：在部署节点部署的时候，可能会因为网络原因导致无法安装ceph和ceph-radosgw，这时候，我们在各个节点上手动安装一下# 添加ceph 源[Ceph]name=Ceph packages for $basearchbaseurl=http://download.ceph.com/rpm-jewel/el7/$basearchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://download.ceph.com/rpm-jewel/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1[ceph-source]name=Ceph source packagesbaseurl=http://download.ceph.com/rpm-jewel/el7/SRPMSenabled=1gpgcheck=1type=rpm-mdgpgkey=https://download.ceph.com/keys/release.ascpriority=1$ yum install ceph ceph-radosgw -y3.初始化monitor node 和密钥文件$ ceph-deploy mon create-initial4.在osd节点上创建一个目录作为 osd 存储，并修改其权限,千万别创建在/usr/local目录下，否则，你的osd可能会无法挂载$ ssh k8s-master01$ sudo mkdir /data/osd1$ chown -R ceph:ceph osd1$ exit$ ssh k8s-master2$ sudo mkdir /data/osd2$ chown -R ceph:ceph osd2$ exit$ ssh k8s-master3$ sudo mkdir /data/osd3$ chown -R ceph:ceph osd3$ exit5.然后，在管理节点上初始化 osd$ ceph-deploy osd prepare k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd36.最后，在管理节点上激活 osd$ ceph-deploy osd activate k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd37.在管理节点上部署 ceph cli 工具和密钥文件$ ceph-deploy admin k8s-master01 k8s-master02 k8s-master038.确保你对 ceph.client.admin.keyring有正确的操作权限，在每个节点上执行$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring9.最后，检查集群的健康状态$ ceph healthHEALTH_OK$ ceph osd treeID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY-1 0.14067 root default                                            -2 0.04689     host k8s-master01                                    0 0.04689         osd.0              up  1.00000          1.00000-3 0.04689     host k8s-master02                                    1 0.04689         osd.1              up  1.00000          1.00000-4 0.04689     host k8s-master03                                    2 0.04689         osd.2              up  1.00000          1.00000我们看到状态是OK的，而且每个节点上的osd的状态都是up的如果在某些地方碰到麻烦，想从头再来，可以用下列命令来清楚配置：$ ceph-deploy purgedata {ceph-node} [{ceph-node}]$ ceph-deploy forgetkeys用下列命令可以连Ceph安装包一起清除：$ ceph-deploy purge {ceph-node} [{ceph-node}]二.操作集群2.1 基础操作# 创建MDS$ ceph-deploy mds create {ceph-node}# 创建RGW$ ceph-deploy rgw create {ceph-node}# 添加mon$ echo "public network = 172.30.33.0/24" &gt;&gt; ceph.conf$ ceph-deploy mon add {ceph-node}# 查看仲裁$ ceph quorum_status --format json-pretty2.2 对象存储测试# 创建pool# rados mkpool {pool-name}$ rados mkpool test-pool# 创建测试文件$ dd if=/dev/zero of=testfile bs=1G count=1# 创建一个对象(这时候也将对象放入了pool中)# rados put {object-name} {file-path} --pool={pool-name}$ rados put test-object testfile --pool=test-pool# 检查存储池，确认ceph存储了此对象$ rados -p test-pool ls# 定位对象，会输出对象位置$ ceph osd map test-pool test-objectosdmap e42 pool 'test-pool' (3) object 'test-file' -&gt; pg 3.b79653d4 (3.4) -&gt; up ([1,5,2,3,4], p1) acting ([1,5,2,3,4], p1)# 删除对象$ rados -p data rm test-object# 删除存储池$ rados rmpool test-pool test-pool --yes-i-really-really-mean-it随着集群的运行，对象的位置可能会动态改变。Ceph有动态均衡机制，无需手动干预即可完成。2.3 块存储测试官方建议使用RBD的客户端最好不要和OSD在同一台物理机上(除非它们都是VM)1.确认你使用了合适的内核版本，详情参见lab_release -auname -a2.在管理节点上用 ceph-deploy安装cephceph-deploy install {rbd-client}3.在管理节点上部署ceph cli工具和密钥ceph-deploy admin {rbd-client}4.在rbd节点上创建块设备imagerbd create test-block --size 40965.映射image到块设备rbd map test-block --name client.admin在上面的map映射操作时，会出现如下报错$ rbd map test-block --name client.adminrbd: sysfs write failedRBD image feature set mismatch. You can disable features unsupported by the kernel with "rbd feature disable".In some cases useful info is found in syslog - try "dmesg | tail" or so.rbd: map failed: (6) No such device or address大致意思是说features不匹配，可以通过disable features关掉一些特性来让内核支持。这是因为在Ceph高本本进行 map image时，默认ceph在创建image(上文test-block)时，会增加很多features，这些features需要内核支持，centos7上的支持有限，所以，我们需要关掉一些我们可以用 rbd info data 看看创建的image目前有哪些features$ rbd info test-blockrbd image 'test-block':	size 4096 MB in 1024 objects	order 22 (4096 kB objects)	block_name_prefix: rbd_data.10bb238e1f29	format: 2	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten	flags:在features中，我们可以看到默认开启了很多：  layering 支持分层  exclusive-lock 支持独占锁  object-map 支持对象映射(依赖exclusive-lock)  fast-diff 快速计算差异(依赖object-map)  deep-flatten 支持快照扁平化操作而实际上在CentOS7的3.10内核中只支持layering，所以我们需要手动关闭一些features，然后重新map；如果想要一劳永逸，可以在 ceph.conf 中加入 rbd_default_features = 1 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)6.关闭不支持的特性之后重新map# 关闭不支持的features$ rbd feature disable test-block exclusive-lock, object-map, fast-diff, deep-flatten# 重新map$ rbd map test-block --name client.admin/dev/rbd07.格式化之后挂载到系统目录# 格式化$ mkfs.xfs /dev/rbd0meta-data=/dev/rbd0              isize=512    agcount=9, agsize=130048 blks         =                       sectsz=512   attr=2, projid32bit=1         =                       crc=1        finobt=0, sparse=0data     =                       bsize=4096   blocks=1048576, imaxpct=25         =                       sunit=1024   swidth=1024 blksnaming   =version 2              bsize=4096   ascii-ci=0 ftype=1log      =internal log           bsize=4096   blocks=2560, version=2         =                       sectsz=512   sunit=8 blks, lazy-count=1realtime =none                   extsz=4096   blocks=0, rtextents=0# 挂载$ mkdir test-block$ mount /dev/rbd0 test-block# 写入测试$ dd if=/dev/zero of=test-block/test-file bs=1G count=11+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 2.96071 s, 363 MB/s$ ls test-block/test-file2.4 CephFS 测试1.创建MDS$ ceph-deploy mds create k8s-node01 k8s-node02 k8s-registry2.创建pool和fs，创建pool需要指定PG数量ceph osd pool create cephfs_data 32ceph osd pool create cephfs_metadata 32ceph fs new test-fs cephfs_metadata cephfs_dataPG 概念：  当Ceph集群接受到存储请求时，ceph会将一个文件会切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD(根据副本数)；一般来说增加PG的数量能降低OSD负载，一般每个OSD大约分配50～100PG，关于PG数量指定，一般遵循以下公式      集群PG总数 = (OSD总数 * 100)/数据最大副本数    单个存储池PG数 = (OSD总数 * 100)/数据最大副本数/存储池数  注意：PG的最终结果应当以最接近以上计算公式的2的N次幂(向上取值)；如我的虚拟机环境的每个存储池 PG数 = 6(OSD) * 100 / 5(副本数) / 4（4个存储池）= 30，向上取2的N次幂为32(即，2的5次方=32，最接近30)3.挂载CephFS有两种方式，一种是使用内核驱动挂载，一种是使用 ceph-fuse用户空间挂载内核挂载需要提取ceph管理key，方式如下：在密钥文件中找到与某用户对于的密钥$ cat /etc/ceph/ceph.client.admin.keyring[client.admin]	key = AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA==复制密钥到文件中保存，并确保其权限echo "AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA==" &gt; ceph-key创建目录挂载mkdir test-fsmount -t ceph 172.30.33.90:6789:/ /root/test-fs -o name=admin,secretfile=ceph-key#写入数据测试$ dd if=/dev/zero of=test-fs/test-fs bs=1G count=11+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 2.77355 s, 387 MB/sceph-fuse用户空间挂载的方式也比较简单，需要先安装ceph-fuse，同时也需要key# 按照前面的步骤添加ceph源$ vi /etc/yum.repos.d/ceph.repo[ceph]name=cephbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/gpgcheck=0[ceph-noarch]name=cephnoarchbaseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/gpgcheck=0# 安装ceph-fuseyum install ceph-fuse -y# 复制配置和ceph key到client端sudo mkdir -p /etc/cephsudo scp root@172.30.33.91:/etc/ceph/ceph.conf /etc/ceph/ceph.confsudo scp root@172.30.33.91:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring# 创建目录挂载mkdir test-fs-fuse$ sudo ceph-fuse -m 172.30.33.91:6789 test-fs-fuseceph-fuse[60551]: starting ceph client2017-09-12 14:47:24.137929 7f63d9719ec0 -1 init, newargv = 0x7f63e51d4840 newargc=11ceph-fuse[60551]: starting fuse# 写入数据测试$ sudo dd if=/dev/zero of=test-fs-fuse/test-fs-fuse bs=1G count=11+0 records in1+0 records out1073741824 bytes (1.1 GB) copied, 10.5426 s, 102 MB/s# 查看确认，发现我们上面通过内核挂载的文件也还在$ ls -lhtotal 2.0G-rw-r--r-- 1 root root 1.0G Sep 12 13:59 test-fs-rw-r--r-- 1 root root 1.0G Sep 12 14:49 test-fs-fuse2.5 Ceph对象网关1.对象网关创建# 创建RGW$ ceph-deploy rgw create k8s-node022.直接访问http://ceph-node-ip:7480返回结果如下&lt;ListAllMyBucketsResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/"&gt;&lt;Owner&gt;&lt;ID&gt;anonymous&lt;/ID&gt;&lt;DisplayName/&gt;&lt;/Owner&gt;&lt;Buckets/&gt;&lt;/ListAllMyBucketsResult&gt;这就说明网关OK了，但是因为没有读写环境，所以暂时测不了。本文参考了ceph官方文档及漠然的ceph笔记(一)部分">
      
    
    
        
    
    <meta property="og:url" content="https://kevinguo.me/2017/09/06/kubernetes-ceph-1/">
    <meta property="og:site_name" content="KevinGuo">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh_CN" />
    
    <meta property="article:published_time" content="2017-09-06">
    
    <script src="/assets/vendor/jquery/dist/jquery.min.js"></script>
    <script src="/assets/js/jquery-ui.js"></script>
    <script type="text/javascript">
    function toggleMenu() {
        var nav = document.getElementsByClassName("site-header-nav")[0];
        if (nav.style.display == "inline-flex") {
          nav.style.display = "none";
        } else {
          nav.style.display = "inline-flex";
        }
    }
    </script>
</head>
<body class="" data-mz="">
    <header class="site-header">
        <div class="container">
            <h1><a href="/" title="KevinGuo"><span class="octicon octicon-mark-github"></span> KevinGuo</a></h1>
            <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <nav class="site-header-nav" role="navigation">
                
                <a href="/" class=" site-header-nav-item" target="" title="首页">首页</a>
                
                <a href="/categories/" class=" site-header-nav-item" target="" title="分类">分类</a>
                
                <a href="/wiki/" class=" site-header-nav-item" target="" title="维基">维基</a>
                
                <a href="/open-source/" class=" site-header-nav-item" target="" title="开源">开源</a>
                
                <a href="/links/" class=" site-header-nav-item" target="" title="链接">链接</a>
                
                <a href="/about/" class=" site-header-nav-item" target="" title="关于">关于</a>
                
            </nav>
        </div>
    </header>
    <!-- / header -->

    <section class="collection-head small geopattern" data-pattern-id="kubernetes ceph">
<div class="container">
  <div class="columns">
    <div class="column three-fourths">
      <div class="collection-title">
        <h1 class="collection-header">kubernetes ceph 笔记 1</h1>
        <div class="collection-info">
          
          <span class="meta-info">
            <span class="octicon octicon-calendar"></span> 2017/09/06
          </span>
          
          
          <span class="meta-info">
            <span class="octicon octicon-file-directory"></span>
            <a href="/categories/#kubernetes" title="kubernetes">kubernetes</a>
          </span>
          
          <span class="meta-info">
            <span class="octicon octicon-file-directory"></span>
            <a href="/categories/#ceph" title="ceph">ceph</a>
          </span>
          
        </div>
      </div>
    </div>
  </div>
</div>
</section>
<!-- / .banner -->
<section class="container content">
<div class="columns">
  <div class="column three-fourths" >
    <article class="article-content markdown-body">
    <blockquote>
  <p>该教程主要是为statefulset有状态服务集群提供持久化存储提供基础，在讲statefulset之前，我们先搭建我们的ceph集群
；具备了极好的可靠性、统一性；经过近几年的发展，ceph开辟了一个全新的数据存储途径。ceph具备了企业级存储的分布式、可大规模扩展、没有当节点故障等特点，越来越受人们的青睐。</p>
</blockquote>

<h3 id="简介">简介</h3>

<p>Ceph是一个符合POSIX、开源的分布式存储系统，不论你是想提供<code class="highlighter-rouge">ceph 对象存储</code>或<code class="highlighter-rouge">ceph 块设备</code>，还是想部署一个<code class="highlighter-rouge">ceph文件系统</code>或者把ceph作为他用，所有 <code class="highlighter-rouge">Ceph存储集群</code> 的部署都始于部署一个个 <code class="highlighter-rouge">ceph节点</code>、<code class="highlighter-rouge">网络</code>和<code class="highlighter-rouge">ceph存储集群</code>，ceph 存储集群至少需要一个 <code class="highlighter-rouge">ceph monitor</code>和两个<code class="highlighter-rouge">osd守护进程</code>。而运行<code class="highlighter-rouge">ceph文件系统客户端</code>，则必须需要MDS(元数据服务器)。</p>

<p>基础组件:</p>

<ul>
  <li>
    <p>Ceph OSDs: Ceph OSD 守护进程的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当Ceph 存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能到到<code class="highlighter-rouge">active+clean</code>状态(Ceph 默认有3个副本，你可以调整<code class="highlighter-rouge">osd poll default size</code>)</p>
  </li>
  <li>
    <p>Ceph Monitors: Ceph Monitor基于PAXOS算法维护着 展示集群状态的各种图表，包括监视器图、OSD图、PG图、CRUSH图。Ceph 保存着发生在Monitors、OSD和PG上的每一次状态变更的历史信息(称为 epoch)</p>
  </li>
  <li>
    <p>MDSs：Ceph MDS为 Ceph 文件系统存储元数据(也就是说，Ceph块设备和Ceph对象存储是不是用MDS)。MDS使得POSIX文件系统的用户们，可以在部队Ceph存储集群造成负担的前提下，执行诸如 <code class="highlighter-rouge">ls</code>、<code class="highlighter-rouge">find</code>等基本命令</p>
  </li>
</ul>

<p>下图展示了Ceph的基础架构图</p>

<p><img src="/images/posts/Ceph-soft-topu.png" alt="ceph-topu" /></p>

<h4 id="1基础存储系统rados">1.基础存储系统RADOS</h4>

<p>RADOS (Reliable Autonomic Distributed Object Store),这一层本身就是一个完整的对象存储系统，包括Ceph的基础服务(OSD,MON,MDS)，所有存储在ceph中的用户数据实际上最终都是由这一层来存储的。而Ceph的高可用，高扩展性，高自动化等特性本质上也是有这一层所提供的，因此，RADOS是ceph的核心精华部分。</p>

<h4 id="2基础库librados">2.基础库LibRados</h4>

<p>这一层是对RADOS进行抽象和封装，并向上层提供不同的API，这样上层的RBD、RGW、CephFS才能访问RADOS，RADOS所提供的原生librados API包括C和C++两种。</p>

<h4 id="3高层存储应用接口">3.高层存储应用接口</h4>

<p>这一层包含了RGW、RBD和CephFS这几个部分，其作用是在librados库的基础上提供抽象层次更高，更便于应用和用户端使用的上层接口。</p>

<ul>
  <li>
    <p>RGW: 是一个提供与S3和Swift兼容的RESTful API的gateway，以供对应的对象存储应用开发使用。通过RGW可以将RADOS响应转化为HTTP响应，同样也可以将外部的HTTP响应状花为RADOS调用。</p>
  </li>
  <li>
    <p>RBD：提供一个标准的块设备接口服务。</p>
  </li>
  <li>
    <p>CephFS: 提供一个POSIX兼容的分布式文件系统。</p>
  </li>
</ul>

<h4 id="4client层">4.client层</h4>

<p>这一层其实就是不同场景下对于ceph各个应用接口的各种应用方式，例如基于librados直接开发的对象存储应用，基于RGW开发的对象存储应用，基于RBD实现的云硬盘等等。</p>

<h4 id="5其他">5.其他</h4>

<p>一个Cluster逻辑上可以划分为多个Pool，一个Pool由若干个逻辑PG组成。</p>

<p>一个文件会被切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD，其中第一个OSD（Primary OSD）为主，其余是备，OSD间通过心跳来互相监控存活状态。</p>

<p>CRUSH： CRUSH是ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。</p>

<h3 id="一快速安装">一.快速安装</h3>

<h4 id="11-安装前准备">1.1 安装前准备</h4>

<p>所需机器如下</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">IP</th>
      <th style="text-align: center">HostName</th>
      <th style="text-align: center">OS</th>
      <th style="text-align: center">role</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">172.30.33.31</td>
      <td style="text-align: center">deploy-node</td>
      <td style="text-align: center">centos7.3.1611</td>
      <td style="text-align: center">deploy node</td>
    </tr>
    <tr>
      <td style="text-align: center">172.30.33.90</td>
      <td style="text-align: center">k8s-master01</td>
      <td style="text-align: center">centos7.3.1611</td>
      <td style="text-align: center">monitor osd node1</td>
    </tr>
    <tr>
      <td style="text-align: center">172.30.33.91</td>
      <td style="text-align: center">k8s-master02</td>
      <td style="text-align: center">centos7.3.1611</td>
      <td style="text-align: center">monitor osd node2</td>
    </tr>
    <tr>
      <td style="text-align: center">172.30.33.92</td>
      <td style="text-align: center">k8s-master03</td>
      <td style="text-align: center">centos7.3.1611</td>
      <td style="text-align: center">monitor osd node3</td>
    </tr>
    <tr>
      <td style="text-align: center">172.30.33.89</td>
      <td style="text-align: center">k8s-registry</td>
      <td style="text-align: center">centos7.3.1611</td>
      <td style="text-align: center">osd mds</td>
    </tr>
    <tr>
      <td style="text-align: center">172.30.33.93</td>
      <td style="text-align: center">k8s-node01</td>
      <td style="text-align: center">centos7.3.1611</td>
      <td style="text-align: center">osd mds</td>
    </tr>
    <tr>
      <td style="text-align: center">172.30.33.94</td>
      <td style="text-align: center">k8s-node02</td>
      <td style="text-align: center">centos7.3.1611</td>
      <td style="text-align: center">osd mds</td>
    </tr>
  </tbody>
</table>

<h5 id="在管理节点上操作">在管理节点上操作</h5>
<p>1.添加ceph源</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>ceph-noarch]
<span class="nv">name</span><span class="o">=</span>Ceph noarch packages
<span class="nv">baseurl</span><span class="o">=</span>https://download.ceph.com/rpm-jewel/el7/noarch
<span class="nv">enabled</span><span class="o">=</span>1
<span class="nv">gpgcheck</span><span class="o">=</span>1
<span class="nb">type</span><span class="o">=</span>rpm-md
<span class="nv">gpgkey</span><span class="o">=</span>https://download.ceph.com/keys/release.asc

<span class="nv">$ </span>yum update <span class="nt">-y</span> <span class="o">&amp;&amp;</span> yum install ceph-deploy <span class="nt">-y</span>
</code></pre></div></div>

<p>2.更新并安装<code class="highlighter-rouge">ceph-deploy</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>yum update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>yum install ceph-deploy
</code></pre></div></div>

<p>2.配置从部署机器到所有其他节点的免密钥登录，具体参考<a href="https://kevinguo.me/2017/07/06/ansible-client/">这里</a></p>

<h5 id="在节点上操作">在节点上操作</h5>

<p>1.安装epel源</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>yum install yum-plugin-priorities
<span class="nv">$ </span>yum install epel-release <span class="nt">-y</span>
</code></pre></div></div>

<p>2.校对时间，由于ceph使用Paxos算法保证数据一致性，所以安装前要先保证各个节点的时间同步</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>yum install ntp ntpdate ntp-doc

<span class="nv">$ </span>ntpdate 0.cn.pool.ntp.org
</code></pre></div></div>

<p>3.开放所需端口或关闭防火墙</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>system stop firewalld
<span class="nv">$ </span><span class="nb">sudo </span>firewall-cmd <span class="nt">--zone</span><span class="o">=</span>public <span class="nt">--add-port</span><span class="o">=</span>6789/tcp <span class="nt">--permanent</span>
</code></pre></div></div>

<p>4.关闭selinux</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>setenforce 0
</code></pre></div></div>

<h4 id="12-创建集群">1.2 创建集群</h4>

<p>1.由于ceph-deploy工具部署集群前需要创建一些集群配置信息，其保存在<code class="highlighter-rouge">ceph.conf</code>文件中，这个文件将来会被复制到每个节点的 <code class="highlighter-rouge">/etc/ceph/ceph.conf</code></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建集群配置目录</span>
mkdir ceph-cluster <span class="o">&amp;&amp;</span> <span class="nb">cd </span>ceph-cluster
<span class="c"># 创建 monitor-node</span>
ceph-deploy new k8s-master01
<span class="c"># 追加 OSD 副本数量(测试虚拟机总共有3台)</span>
<span class="nb">echo</span> <span class="s2">"osd pool default size = 3"</span> <span class="o">&gt;&gt;</span> ceph.conf
</code></pre></div></div>

<p>2.创建集群使用 <code class="highlighter-rouge">ceph-deploy</code>工具在部署节点上执行即可</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 安装ceph</span>
<span class="nv">$ </span>ceph-deploy install k8s-master01 k8s-master02 k8s-master03
</code></pre></div></div>
<p><strong>注意：在部署节点部署的时候，可能会因为网络原因导致无法安装ceph和ceph-radosgw，这时候，我们在各个节点上手动安装一下</strong></p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 添加ceph 源</span>
<span class="o">[</span>Ceph]
<span class="nv">name</span><span class="o">=</span>Ceph packages <span class="k">for</span> <span class="nv">$basearch</span>
<span class="nv">baseurl</span><span class="o">=</span>http://download.ceph.com/rpm-jewel/el7/<span class="nv">$basearch</span>
<span class="nv">enabled</span><span class="o">=</span>1
<span class="nv">gpgcheck</span><span class="o">=</span>1
<span class="nb">type</span><span class="o">=</span>rpm-md
<span class="nv">gpgkey</span><span class="o">=</span>https://download.ceph.com/keys/release.asc
<span class="nv">priority</span><span class="o">=</span>1

<span class="o">[</span>Ceph-noarch]
<span class="nv">name</span><span class="o">=</span>Ceph noarch packages
<span class="nv">baseurl</span><span class="o">=</span>http://download.ceph.com/rpm-jewel/el7/noarch
<span class="nv">enabled</span><span class="o">=</span>1
<span class="nv">gpgcheck</span><span class="o">=</span>1
<span class="nb">type</span><span class="o">=</span>rpm-md
<span class="nv">gpgkey</span><span class="o">=</span>https://download.ceph.com/keys/release.asc
<span class="nv">priority</span><span class="o">=</span>1

<span class="o">[</span>ceph-source]
<span class="nv">name</span><span class="o">=</span>Ceph <span class="nb">source </span>packages
<span class="nv">baseurl</span><span class="o">=</span>http://download.ceph.com/rpm-jewel/el7/SRPMS
<span class="nv">enabled</span><span class="o">=</span>1
<span class="nv">gpgcheck</span><span class="o">=</span>1
<span class="nb">type</span><span class="o">=</span>rpm-md
<span class="nv">gpgkey</span><span class="o">=</span>https://download.ceph.com/keys/release.asc
<span class="nv">priority</span><span class="o">=</span>1


<span class="nv">$ </span>yum install ceph ceph-radosgw <span class="nt">-y</span>
</code></pre></div></div>

<p>3.初始化monitor node 和密钥文件</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ceph-deploy mon create-initial
</code></pre></div></div>

<p>4.在osd节点上创建一个目录作为 osd 存储，并修改其权限,千万别创建在<code class="highlighter-rouge">/usr/local</code>目录下，否则，你的osd可能会无法挂载</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ssh k8s-master01
<span class="nv">$ </span><span class="nb">sudo </span>mkdir /data/osd1
<span class="nv">$ </span>chown <span class="nt">-R</span> ceph:ceph osd1
<span class="nv">$ </span><span class="nb">exit</span>

<span class="nv">$ </span>ssh k8s-master2
<span class="nv">$ </span><span class="nb">sudo </span>mkdir /data/osd2
<span class="nv">$ </span>chown <span class="nt">-R</span> ceph:ceph osd2
<span class="nv">$ </span><span class="nb">exit</span>

<span class="nv">$ </span>ssh k8s-master3
<span class="nv">$ </span><span class="nb">sudo </span>mkdir /data/osd3
<span class="nv">$ </span>chown <span class="nt">-R</span> ceph:ceph osd3
<span class="nv">$ </span><span class="nb">exit</span>
</code></pre></div></div>

<p>5.然后，在管理节点上初始化 osd</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ceph-deploy osd prepare k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd3
</code></pre></div></div>

<p>6.最后，在管理节点上激活 osd</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ceph-deploy osd activate k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd3
</code></pre></div></div>

<p>7.在管理节点上部署 ceph cli 工具和密钥文件</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ceph-deploy admin k8s-master01 k8s-master02 k8s-master03
</code></pre></div></div>

<p>8.确保你对 <code class="highlighter-rouge">ceph.client.admin.keyring</code>有正确的操作权限，在每个节点上执行</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>chmod +r /etc/ceph/ceph.client.admin.keyring
</code></pre></div></div>

<p>9.最后，检查集群的健康状态</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ceph health
HEALTH_OK

<span class="nv">$ </span>ceph osd tree
ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY
<span class="nt">-1</span> 0.14067 root default                                            
<span class="nt">-2</span> 0.04689     host k8s-master01                                   
 0 0.04689         osd.0              up  1.00000          1.00000
<span class="nt">-3</span> 0.04689     host k8s-master02                                   
 1 0.04689         osd.1              up  1.00000          1.00000
<span class="nt">-4</span> 0.04689     host k8s-master03                                   
 2 0.04689         osd.2              up  1.00000          1.00000
</code></pre></div></div>

<p>我们看到状态是OK的，而且每个节点上的osd的状态都是up的</p>

<p>如果在某些地方碰到麻烦，想从头再来，可以用下列命令来清楚配置：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ceph-deploy purgedata <span class="o">{</span>ceph-node<span class="o">}</span> <span class="o">[{</span>ceph-node<span class="o">}]</span>
<span class="nv">$ </span>ceph-deploy forgetkeys
</code></pre></div></div>

<p>用下列命令可以连Ceph安装包一起清除：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ceph-deploy purge <span class="o">{</span>ceph-node<span class="o">}</span> <span class="o">[{</span>ceph-node<span class="o">}]</span>
</code></pre></div></div>

<h3 id="二操作集群">二.操作集群</h3>

<h4 id="21-基础操作">2.1 基础操作</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建MDS</span>
<span class="nv">$ </span>ceph-deploy mds create <span class="o">{</span>ceph-node<span class="o">}</span>

<span class="c"># 创建RGW</span>
<span class="nv">$ </span>ceph-deploy rgw create <span class="o">{</span>ceph-node<span class="o">}</span>

<span class="c"># 添加mon</span>
<span class="nv">$ </span><span class="nb">echo</span> <span class="s2">"public network = 172.30.33.0/24"</span> <span class="o">&gt;&gt;</span> ceph.conf
<span class="nv">$ </span>ceph-deploy mon add <span class="o">{</span>ceph-node<span class="o">}</span>

<span class="c"># 查看仲裁</span>
<span class="nv">$ </span>ceph quorum_status <span class="nt">--format</span> json-pretty
</code></pre></div></div>

<h4 id="22-对象存储测试">2.2 对象存储测试</h4>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建pool</span>
<span class="c"># rados mkpool {pool-name}</span>
<span class="nv">$ </span>rados mkpool test-pool

<span class="c"># 创建测试文件</span>
<span class="nv">$ </span>dd <span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>testfile <span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1

<span class="c"># 创建一个对象(这时候也将对象放入了pool中)</span>
<span class="c"># rados put {object-name} {file-path} --pool={pool-name}</span>
<span class="nv">$ </span>rados put test-object testfile <span class="nt">--pool</span><span class="o">=</span>test-pool

<span class="c"># 检查存储池，确认ceph存储了此对象</span>
<span class="nv">$ </span>rados <span class="nt">-p</span> test-pool <span class="nb">ls</span>

<span class="c"># 定位对象，会输出对象位置</span>
<span class="nv">$ </span>ceph osd map test-pool test-object
osdmap e42 pool <span class="s1">'test-pool'</span> <span class="o">(</span>3<span class="o">)</span> object <span class="s1">'test-file'</span> -&gt; pg 3.b79653d4 <span class="o">(</span>3.4<span class="o">)</span> -&gt; up <span class="o">([</span>1,5,2,3,4], p1<span class="o">)</span> acting <span class="o">([</span>1,5,2,3,4], p1<span class="o">)</span>

<span class="c"># 删除对象</span>
<span class="nv">$ </span>rados <span class="nt">-p</span> data rm test-object

<span class="c"># 删除存储池</span>
<span class="nv">$ </span>rados rmpool test-pool test-pool <span class="nt">--yes-i-really-really-mean-it</span>
</code></pre></div></div>
<p>随着集群的运行，对象的位置可能会动态改变。Ceph有动态均衡机制，无需手动干预即可完成。</p>

<h4 id="23-块存储测试">2.3 块存储测试</h4>

<p><strong>官方建议使用RBD的客户端最好不要和OSD在同一台物理机上(除非它们都是VM)</strong></p>

<p>1.确认你使用了合适的内核版本，详情<a href="http://docs.ceph.com/docs/master/start/os-recommendations/">参见</a></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lab_release <span class="nt">-a</span>
uname <span class="nt">-a</span>
</code></pre></div></div>

<p>2.在管理节点上用 <code class="highlighter-rouge">ceph-deploy</code>安装ceph</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph-deploy install <span class="o">{</span>rbd-client<span class="o">}</span>
</code></pre></div></div>

<p>3.在管理节点上部署ceph cli工具和密钥</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph-deploy admin <span class="o">{</span>rbd-client<span class="o">}</span>
</code></pre></div></div>

<p>4.在rbd节点上创建块设备image</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rbd create test-block <span class="nt">--size</span> 4096
</code></pre></div></div>

<p>5.映射image到块设备</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rbd map test-block <span class="nt">--name</span> client.admin
</code></pre></div></div>

<p><strong>在上面的map映射操作时，会出现如下报错</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>rbd map test-block <span class="nt">--name</span> client.admin
rbd: sysfs write failed
RBD image feature <span class="nb">set </span>mismatch. You can disable features unsupported by the kernel with <span class="s2">"rbd feature disable"</span><span class="nb">.</span>
In some cases useful info is found <span class="k">in </span>syslog - try <span class="s2">"dmesg | tail"</span> or so.
rbd: map failed: <span class="o">(</span>6<span class="o">)</span> No such device or address
</code></pre></div></div>

<p><strong>大致意思是说features不匹配，可以通过disable features关掉一些特性来让内核支持。这是因为在Ceph高本本进行 map image时，默认ceph在创建image(上文test-block)时，会增加很多features，这些features需要内核支持，centos7上的支持有限，所以，我们需要关掉一些</strong></p>

<p>我们可以用 <code class="highlighter-rouge">rbd info data</code> 看看创建的image目前有哪些features</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>rbd info test-block
rbd image <span class="s1">'test-block'</span>:
	size 4096 MB <span class="k">in </span>1024 objects
	order 22 <span class="o">(</span>4096 kB objects<span class="o">)</span>
	block_name_prefix: rbd_data.10bb238e1f29
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	flags:
</code></pre></div></div>

<p>在features中，我们可以看到默认开启了很多：</p>

<ul>
  <li>layering 支持分层</li>
  <li>exclusive-lock 支持独占锁</li>
  <li>object-map 支持对象映射(依赖exclusive-lock)</li>
  <li>fast-diff 快速计算差异(依赖object-map)</li>
  <li>deep-flatten 支持快照扁平化操作</li>
</ul>

<p><strong>而实际上在CentOS7的3.10内核中只支持layering，所以我们需要手动关闭一些features，然后重新map；如果想要一劳永逸，可以在 ceph.conf 中加入 rbd_default_features = 1 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)</strong></p>

<p>6.关闭不支持的特性之后重新map</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 关闭不支持的features</span>
<span class="nv">$ </span>rbd feature disable test-block exclusive-lock, object-map, fast-diff, deep-flatten

<span class="c"># 重新map</span>
<span class="nv">$ </span>rbd map test-block <span class="nt">--name</span> client.admin
/dev/rbd0
</code></pre></div></div>

<p>7.格式化之后挂载到系统目录</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 格式化</span>
<span class="nv">$ </span>mkfs.xfs /dev/rbd0
meta-data<span class="o">=</span>/dev/rbd0              <span class="nv">isize</span><span class="o">=</span>512    <span class="nv">agcount</span><span class="o">=</span>9, <span class="nv">agsize</span><span class="o">=</span>130048 blks
         <span class="o">=</span>                       <span class="nv">sectsz</span><span class="o">=</span>512   <span class="nv">attr</span><span class="o">=</span>2, <span class="nv">projid32bit</span><span class="o">=</span>1
         <span class="o">=</span>                       <span class="nv">crc</span><span class="o">=</span>1        <span class="nv">finobt</span><span class="o">=</span>0, <span class="nv">sparse</span><span class="o">=</span>0
data     <span class="o">=</span>                       <span class="nv">bsize</span><span class="o">=</span>4096   <span class="nv">blocks</span><span class="o">=</span>1048576, <span class="nv">imaxpct</span><span class="o">=</span>25
         <span class="o">=</span>                       <span class="nv">sunit</span><span class="o">=</span>1024   <span class="nv">swidth</span><span class="o">=</span>1024 blks
naming   <span class="o">=</span>version 2              <span class="nv">bsize</span><span class="o">=</span>4096   ascii-ci<span class="o">=</span>0 <span class="nv">ftype</span><span class="o">=</span>1
log      <span class="o">=</span>internal log           <span class="nv">bsize</span><span class="o">=</span>4096   <span class="nv">blocks</span><span class="o">=</span>2560, <span class="nv">version</span><span class="o">=</span>2
         <span class="o">=</span>                       <span class="nv">sectsz</span><span class="o">=</span>512   <span class="nv">sunit</span><span class="o">=</span>8 blks, lazy-count<span class="o">=</span>1
realtime <span class="o">=</span>none                   <span class="nv">extsz</span><span class="o">=</span>4096   <span class="nv">blocks</span><span class="o">=</span>0, <span class="nv">rtextents</span><span class="o">=</span>0

<span class="c"># 挂载</span>
<span class="nv">$ </span>mkdir test-block
<span class="nv">$ </span>mount /dev/rbd0 test-block

<span class="c"># 写入测试</span>
<span class="nv">$ </span>dd <span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>test-block/test-file <span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1
1+0 records <span class="k">in
</span>1+0 records out
1073741824 bytes <span class="o">(</span>1.1 GB<span class="o">)</span> copied, 2.96071 s, 363 MB/s

<span class="nv">$ </span><span class="nb">ls </span>test-block/
test-file
</code></pre></div></div>

<h4 id="24-cephfs-测试">2.4 CephFS 测试</h4>

<p>1.创建MDS</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>ceph-deploy mds create k8s-node01 k8s-node02 k8s-registry
</code></pre></div></div>

<p>2.创建pool和fs，创建pool需要指定PG数量</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ceph osd pool create cephfs_data 32
ceph osd pool create cephfs_metadata 32
ceph fs new test-fs cephfs_metadata cephfs_data
</code></pre></div></div>

<p><strong>PG 概念：</strong></p>

<blockquote>
  <p>当Ceph集群接受到存储请求时，ceph会将一个文件会切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD(根据副本数)；一般来说增加PG的数量能降低OSD负载，一般每个OSD大约分配50～100PG，关于PG数量指定，一般遵循以下公式</p>
  <ul>
    <li>集群PG总数 = (OSD总数 * 100)/数据最大副本数</li>
    <li>单个存储池PG数 = (OSD总数 * 100)/数据最大副本数/存储池数</li>
  </ul>
</blockquote>

<p><strong>注意：PG的最终结果应当以最接近以上计算公式的2的N次幂(向上取值)；如我的虚拟机环境的每个存储池 PG数 = 6(OSD) * 100 / 5(副本数) / 4（4个存储池）= 30，向上取2的N次幂为32(即，2的5次方=32，最接近30)</strong></p>

<p>3.挂载CephFS有两种方式，一种是使用内核驱动挂载，一种是使用 <code class="highlighter-rouge">ceph-fuse</code>用户空间挂载</p>

<p>内核挂载需要提取ceph管理key，方式如下：</p>

<p>在密钥文件中找到与某用户对于的密钥</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">cat</span> /etc/ceph/ceph.client.admin.keyring
<span class="o">[</span>client.admin]
	key <span class="o">=</span> <span class="nv">AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA</span><span class="o">==</span>
</code></pre></div></div>

<p>复制密钥到文件中保存，并确保其权限</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA=="</span> <span class="o">&gt;</span> ceph-key
</code></pre></div></div>

<p>创建目录挂载</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>mkdir test-fs
mount <span class="nt">-t</span> ceph 172.30.33.90:6789:/ /root/test-fs <span class="nt">-o</span> <span class="nv">name</span><span class="o">=</span>admin,secretfile<span class="o">=</span>ceph-key

<span class="c">#写入数据测试</span>
<span class="nv">$ </span>dd <span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>test-fs/test-fs <span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1
1+0 records <span class="k">in
</span>1+0 records out
1073741824 bytes <span class="o">(</span>1.1 GB<span class="o">)</span> copied, 2.77355 s, 387 MB/s
</code></pre></div></div>

<p><code class="highlighter-rouge">ceph-fuse</code>用户空间挂载的方式也比较简单，需要先安装ceph-fuse，同时也需要key</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 按照前面的步骤添加ceph源</span>
<span class="nv">$ </span>vi /etc/yum.repos.d/ceph.repo
<span class="o">[</span>ceph]
<span class="nv">name</span><span class="o">=</span>ceph
<span class="nv">baseurl</span><span class="o">=</span>http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/
<span class="nv">gpgcheck</span><span class="o">=</span>0
<span class="o">[</span>ceph-noarch]
<span class="nv">name</span><span class="o">=</span>cephnoarch
<span class="nv">baseurl</span><span class="o">=</span>http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/
<span class="nv">gpgcheck</span><span class="o">=</span>0

<span class="c"># 安装ceph-fuse</span>
yum install ceph-fuse <span class="nt">-y</span>

<span class="c"># 复制配置和ceph key到client端</span>
<span class="nb">sudo </span>mkdir <span class="nt">-p</span> /etc/ceph
<span class="nb">sudo </span>scp root@172.30.33.91:/etc/ceph/ceph.conf /etc/ceph/ceph.conf
<span class="nb">sudo </span>scp root@172.30.33.91:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring

<span class="c"># 创建目录挂载</span>
mkdir test-fs-fuse
<span class="nv">$ </span><span class="nb">sudo </span>ceph-fuse <span class="nt">-m</span> 172.30.33.91:6789 test-fs-fuse
ceph-fuse[60551]: starting ceph client
2017-09-12 14:47:24.137929 7f63d9719ec0 <span class="nt">-1</span> init, newargv <span class="o">=</span> 0x7f63e51d4840 <span class="nv">newargc</span><span class="o">=</span>11
ceph-fuse[60551]: starting fuse

<span class="c"># 写入数据测试</span>
<span class="nv">$ </span><span class="nb">sudo </span>dd <span class="k">if</span><span class="o">=</span>/dev/zero <span class="nv">of</span><span class="o">=</span>test-fs-fuse/test-fs-fuse <span class="nv">bs</span><span class="o">=</span>1G <span class="nv">count</span><span class="o">=</span>1
1+0 records <span class="k">in
</span>1+0 records out
1073741824 bytes <span class="o">(</span>1.1 GB<span class="o">)</span> copied, 10.5426 s, 102 MB/s

<span class="c"># 查看确认，发现我们上面通过内核挂载的文件也还在</span>
<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-lh</span>
total 2.0G
<span class="nt">-rw-r--r--</span> 1 root root 1.0G Sep 12 13:59 test-fs
<span class="nt">-rw-r--r--</span> 1 root root 1.0G Sep 12 14:49 test-fs-fuse
</code></pre></div></div>

<h4 id="25-ceph对象网关">2.5 Ceph对象网关</h4>

<p>1.对象网关创建</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 创建RGW</span>
<span class="nv">$ </span>ceph-deploy rgw create k8s-node02
</code></pre></div></div>

<p>2.直接访问<code class="highlighter-rouge">http://ceph-node-ip:7480</code>返回结果如下</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;ListAllMyBucketsResult</span> <span class="na">xmlns=</span><span class="s">"http://s3.amazonaws.com/doc/2006-03-01/"</span><span class="nt">&gt;</span>
<span class="nt">&lt;Owner&gt;</span>
<span class="nt">&lt;ID&gt;</span>anonymous<span class="nt">&lt;/ID&gt;</span>
<span class="nt">&lt;DisplayName/&gt;</span>
<span class="nt">&lt;/Owner&gt;</span>
<span class="nt">&lt;Buckets/&gt;</span>
<span class="nt">&lt;/ListAllMyBucketsResult&gt;</span>
</code></pre></div></div>

<p>这就说明网关OK了，但是因为没有读写环境，所以暂时测不了。</p>

<p>本文参考了<a href="http://docs.ceph.com/docs/master/start/">ceph官方文档</a>及漠然的<a href="https://mritd.me/2017/05/27/ceph-note-1/">ceph笔记(一)</a>部分</p>

    </article>
    <div class="share">
      <div class="share-component"></div>
    </div>
    <div class="comment">
      

  

  
      
        
        <!-- Disqus Protection, see https://github.com/mzlogin/mzlogin.github.io/issues/2 -->
        
        
          <div id="disqus_thread"></div>
          <script>
            var disqus_config = function () {
              this.page.url = 'https://kevinguo.me/2017/09/06/kubernetes-ceph-1/';
              this.page.identifier = '/2017/09/06/kubernetes-ceph-1/';
              this.page.title = 'kubernetes ceph 笔记 1';
            };
            (function() { // DON'T EDIT BELOW THIS LINE
              var d = document, s = d.createElement('script');

              s.type = 'text/javascript';
              s.async = true;
              var shortname = 'kevinguo';

              s.src = '//' + shortname + '.disqus.com/embed.js';

              s.setAttribute('data-timestamp', +new Date());
              (d.head || d.body).appendChild(s);
            })();
          </script>
          <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
        
      
    


    </div>
  </div>
  <div class="column one-fourth">
    
<h3>Search</h3>
<div id="site_search">
    <input type="text" id="search_box" placeholder="Search">
    <button class="btn btn-default" id="site_search_do"><span class="octicon octicon-search"></span></button>
</div>

<ul id="search_results"></ul>

<link rel="stylesheet" type="text/css" href="/assets/css/modules/sidebar-search.css">
<script src="/assets/js/lunr.min.js"></script>
<script src="/assets/js/search.js"></script>


    

    
<h3 class="post-directory-title mobile-hidden">Table of Contents</h3>
<div id="post-directory-module" class="mobile-hidden">
  <section class="post-directory">
  <!-- Links that trigger the jumping -->
  <!-- Added by javascript below -->
  <dl></dl>
  </section>
</div>

<script src="/assets/js/jquery.toc.js"></script>

  </div>
</div>
</section>
<!-- /section.content -->

    <footer class="container">
        <div class="site-footer" role="contentinfo">
            <div class="copyright left mobile-block">
                    © 2015
                    <span title="KevinGuo">KevinGuo</span>
                    <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
            </div>

            <ul class="site-footer-links right mobile-hidden">
                <li>
                    <a href="javascript:window.scrollTo(0,0)" >TOP</a>
                </li>
            </ul>
            <a href="http://github.com/chinakevinguo/chinakevinguo.github.io" target="_blank" aria-label="view source code">
                <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
            </a>
            <ul class="site-footer-links mobile-hidden">
                
                <li>
                    <a href="/" title="首页" target="">首页</a>
                </li>
                
                <li>
                    <a href="/categories/" title="分类" target="">分类</a>
                </li>
                
                <li>
                    <a href="/wiki/" title="维基" target="">维基</a>
                </li>
                
                <li>
                    <a href="/open-source/" title="开源" target="">开源</a>
                </li>
                
                <li>
                    <a href="/links/" title="链接" target="">链接</a>
                </li>
                
                <li>
                    <a href="/about/" title="关于" target="">关于</a>
                </li>
                
                <li><a href="/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li>
            </ul>

        </div>
    </footer>
    <!-- / footer -->
    <script src="/assets/vendor/share.js/dist/js/share.min.js"></script>
    <script src="/assets/js/geopattern.js"></script>
    <script src="/assets/js/prism.js"></script>
    <link rel="stylesheet" href="/assets/css/globals/prism.css">
    <script>
      jQuery(document).ready(function($) {
        // geopattern
        $('.geopattern').each(function(){
          $(this).geopattern($(this).data('pattern-id'));
        });
       // hljs.initHighlightingOnLoad();
      });
    </script>
    
    <div style="display:none">
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-80669434-1', 'auto');
        ga('send', 'pageview');

      </script>
    </div>
    
</body>
</html>
