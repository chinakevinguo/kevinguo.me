<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="https://kevinguo.me/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kevinguo.me/" rel="alternate" type="text/html" /><updated>2018-03-19T15:28:37+08:00</updated><id>https://kevinguo.me/</id><title type="html">KevinGuo</title><subtitle>KevinGuo's blog</subtitle><author><name>KevinGuo</name></author><entry><title type="html">Linux top 详解</title><link href="https://kevinguo.me/2018/02/09/Linux-top/" rel="alternate" type="text/html" title="Linux top 详解" /><published>2018-02-09T00:00:00+08:00</published><updated>2018-02-09T00:00:00+08:00</updated><id>https://kevinguo.me/2018/02/09/Linux-top</id><content type="html" xml:base="https://kevinguo.me/2018/02/09/Linux-top/">&lt;blockquote&gt;
  &lt;p&gt;一直都对服务性能这块的东西不怎么感冒，但是，有一次面试的时候，被问到了，突然发现自己对这些基础的知识点，好匮乏，正好今天在学python的psutil模块的时候，看到了cpu_times，顺便记录下关于top这个命令的内容；top这个命令其实很多人都会用，但是用的好的人却不多，甚至有人会对监控视图中的内容含义有不少曲解。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;先来一份top命令的结果，后面会详细解读&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;top - 18:02:01 up  1:06,  1 user,  load average: 0.22, 0.12, 0.10
Tasks: 200 total,   2 running, 198 sleeping,   0 stopped,   0 zombie
%Cpu&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:  3.0 us,  1.0 sy,  0.0 ni, 96.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem :  4039816 total,  1270872 free,  1432504 used,  1336440 buff/cache
KiB Swap:   385836 total,   385836 free,        0 used.  2321064 avail Mem

PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND   
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;第一行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;top - 18:02:01 up  1:06,  1 user,  load average: 0.22, 0.12, 0.10&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
18:02:01 -- 当前系统时间

1:06 -- 系统已经运行了多长时间

1 user -- 当前有一个用户登录系统

load average: 0.22, 0.12, 0.10 -- load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5就表明系统在超负荷运转了&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;第二行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Tasks: 200 total,   2 running, 198 sleeping,   0 stopped,   0 zombie&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
200 total -- 系统现在一共有多少个进程

2 running -- 其中处于运行中的有几个

198 sleeping -- 处于休眠状态的有多少个

0 stopped -- 处于停止状态的有多少个

0 zombie -- 僵尸进程有多少个
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;第三行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;%Cpu(s):  3.0 us,  1.0 sy,  0.0 ni, 96.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
3.0 us -- 用户空间占用CPU的百分比

1.0 sy -- 内核空间占用CPU的百分比

0.0 ni -- 用户进程空间内改变过优先级的进程占用CPU的百分比

96.0 id -- 空闲CPU百分比

0.0 wa -- IO等待占用CPU百分比

0.0 hi -- 硬中断&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Hardware IRQ&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;占用CPU百分比

0.0 si -- 软中断&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Software Interrupts&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;占用CPU百分比

0.0 st -- 虚拟机被hypervisor偷去的CPU时间
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;第四行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;KiB Mem :  4039816 total,  1270872 free,  1432504 used,  1336440 buff/cache&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
4039816 total -- 物理总内存

1270872 free -- 空闲内存总量

1432504 used -- 使用内存量

1336440 buff/cache -- 用作内核缓存的内存量
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;第五行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;KiB Swap:   385836 total,   385836 free,        0 used.  2321064 avail Mem&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
385836 total -- 交换区总量

385836 free -- 空闲的交换区总量

0 used -- 使用的交换区量

2321064 avail Mem -- 可用于进程下一次分配的物理内存数量
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;第七行：&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
PID -- 进程ID

USER -- 进程所有者

PR -- 进程优先级

NI -- nice值。负值表示高优先级，正值表示低优先级

VIRT -- 进程使用的虚拟内存总量 VIRT &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; SWAP+RES

RES -- 进程使用的，未被换出的物理内存大小

SHR -- 共享内存大小

S -- 进程状态 &lt;span class=&quot;nv&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;运行，S&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;睡眠，T&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;跟踪/停止，Z&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;僵尸经常，D&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;不可中断的睡眠状态

%CPU -- 上次更新到现在的CPU时间占用百分比

%MEM -- 进程使用的物理内存百分比

TIME+ 进程使用的CPU时间总计

COMMAND - 进程名称&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;命令名/命令行&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;交互的时候，使用P，以CPU百分比大小进行排序，使用M，以内存大小进行排序，使用T，以时间进行排序&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;最后记录下如何用python来计算top里面的各个利用率&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
top上的cpu利用率，大致算法如下

CPU总时间2&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;user2+system2+nice2+idle2+iowait2+irq2+softirq2

CPU总时间1&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;user1+system1+nice1+idle1+iowait1+irq1+softirq1

用户cpu利用率 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; user_pass &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; 100% / &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;CPU总时间2 - CPU总时间1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

内核cpu利用率 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; system_pass &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; 100% / &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;CPU总时间2 - CPU总时间1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

总的cpu利用率&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 用户cpu利用率 + 内核cpu利用率
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cs1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;psutil&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_times&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cu1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psutil&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_times&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cs2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;psutil&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_times&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cu2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;psutil&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_times&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cs2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cs1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cu2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cu1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name>KevinGuo</name></author><summary type="html">一直都对服务性能这块的东西不怎么感冒，但是，有一次面试的时候，被问到了，突然发现自己对这些基础的知识点，好匮乏，正好今天在学python的psutil模块的时候，看到了cpu_times，顺便记录下关于top这个命令的内容；top这个命令其实很多人都会用，但是用的好的人却不多，甚至有人会对监控视图中的内容含义有不少曲解。</summary></entry><entry><title type="html">python 多重继承之拓扑排序</title><link href="https://kevinguo.me/2018/01/19/python-topological-sorting/" rel="alternate" type="text/html" title="python 多重继承之拓扑排序" /><published>2018-01-19T00:00:00+08:00</published><updated>2018-01-19T00:00:00+08:00</updated><id>https://kevinguo.me/2018/01/19/python-topological-sorting</id><content type="html" xml:base="https://kevinguo.me/2018/01/19/python-topological-sorting/">&lt;blockquote&gt;
  &lt;p&gt;最近在学python，学到class 多重继承，降到了c3算法，这里记录一下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;一什么是拓扑排序&quot;&gt;一、什么是拓扑排序&lt;/h3&gt;

&lt;p&gt;在图论中，&lt;strong&gt;拓扑排序(Topological Sorting)&lt;/strong&gt; 是一个 &lt;strong&gt;有向无环图(DAG,Directed Acyclic Graph)&lt;/strong&gt; 的所有顶点的线性序列。且该序列必须满足下面两个条件：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;每个顶点出现且只出现一次。&lt;/li&gt;
  &lt;li&gt;若存在一条从顶点A到顶点B的路径，那么在序列中顶点A出现在顶点B的前面。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;例如，下面这个图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/original.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;它是一个DAG图，那么如何写出它的拓扑顺序呢？这里说一种比较常用的方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;从DAG途中选择一个没有前驱(即入度为0)的顶点并输出&lt;/li&gt;
  &lt;li&gt;从图中删除该顶点和所有以它为起点的有向边。&lt;/li&gt;
  &lt;li&gt;重复1和2直到当前DAG图为空或当前途中不存在无前驱的顶点为止。后一种情况说明有向图中必然存在环。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/topological-sorting.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;于是，得到拓扑排序后的结果是{1,2,4,3,5}&lt;/p&gt;

&lt;p&gt;下面，我们看看拓扑排序在python多重继承中的例子&lt;/p&gt;

&lt;h3 id=&quot;二python-多重继承&quot;&gt;二、python 多重继承&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python3&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'A foo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'A bar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'B foo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'B bar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;C1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;C2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C2-bar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__mro__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;首先，我们根据上面的继承关系构成一张图，如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/python-inherit.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;找到入度为0的点，只有一个D，把D拿出来，把D相关的边剪掉&lt;/li&gt;
  &lt;li&gt;现在有两个入度为0的点(C1,C2)，取最左原则，拿C1，剪掉C1相关的边，这时候的排序是{D,C1}&lt;/li&gt;
  &lt;li&gt;现在我们看，入度为0的点(C2),拿C2,剪掉C2相关的边，这时候排序是{D,C1,C2}&lt;/li&gt;
  &lt;li&gt;接着看，入度为0的点(A,B),取最左原则，拿A，剪掉A相关的边，这时候的排序是{D,C1,C2,A}&lt;/li&gt;
  &lt;li&gt;继续，入度哦为0的点只有B，拿B，剪掉B相关的边，最后只剩下object&lt;/li&gt;
  &lt;li&gt;所以最后的排序是{D,C1,C2,A,B,object}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们执行上面的代码，发现&lt;code class=&quot;highlighter-rouge&quot;&gt;print(D.__mro__)&lt;/code&gt;的结果也正是这样，而这也就是多重继承所使用的C3算法啦&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;为了进一步熟悉这个拓扑排序的方法，我们再来一张图，试试看排序结果是怎样的，它继承的内容是否如你所想&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python3&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'A foo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'A bar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'B foo'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'B bar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;C1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;C2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'C2-bar'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'__main__'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__mro__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;还是先根据继承关系构一个继承图&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/python-inherit2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;找到入度为0的顶点，只有一个D，拿D，剪掉D相关的边&lt;/li&gt;
  &lt;li&gt;得到两个入度为0的顶点(C1,C2),根据最左原则，拿C1，剪掉C1相关的边，这时候序列为{D,C1}&lt;/li&gt;
  &lt;li&gt;接着看，入度为0的顶点有两个(A,C1),根据最左原则，拿A，剪掉A相关的边，这时候序列为{D,C1,A}&lt;/li&gt;
  &lt;li&gt;接着看，入度为0的顶点为C2,拿C2，剪掉C2相关的边，这时候序列为{D,C1,A,C2}&lt;/li&gt;
  &lt;li&gt;继续，入度为0的顶点为B，拿B，剪掉B相关的边，最后还有一个object&lt;/li&gt;
  &lt;li&gt;所以最后的序列为{D,C1,A,C2,B,object}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;最后，我们执行上面的代码，发现&lt;code class=&quot;highlighter-rouge&quot;&gt;print(D.__mro__)&lt;/code&gt;的结果正如上面所计算的结果&lt;/p&gt;

&lt;p&gt;最后的最后，python继承顺序遵循C3算法，只要在一个地方找到了所需的内容，就不再继续查找&lt;/p&gt;</content><author><name>KevinGuo</name></author><summary type="html">最近在学python，学到class 多重继承，降到了c3算法，这里记录一下 一、什么是拓扑排序 在图论中，拓扑排序(Topological Sorting) 是一个 有向无环图(DAG,Directed Acyclic Graph) 的所有顶点的线性序列。且该序列必须满足下面两个条件： 每个顶点出现且只出现一次。 若存在一条从顶点A到顶点B的路径，那么在序列中顶点A出现在顶点B的前面。 例如，下面这个图： 它是一个DAG图，那么如何写出它的拓扑顺序呢？这里说一种比较常用的方法： 从DAG途中选择一个没有前驱(即入度为0)的顶点并输出 从图中删除该顶点和所有以它为起点的有向边。 重复1和2直到当前DAG图为空或当前途中不存在无前驱的顶点为止。后一种情况说明有向图中必然存在环。 于是，得到拓扑排序后的结果是{1,2,4,3,5} 下面，我们看看拓扑排序在python多重继承中的例子 二、python 多重继承 #!/usr/bin/env python3 # -*- coding: utf-8 -*- class A(object): def foo(self): print('A foo') def bar(self): print('A bar') class B(object): def foo(self): print('B foo') def bar(self): print('B bar') class C1(A,B): pass class C2(A,B): def bar(self): print('C2-bar') class D(C1,C2): pass if __name__ == '__main__': print(D.__mro__) d=D() d.foo() d.bar() 首先，我们根据上面的继承关系构成一张图，如下 找到入度为0的点，只有一个D，把D拿出来，把D相关的边剪掉 现在有两个入度为0的点(C1,C2)，取最左原则，拿C1，剪掉C1相关的边，这时候的排序是{D,C1} 现在我们看，入度为0的点(C2),拿C2,剪掉C2相关的边，这时候排序是{D,C1,C2} 接着看，入度为0的点(A,B),取最左原则，拿A，剪掉A相关的边，这时候的排序是{D,C1,C2,A} 继续，入度哦为0的点只有B，拿B，剪掉B相关的边，最后只剩下object 所以最后的排序是{D,C1,C2,A,B,object} 我们执行上面的代码，发现print(D.__mro__)的结果也正是这样，而这也就是多重继承所使用的C3算法啦 为了进一步熟悉这个拓扑排序的方法，我们再来一张图，试试看排序结果是怎样的，它继承的内容是否如你所想 #!/usr/bin/env python3 # -*- coding: utf-8 -*- class A(object): def foo(self): print('A foo') def bar(self): print('A bar') class B(object): def foo(self): print('B foo') def bar(self): print('B bar') class C1(A): pass class C2(B): def bar(self): print('C2-bar') class D(C1,C2): pass if __name__ == '__main__': print(D.__mro__) d=D() d.foo() d.bar() 还是先根据继承关系构一个继承图 找到入度为0的顶点，只有一个D，拿D，剪掉D相关的边 得到两个入度为0的顶点(C1,C2),根据最左原则，拿C1，剪掉C1相关的边，这时候序列为{D,C1} 接着看，入度为0的顶点有两个(A,C1),根据最左原则，拿A，剪掉A相关的边，这时候序列为{D,C1,A} 接着看，入度为0的顶点为C2,拿C2，剪掉C2相关的边，这时候序列为{D,C1,A,C2} 继续，入度为0的顶点为B，拿B，剪掉B相关的边，最后还有一个object 所以最后的序列为{D,C1,A,C2,B,object} 最后，我们执行上面的代码，发现print(D.__mro__)的结果正如上面所计算的结果 最后的最后，python继承顺序遵循C3算法，只要在一个地方找到了所需的内容，就不再继续查找</summary></entry><entry><title type="html">jenkins with pipeline on kubernetes</title><link href="https://kevinguo.me/2017/12/27/jenkins-on-kubernetes-with-pipeline/" rel="alternate" type="text/html" title="jenkins with pipeline on kubernetes" /><published>2017-12-27T00:00:00+08:00</published><updated>2017-12-27T00:00:00+08:00</updated><id>https://kevinguo.me/2017/12/27/jenkins-on-kubernetes-with-pipeline</id><content type="html" xml:base="https://kevinguo.me/2017/12/27/jenkins-on-kubernetes-with-pipeline/">&lt;blockquote&gt;
  &lt;p&gt;jenkins CI/CD用了有很长一段时间了，包括现公司的docker container deployment也是通过写pipeline workflow来实现的，但是当我在将jenkins迁往kubernetes的过程中，还是踩了不少的坑，这里记录下来。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;该流程包含了 &lt;code class=&quot;highlighter-rouge&quot;&gt;checkout scm&lt;/code&gt; –&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;build artifacts&lt;/code&gt; –&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;build image&lt;/code&gt; –&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;deploy to k8s&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;流程相对简单，而且并没有涉及到代码分支，集中测试，蓝绿部署等等&lt;/p&gt;

&lt;h3 id=&quot;一集群以及必要组件的搭建&quot;&gt;一、集群以及必要组件的搭建&lt;/h3&gt;

&lt;p&gt;请参考&lt;a href=&quot;https://kevinguo.me/categories/#kubernetes&quot;&gt;手动搭建kubernetes HA集群&lt;/a&gt;,&lt;a href=&quot;https://kevinguo.me/categories/#ceph&quot;&gt;kubernetes ceph笔记&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;二jenkins各个yaml文件&quot;&gt;二、jenkins各个yaml文件&lt;/h3&gt;

&lt;p&gt;所有文件都放在&lt;a href=&quot;https://github.com/chinakevinguo/kubernetes-jenkins.git&quot;&gt;这里&lt;/a&gt;，我们搭建的时候只需将对应的位置修改成自己的即可&lt;/p&gt;

&lt;h3 id=&quot;三配置jenkins&quot;&gt;三、配置jenkins&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;jenkins 部署成功之后，我们需要安装对应的插件，配置和kubernetes的关联，这里除了必要的插件之外，我们额外需要安装一个kubernetes Plugin&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;kubernetes cloud的配置相对简单，我们只需要指定&lt;code class=&quot;highlighter-rouge&quot;&gt;Kubernetes URL&lt;/code&gt;以及&lt;code class=&quot;highlighter-rouge&quot;&gt;Jenkins URL&lt;/code&gt;即可，因为jenkins在kubernetes中，所以&lt;code class=&quot;highlighter-rouge&quot;&gt;Kubernetes URL&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;Jenkins URL&lt;/code&gt;均为内部service就行了，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/kubernetes-cloud.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;四新建pipeline-job-测试&quot;&gt;四、新建pipeline job 测试&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;jenkins kubernetes cloud配置成功之后，我们就需要来新建一个pipeline job测试一下，这里我新建了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;learn-groovy&lt;/code&gt;的job&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;新建job更加的简单，只需要指定你的&lt;code class=&quot;highlighter-rouge&quot;&gt;Jenkinsfile&lt;/code&gt;的地址即可，如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/jenkins-pipeline.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;所有的工作都在&lt;code class=&quot;highlighter-rouge&quot;&gt;Jenkinsfile&lt;/code&gt;中定义完成，这就是pipeline了&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于当前这个example项目的对应配置文件有如下几个&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;app-deploy.yaml 当前项目部署所需的yaml文件&lt;/li&gt;
  &lt;li&gt;Jenkinsfile 当前项目部署流程所需文件&lt;/li&gt;
  &lt;li&gt;Jenkinsfile.yaml 当前项目构建部署过程中可变参数的变量文件&lt;/li&gt;
  &lt;li&gt;Dockerfile 构建image所需文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上所有文件在&lt;a href=&quot;https://github.com/chinakevinguo/learn-groovy.git&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;我们的job构建成功后，最终的结果如下&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/jenkins-kubernetes-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/jenkins-kubernetes-result-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name>KevinGuo</name></author><summary type="html">jenkins CI/CD用了有很长一段时间了，包括现公司的docker container deployment也是通过写pipeline workflow来实现的，但是当我在将jenkins迁往kubernetes的过程中，还是踩了不少的坑，这里记录下来。 该流程包含了 checkout scm –&amp;gt; build artifacts –&amp;gt; build image –&amp;gt; deploy to k8s 流程相对简单，而且并没有涉及到代码分支，集中测试，蓝绿部署等等 一、集群以及必要组件的搭建 请参考手动搭建kubernetes HA集群,kubernetes ceph笔记 二、jenkins各个yaml文件 所有文件都放在这里，我们搭建的时候只需将对应的位置修改成自己的即可 三、配置jenkins jenkins 部署成功之后，我们需要安装对应的插件，配置和kubernetes的关联，这里除了必要的插件之外，我们额外需要安装一个kubernetes Plugin kubernetes cloud的配置相对简单，我们只需要指定Kubernetes URL以及Jenkins URL即可，因为jenkins在kubernetes中，所以Kubernetes URL和Jenkins URL均为内部service就行了，如下图 四、新建pipeline job 测试 jenkins kubernetes cloud配置成功之后，我们就需要来新建一个pipeline job测试一下，这里我新建了一个learn-groovy的job 新建job更加的简单，只需要指定你的Jenkinsfile的地址即可，如下图 所有的工作都在Jenkinsfile中定义完成，这就是pipeline了 关于当前这个example项目的对应配置文件有如下几个 app-deploy.yaml 当前项目部署所需的yaml文件 Jenkinsfile 当前项目部署流程所需文件 Jenkinsfile.yaml 当前项目构建部署过程中可变参数的变量文件 Dockerfile 构建image所需文件 以上所有文件在这里 我们的job构建成功后，最终的结果如下</summary></entry><entry><title type="html">kubernetes RBAC 概念</title><link href="https://kevinguo.me/2017/12/01/kubernetes-rbac-concept/" rel="alternate" type="text/html" title="kubernetes RBAC 概念" /><published>2017-12-01T00:00:00+08:00</published><updated>2017-12-01T00:00:00+08:00</updated><id>https://kevinguo.me/2017/12/01/kubernetes-rbac-concept</id><content type="html" xml:base="https://kevinguo.me/2017/12/01/kubernetes-rbac-concept/">&lt;blockquote&gt;
  &lt;p&gt;注：全文转载于https://jimmysong.io/kubernetes-handbook/guide/rbac.html
主要是为了避免以后想查看概念的时候找不到位置，望作者见谅
以下所有内容是 &lt;a href=&quot;https://github.com/xingzhou&quot;&gt;xingzhou&lt;/a&gt; 对 kubernetes 官方文档的翻译，原文地址 https://k8smeetup.github.io/docs/admin/authorization/rbac/&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;rbac基于角色的访问控制&quot;&gt;RBAC——基于角色的访问控制&lt;/h1&gt;

&lt;p&gt;基于角色的访问控制（Role-Based Access Control, 即”RBAC”）使用”rbac.authorization.k8s.io” API Group实现授权决策，允许管理员通过Kubernetes API动态配置策略。&lt;/p&gt;

&lt;p&gt;截至Kubernetes 1.6，RBAC模式处于beta版本。&lt;/p&gt;

&lt;p&gt;要启用RBAC，请使用&lt;code class=&quot;highlighter-rouge&quot;&gt;--authorization-mode=RBAC&lt;/code&gt;启动API Server。&lt;/p&gt;

&lt;h2 id=&quot;api概述&quot;&gt;API概述&lt;/h2&gt;

&lt;p&gt;本节将介绍RBAC API所定义的四种顶级类型。用户可以像使用其他Kubernetes API资源一样 （例如通过&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl&lt;/code&gt;、API调用等）与这些资源进行交互。例如，命令&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create -f (resource).yml&lt;/code&gt; 可以被用于以下所有的例子，当然，读者在尝试前可能需要先阅读以下相关章节的内容。&lt;/p&gt;

&lt;h3 id=&quot;role与clusterrole&quot;&gt;Role与ClusterRole&lt;/h3&gt;

&lt;p&gt;在RBAC API中，一个角色包含了一套表示一组权限的规则。 权限以纯粹的累加形式累积（没有”否定”的规则）。 角色可以由命名空间（namespace）内的&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;对象定义，而整个Kubernetes集群范围内有效的角色则通过&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;对象实现。&lt;/p&gt;

&lt;p&gt;一个&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;对象只能用于授予对某一单一命名空间中资源的访问权限。 以下示例描述了”default”命名空间中的一个&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;对象的定义，用于授予对pod的读访问权限：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Role&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pod-reader&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 空字符串&quot;&quot;表明使用core API group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;pods&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;watch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;对象可以授予与&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;对象相同的权限，但由于它们属于集群范围对象， 也可以使用它们授予对以下几种资源的访问权限：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;集群范围资源（例如节点，即node）&lt;/li&gt;
  &lt;li&gt;非资源类型endpoint（例如”/healthz”）&lt;/li&gt;
  &lt;li&gt;跨所有命名空间的命名空间范围资源（例如pod，需要运行命令&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl get pods --all-namespaces&lt;/code&gt;来查询集群中所有的pod）&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面示例中的&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;定义可用于授予用户对某一特定命名空间，或者所有命名空间中的secret（取决于其&lt;a href=&quot;https://k8smeetup.github.io/docs/admin/authorization/rbac/#rolebinding-and-clusterrolebinding&quot;&gt;绑定&lt;/a&gt;方式）的读访问权限：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Yaml&quot;&gt;kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  # 鉴于ClusterRole是集群范围对象，所以这里不需要定义&quot;namespace&quot;字段
  name: secret-reader
rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;secrets&quot;]
  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;rolebinding与clusterrolebinding&quot;&gt;RoleBinding与ClusterRoleBinding&lt;/h3&gt;

&lt;p&gt;角色绑定将一个角色中定义的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即subject, 包括用户——User、用户组——Group、或者服务账户——Service Account）以及对被授予角色的引用。 在命名空间中可以通过&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;对象授予权限，而集群范围的权限授予则通过&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt;对象完成。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;可以引用在同一命名空间内定义的&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;对象。 下面示例中定义的&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;对象在”default”命名空间中将”pod-reader”角色授予用户”jane”。 这一授权将允许用户”jane”从”default”命名空间中读取pod。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Yaml&quot;&gt;# 以下角色绑定定义将允许用户&quot;jane&quot;从&quot;default&quot;命名空间中读取pod。
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: jane
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;对象也可以引用一个&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;对象用于在&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;所在的命名空间内授予用户对所引用的&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;中 定义的命名空间资源的访问权限。这一点允许管理员在整个集群范围内首先定义一组通用的角色，然后再在不同的命名空间中复用这些角色。&lt;/p&gt;

&lt;p&gt;例如，尽管下面示例中的&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;引用的是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;对象，但是用户”dave”（即角色绑定主体）还是只能读取”development” 命名空间中的secret（即&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;所在的命名空间）。&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 以下角色绑定允许用户&quot;dave&quot;读取&quot;development&quot;命名空间中的secret。&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RoleBinding&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;read-secrets&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;development&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 这里表明仅授权读取&quot;development&quot;命名空间中的资源。&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;User&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;dave&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;roleRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRole&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;secret-reader&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;最后，可以使用&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt;在集群级别和所有命名空间中授予权限。下面示例中所定义的&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt; 允许在用户组”manager”中的任何用户都可以读取集群中任何命名空间中的secret。&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 以下`ClusterRoleBinding`对象允许在用户组&quot;manager&quot;中的任何用户都可以读取集群中任何命名空间中的secret。&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRoleBinding&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;read-secrets-global&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;manager&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;roleRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRole&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;secret-reader&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;对资源的引用&quot;&gt;对资源的引用&lt;/h3&gt;

&lt;p&gt;大多数资源由代表其名字的字符串表示，例如”pods”，就像它们出现在相关API endpoint的URL中一样。然而，有一些Kubernetes API还 包含了”子资源”，比如pod的logs。在Kubernetes中，pod logs endpoint的URL格式为：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;GET /api/v1/namespaces/{namespace}/pods/{name}/log

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在这种情况下，”pods”是命名空间资源，而”log”是pods的子资源。为了在RBAC角色中表示出这一点，我们需要使用斜线来划分资源 与子资源。如果需要角色绑定主体读取pods以及pod log，您需要定义以下角色：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Role&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;pod-and-pod-logs-reader&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;pods&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;pods/log&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;通过&lt;code class=&quot;highlighter-rouge&quot;&gt;resourceNames&lt;/code&gt;列表，角色可以针对不同种类的请求根据资源名引用资源实例。当指定了&lt;code class=&quot;highlighter-rouge&quot;&gt;resourceNames&lt;/code&gt;列表时，不同动作 种类的请求的权限，如使用”get”、”delete”、”update”以及”patch”等动词的请求，将被限定到资源列表中所包含的资源实例上。 例如，如果需要限定一个角色绑定主体只能”get”或者”update”一个configmap时，您可以定义以下角色：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Role&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;configmap-updater&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;configmap&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resourceNames&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;my-configmap&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;update&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;值得注意的是，如果设置了&lt;code class=&quot;highlighter-rouge&quot;&gt;resourceNames&lt;/code&gt;，则请求所使用的动词不能是list、watch、create或者deletecollection。 由于资源名不会出现在create、list、watch和deletecollection等API请求的URL中，所以这些请求动词不会被设置了&lt;code class=&quot;highlighter-rouge&quot;&gt;resourceNames&lt;/code&gt; 的规则所允许，因为规则中的&lt;code class=&quot;highlighter-rouge&quot;&gt;resourceNames&lt;/code&gt;部分不会匹配这些请求。&lt;/p&gt;

&lt;h4 id=&quot;一些角色定义的例子&quot;&gt;一些角色定义的例子&lt;/h4&gt;

&lt;p&gt;在以下示例中，我们仅截取展示了&lt;code class=&quot;highlighter-rouge&quot;&gt;rules&lt;/code&gt;部分的定义。&lt;/p&gt;

&lt;p&gt;允许读取core API Group中定义的资源”pods”：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Yaml&quot;&gt;rules:
- apiGroups: [&quot;&quot;]
  resources: [&quot;pods&quot;]
  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;允许读写在”extensions”和”apps” API Group中定义的”deployments”：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;extensions&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;apps&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;deployments&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;watch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;create&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;update&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;patch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;delete&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;允许读取”pods”以及读写”jobs”：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;pods&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;watch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;batch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;extensions&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;jobs&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;watch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;create&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;update&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;patch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;delete&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;允许读取一个名为”my-config”的&lt;code class=&quot;highlighter-rouge&quot;&gt;ConfigMap&lt;/code&gt;实例（需要将其通过&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;绑定从而限制针对某一个命名空间中定义的一个&lt;code class=&quot;highlighter-rouge&quot;&gt;ConfigMap&lt;/code&gt;实例的访问）：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;configmaps&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resourceNames&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;my-config&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;允许读取core API Group中的”nodes”资源（由于&lt;code class=&quot;highlighter-rouge&quot;&gt;Node&lt;/code&gt;是集群级别资源，所以此&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;定义需要与一个&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt;绑定才能有效）：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;nodes&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;watch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;允许对非资源endpoint “/healthz”及其所有子路径的”GET”和”POST”请求（此&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;定义需要与一个&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt;绑定才能有效）：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nonResourceURLs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/healthz&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/healthz/*&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 在非资源URL中，'*'代表后缀通配符&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;post&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;对角色绑定主体subject的引用&quot;&gt;对角色绑定主体（Subject）的引用&lt;/h3&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt;将角色绑定到&lt;em&gt;角色绑定主体&lt;/em&gt;（Subject）。 角色绑定主体可以是用户组（Group）、用户（User）或者服务账户（Service Accounts）。&lt;/p&gt;

&lt;p&gt;用户由字符串表示。可以是纯粹的用户名，例如”alice”、电子邮件风格的名字，如 “bob@example.com” 或者是用字符串表示的数字id。由Kubernetes管理员配置&lt;a href=&quot;https://k8smeetup.github.io/docs/admin/authentication/&quot;&gt;认证模块&lt;/a&gt; 以产生所需格式的用户名。对于用户名，RBAC授权系统不要求任何特定的格式。然而，前缀&lt;code class=&quot;highlighter-rouge&quot;&gt;system:&lt;/code&gt;是 为Kubernetes系统使用而保留的，所以管理员应该确保用户名不会意外地包含这个前缀。&lt;/p&gt;

&lt;p&gt;Kubernetes中的用户组信息由授权模块提供。用户组与用户一样由字符串表示。Kubernetes对用户组 字符串没有格式要求，但前缀&lt;code class=&quot;highlighter-rouge&quot;&gt;system:&lt;/code&gt;同样是被系统保留的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://k8smeetup.github.io/docs/tasks/configure-pod-container/configure-service-account/&quot;&gt;服务账户&lt;/a&gt;拥有包含 &lt;code class=&quot;highlighter-rouge&quot;&gt;system:serviceaccount:&lt;/code&gt;前缀的用户名，并属于拥有&lt;code class=&quot;highlighter-rouge&quot;&gt;system:serviceaccounts:&lt;/code&gt;前缀的用户组。&lt;/p&gt;

&lt;h4 id=&quot;角色绑定的一些例子&quot;&gt;角色绑定的一些例子&lt;/h4&gt;

&lt;p&gt;以下示例中，仅截取展示了&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;的&lt;code class=&quot;highlighter-rouge&quot;&gt;subjects&lt;/code&gt;字段。&lt;/p&gt;

&lt;p&gt;一个名为”alice@example.com”的用户：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;User&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;alice@example.com&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一个名为”frontend-admins”的用户组：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;frontend-admins&quot;&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;kube-system命名空间中的默认服务账户：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ServiceAccount&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;名为”qa”命名空间中的所有服务账户：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:serviceaccounts:qa&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在集群中的所有服务账户：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:serviceaccounts&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;所有认证过的用户（version 1.5+）：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:authenticated&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;所有未认证的用户（version 1.5+）：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:unauthenticated&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;所有用户（version 1.5+）：&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:authenticated&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Group&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;system:unauthenticated&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;默认角色与默认角色绑定&quot;&gt;默认角色与默认角色绑定&lt;/h2&gt;

&lt;p&gt;API Server会创建一组默认的&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt;对象。 这些默认对象中有许多包含&lt;code class=&quot;highlighter-rouge&quot;&gt;system:&lt;/code&gt;前缀，表明这些资源由Kubernetes基础组件”拥有”。 对这些资源的修改可能导致非功能性集群（non-functional cluster）。一个例子是&lt;code class=&quot;highlighter-rouge&quot;&gt;system:node&lt;/code&gt; ClusterRole对象。 这个角色定义了kubelets的权限。如果这个角色被修改，可能会导致kubelets无法正常工作。&lt;/p&gt;

&lt;p&gt;所有默认的ClusterRole和ClusterRoleBinding对象都会被标记为&lt;code class=&quot;highlighter-rouge&quot;&gt;kubernetes.io/bootstrapping=rbac-defaults&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;自动更新&quot;&gt;自动更新&lt;/h3&gt;

&lt;p&gt;每次启动时，API Server都会更新默认ClusterRole所缺乏的各种权限，并更新默认ClusterRoleBinding所缺乏的各个角色绑定主体。 这种自动更新机制允许集群修复一些意外的修改。由于权限和角色绑定主体在新的Kubernetes释出版本中可能变化，这也能够保证角色和角色 绑定始终保持是最新的。&lt;/p&gt;

&lt;p&gt;如果需要禁用自动更新，请将默认ClusterRole以及ClusterRoleBinding的&lt;code class=&quot;highlighter-rouge&quot;&gt;rbac.authorization.kubernetes.io/autoupdate&lt;/code&gt; 设置成为&lt;code class=&quot;highlighter-rouge&quot;&gt;false&lt;/code&gt;。 请注意，缺乏默认权限和角色绑定主体可能会导致非功能性集群问题。&lt;/p&gt;

&lt;p&gt;自Kubernetes 1.6+起，当集群RBAC授权器（RBAC Authorizer）处于开启状态时，可以启用自动更新功能.&lt;/p&gt;

&lt;h3 id=&quot;发现类角色&quot;&gt;发现类角色&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;默认ClusterRole&lt;/th&gt;
      &lt;th&gt;默认ClusterRoleBinding&lt;/th&gt;
      &lt;th&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:basic-user&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;system:authenticated&lt;/strong&gt; and &lt;strong&gt;system:unauthenticated&lt;/strong&gt;groups&lt;/td&gt;
      &lt;td&gt;允许用户只读访问有关自己的基本信息。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:discovery&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;system:authenticated&lt;/strong&gt; and &lt;strong&gt;system:unauthenticated&lt;/strong&gt;groups&lt;/td&gt;
      &lt;td&gt;允许只读访问API discovery endpoints, 用于在API级别进行发现和协商。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;面向用户的角色&quot;&gt;面向用户的角色&lt;/h3&gt;

&lt;p&gt;一些默认角色并不包含&lt;code class=&quot;highlighter-rouge&quot;&gt;system:&lt;/code&gt;前缀，它们是面向用户的角色。 这些角色包含超级用户角色（&lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt;），即旨在利用ClusterRoleBinding（&lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-status&lt;/code&gt;）在集群范围内授权的角色， 以及那些使用RoleBinding（&lt;code class=&quot;highlighter-rouge&quot;&gt;admin&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;edit&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;view&lt;/code&gt;）在特定命名空间中授权的角色。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;默认ClusterRole&lt;/th&gt;
      &lt;th&gt;默认ClusterRoleBinding&lt;/th&gt;
      &lt;th&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;cluster-admin&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;system:masters&lt;/strong&gt; group&lt;/td&gt;
      &lt;td&gt;超级用户权限，允许对任何资源执行任何操作。 在&lt;strong&gt;ClusterRoleBinding&lt;/strong&gt;中使用时，可以完全控制集群和所有命名空间中的所有资源。 在&lt;strong&gt;RoleBinding&lt;/strong&gt;中使用时，可以完全控制RoleBinding所在命名空间中的所有资源，包括命名空间自己。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;admin&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;管理员权限，利用&lt;strong&gt;RoleBinding&lt;/strong&gt;在某一命名空间内部授予。 在&lt;strong&gt;RoleBinding&lt;/strong&gt;中使用时，允许针对命名空间内大部分资源的读写访问， 包括在命名空间内创建角色与角色绑定的能力。 但不允许对资源配额（resource quota）或者命名空间本身的写访问。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;edit&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;允许对某一个命名空间内大部分对象的读写访问，但不允许查看或者修改角色或者角色绑定。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;view&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;允许对某一个命名空间内大部分对象的只读访问。 不允许查看角色或者角色绑定。 由于可扩散性等原因，不允许查看secret资源。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;core-component-roles&quot;&gt;Core Component Roles&lt;/h3&gt;

&lt;h3 id=&quot;核心组件角色&quot;&gt;核心组件角色&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;默认ClusterRole&lt;/th&gt;
      &lt;th&gt;默认ClusterRoleBinding&lt;/th&gt;
      &lt;th&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:kube-scheduler&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;system:kube-scheduler&lt;/strong&gt; user&lt;/td&gt;
      &lt;td&gt;允许访问kube-scheduler组件所需要的资源。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:kube-controller-manager&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;system:kube-controller-manager&lt;/strong&gt; user&lt;/td&gt;
      &lt;td&gt;允许访问kube-controller-manager组件所需要的资源。 单个控制循环所需要的权限请参阅&lt;a href=&quot;https://k8smeetup.github.io/docs/admin/authorization/rbac/#controller-roles&quot;&gt;控制器（controller）角色&lt;/a&gt;.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:node&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;system:nodes&lt;/strong&gt; group (deprecated in 1.7)&lt;/td&gt;
      &lt;td&gt;允许对kubelet组件所需要的资源的访问，&lt;strong&gt;包括读取所有secret和对所有pod的写访问&lt;/strong&gt;。 自Kubernetes 1.7开始, 相比较于这个角色，更推荐使用&lt;a href=&quot;https://kubernetes.io/docs/admin/authorization/node/&quot;&gt;Node authorizer&lt;/a&gt; 以及&lt;a href=&quot;https://kubernetes.io/docs/admin/admission-controllers#NodeRestriction&quot;&gt;NodeRestriction admission plugin&lt;/a&gt;， 并允许根据调度运行在节点上的pod授予kubelets API访问的权限。 自Kubernetes 1.7开始，当启用&lt;code class=&quot;highlighter-rouge&quot;&gt;Node&lt;/code&gt;授权模式时，对&lt;code class=&quot;highlighter-rouge&quot;&gt;system:nodes&lt;/code&gt;用户组的绑定将不会被自动创建。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:node-proxier&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;system:kube-proxy&lt;/strong&gt; user&lt;/td&gt;
      &lt;td&gt;允许对kube-proxy组件所需要资源的访问。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;其它组件角色&quot;&gt;其它组件角色&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;默认ClusterRole&lt;/th&gt;
      &lt;th&gt;默认ClusterRoleBinding&lt;/th&gt;
      &lt;th&gt;描述&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:auth-delegator&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;允许委托认证和授权检查。 通常由附加API Server用于统一认证和授权。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:heapster&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/kubernetes/heapster&quot;&gt;Heapster&lt;/a&gt;组件的角色。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:kube-aggregator&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/kubernetes/kube-aggregator&quot;&gt;kube-aggregator&lt;/a&gt;组件的角色。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:kube-dns&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;kube-dns&lt;/strong&gt; service account in the &lt;strong&gt;kube-system&lt;/strong&gt;namespace&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://k8smeetup.github.io/docs/admin/dns/&quot;&gt;kube-dns&lt;/a&gt;组件的角色。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:node-bootstrapper&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;允许对执行&lt;a href=&quot;https://k8smeetup.github.io/docs/admin/kubelet-tls-bootstrapping/&quot;&gt;Kubelet TLS引导（Kubelet TLS bootstrapping）&lt;/a&gt;所需要资源的访问.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:node-problem-detector&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;&lt;a href=&quot;https://github.com/kubernetes/node-problem-detector&quot;&gt;node-problem-detector&lt;/a&gt;组件的角色。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;system:persistent-volume-provisioner&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;None&lt;/td&gt;
      &lt;td&gt;允许对大部分&lt;a href=&quot;https://k8smeetup.github.io/docs/user-guide/persistent-volumes/#provisioner&quot;&gt;动态存储卷创建组件（dynamic volume provisioner）&lt;/a&gt;所需要资源的访问。&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;控制器controller角色&quot;&gt;控制器（Controller）角色&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://k8smeetup.github.io/docs/admin/kube-controller-manager/&quot;&gt;Kubernetes controller manager&lt;/a&gt;负责运行核心控制循环。 当使用&lt;code class=&quot;highlighter-rouge&quot;&gt;--use-service-account-credentials&lt;/code&gt;选项运行controller manager时，每个控制循环都将使用单独的服务账户启动。 而每个控制循环都存在对应的角色，前缀名为&lt;code class=&quot;highlighter-rouge&quot;&gt;system:controller:&lt;/code&gt;。 如果不使用&lt;code class=&quot;highlighter-rouge&quot;&gt;--use-service-account-credentials&lt;/code&gt;选项时，controller manager将会使用自己的凭证运行所有控制循环，而这些凭证必须被授予相关的角色。 这些角色包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;system:controller:attachdetach-controller&lt;/li&gt;
  &lt;li&gt;system:controller:certificate-controller&lt;/li&gt;
  &lt;li&gt;system:controller:cronjob-controller&lt;/li&gt;
  &lt;li&gt;system:controller:daemon-set-controller&lt;/li&gt;
  &lt;li&gt;system:controller:deployment-controller&lt;/li&gt;
  &lt;li&gt;system:controller:disruption-controller&lt;/li&gt;
  &lt;li&gt;system:controller:endpoint-controller&lt;/li&gt;
  &lt;li&gt;system:controller:generic-garbage-collector&lt;/li&gt;
  &lt;li&gt;system:controller:horizontal-pod-autoscaler&lt;/li&gt;
  &lt;li&gt;system:controller:job-controller&lt;/li&gt;
  &lt;li&gt;system:controller:namespace-controller&lt;/li&gt;
  &lt;li&gt;system:controller:node-controller&lt;/li&gt;
  &lt;li&gt;system:controller:persistent-volume-binder&lt;/li&gt;
  &lt;li&gt;system:controller:pod-garbage-collector&lt;/li&gt;
  &lt;li&gt;system:controller:replicaset-controller&lt;/li&gt;
  &lt;li&gt;system:controller:replication-controller&lt;/li&gt;
  &lt;li&gt;system:controller:resourcequota-controller&lt;/li&gt;
  &lt;li&gt;system:controller:route-controller&lt;/li&gt;
  &lt;li&gt;system:controller:service-account-controller&lt;/li&gt;
  &lt;li&gt;system:controller:service-controller&lt;/li&gt;
  &lt;li&gt;system:controller:statefulset-controller&lt;/li&gt;
  &lt;li&gt;system:controller:ttl-controller&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;初始化与预防权限升级&quot;&gt;初始化与预防权限升级&lt;/h2&gt;

&lt;p&gt;RBAC API会阻止用户通过编辑角色或者角色绑定来升级权限。 由于这一点是在API级别实现的，所以在RBAC授权器（RBAC authorizer）未启用的状态下依然可以正常工作。&lt;/p&gt;

&lt;p&gt;用户只有在拥有了角色所包含的所有权限的条件下才能创建／更新一个角色，这些操作还必须在角色所处的相同范围内进行（对于&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;来说是集群范围，对于&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;来说是在与角色相同的命名空间或者集群范围）。 例如，如果用户”user-1”没有权限读取集群范围内的secret列表，那么他也不能创建包含这种权限的&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;。为了能够让用户创建／更新角色，需要：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;授予用户一个角色以允许他们根据需要创建／更新&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;对象。&lt;/li&gt;
  &lt;li&gt;授予用户一个角色包含他们在&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;中所能够设置的所有权限。如果用户尝试创建或者修改&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;以设置那些他们未被授权的权限时，这些API请求将被禁止。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;用户只有在拥有所引用的角色中包含的所有权限时才可以创建／更新角色绑定（这些操作也必须在角色绑定所处的相同范围内进行）&lt;em&gt;或者&lt;/em&gt;用户被明确授权可以在所引用的角色上执行绑定操作。 例如，如果用户”user-1”没有权限读取集群范围内的secret列表，那么他将不能创建&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;来引用那些授予了此项权限的角色。为了能够让用户创建／更新角色绑定，需要：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;授予用户一个角色以允许他们根据需要创建／更新&lt;code class=&quot;highlighter-rouge&quot;&gt;RoleBinding&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRoleBinding&lt;/code&gt;对象。&lt;/li&gt;
  &lt;li&gt;授予用户绑定某一特定角色所需要的权限：
    &lt;ul&gt;
      &lt;li&gt;隐式地，通过授予用户所有所引用的角色中所包含的权限&lt;/li&gt;
      &lt;li&gt;显式地，通过授予用户在特定Role（或者ClusterRole）对象上执行&lt;code class=&quot;highlighter-rouge&quot;&gt;bind&lt;/code&gt;操作的权限&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;例如，下面例子中的ClusterRole和RoleBinding将允许用户”user-1”授予其它用户”user-1-namespace”命名空间内的&lt;code class=&quot;highlighter-rouge&quot;&gt;admin&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;edit&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;view&lt;/code&gt;等角色和角色绑定。&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRole&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role-grantor&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rolebindings&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;create&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;clusterroles&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;bind&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;resourceNames&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;admin&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;edit&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;view&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RoleBinding&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role-grantor-binding&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;user-1-namespace&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;roleRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClusterRole&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;role-grantor&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;User&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;user-1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;当初始化第一个角色和角色绑定时，初始用户需要能够授予他们尚未拥有的权限。 初始化初始角色和角色绑定时需要：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;使用包含&lt;code class=&quot;highlighter-rouge&quot;&gt;system：masters&lt;/code&gt;用户组的凭证，该用户组通过默认绑定绑定到&lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt;超级用户角色。&lt;/li&gt;
  &lt;li&gt;如果您的API Server在运行时启用了非安全端口（&lt;code class=&quot;highlighter-rouge&quot;&gt;--insecure-port&lt;/code&gt;），您也可以通过这个没有施行认证或者授权的端口发送角色或者角色绑定请求。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;一些命令行工具&quot;&gt;一些命令行工具&lt;/h2&gt;

&lt;p&gt;有两个&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl&lt;/code&gt;命令可以用于在命名空间内或者整个集群内授予角色。&lt;/p&gt;

&lt;h3 id=&quot;kubectl-create-rolebinding&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create rolebinding&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;在某一特定命名空间内授予&lt;code class=&quot;highlighter-rouge&quot;&gt;Role&lt;/code&gt;或者&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;。示例如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;在名为”acme”的命名空间中将&lt;code class=&quot;highlighter-rouge&quot;&gt;admin&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;授予用户”bob”：&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在名为”acme”的命名空间中将&lt;code class=&quot;highlighter-rouge&quot;&gt;view&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;授予服务账户”myapp”：&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kubectl-create-clusterrolebinding&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create clusterrolebinding&lt;/code&gt;&lt;/h3&gt;

&lt;p&gt;在整个集群中授予&lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;，包括所有命名空间。示例如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;在整个集群范围内将&lt;code class=&quot;highlighter-rouge&quot;&gt;cluster-admin&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;授予用户”root”：&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在整个集群范围内将&lt;code class=&quot;highlighter-rouge&quot;&gt;system:node&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;授予用户”kubelet”：&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create clusterrolebinding kubelet-node-binding --clusterrole=system:node --user=kubelet&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在整个集群范围内将&lt;code class=&quot;highlighter-rouge&quot;&gt;view&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;ClusterRole&lt;/code&gt;授予命名空间”acme”内的服务账户”myapp”：&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;请参阅CLI帮助文档以获得上述命令的详细用法&lt;/p&gt;

&lt;h2 id=&quot;服务账户service-account权限&quot;&gt;服务账户（Service Account）权限&lt;/h2&gt;

&lt;p&gt;默认的RBAC策略将授予控制平面组件（control-plane component）、节点（node）和控制器（controller）一组范围受限的权限， 但对于”kube-system”命名空间以外的服务账户，则&lt;em&gt;不授予任何权限&lt;/em&gt;（超出授予所有认证用户的发现权限）。&lt;/p&gt;

&lt;p&gt;这一点允许您根据需要向特定服务账号授予特定权限。 细粒度的角色绑定将提供更好的安全性，但需要更多精力管理。 更粗粒度的授权可能授予服务账号不需要的API访问权限（甚至导致潜在授权扩散），但更易于管理。&lt;/p&gt;

&lt;p&gt;从最安全到最不安全可以排序以下方法：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;对某一特定应用程序的服务账户授予角色（最佳实践）&lt;/p&gt;

    &lt;p&gt;要求应用程序在其pod规范（pod spec）中指定&lt;code class=&quot;highlighter-rouge&quot;&gt;serviceAccountName&lt;/code&gt;字段，并且要创建相应服务账户（例如通过API、应用程序清单或者命令&lt;code class=&quot;highlighter-rouge&quot;&gt;kubectl create serviceaccount&lt;/code&gt;等）。&lt;/p&gt;

    &lt;p&gt;例如，在”my-namespace”命名空间中授予服务账户”my-sa”只读权限：&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create rolebinding my-sa-view &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --clusterrole&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;view &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --serviceaccount&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;my-namespace:my-sa &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --namespace&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;my-namespace
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在某一命名空间中授予”default”服务账号一个角色&lt;/p&gt;

    &lt;p&gt;如果一个应用程序没有在其pod规范中指定&lt;code class=&quot;highlighter-rouge&quot;&gt;serviceAccountName&lt;/code&gt;，它将默认使用”default”服务账号。&lt;/p&gt;

    &lt;p&gt;注意：授予”default”服务账号的权限将可用于命名空间内任何没有指定&lt;code class=&quot;highlighter-rouge&quot;&gt;serviceAccountName&lt;/code&gt;的pod。&lt;/p&gt;

    &lt;p&gt;下面的例子将在”my-namespace”命名空间内授予”default”服务账号只读权限：&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create rolebinding default-view &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --clusterrole&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;view &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --serviceaccount&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;my-namespace:default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --namespace&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;my-namespace
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;

    &lt;p&gt;目前，许多[加载项（addon）]（/ docs / concepts / cluster-administration / addons /）作为”kube-system”命名空间中的”default”服务帐户运行。 要允许这些加载项使用超级用户访问权限，请将cluster-admin权限授予”kube-system”命名空间中的”default”服务帐户。 注意：启用上述操作意味着”kube-system”命名空间将包含允许超级用户访问API的秘钥。&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create clusterrolebinding add-on-cluster-admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --clusterrole&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster-admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --serviceaccount&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-system:default
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;为命名空间中所有的服务账号授予角色&lt;/p&gt;

    &lt;p&gt;如果您希望命名空间内的所有应用程序都拥有同一个角色，无论它们使用什么服务账户，您可以为该命名空间的服务账户用户组授予角色。&lt;/p&gt;

    &lt;p&gt;下面的例子将授予”my-namespace”命名空间中的所有服务账户只读权限：&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create rolebinding serviceaccounts-view &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --clusterrole&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;view &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --group&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;system:serviceaccounts:my-namespace &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --namespace&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;my-namespace
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对集群范围内的所有服务账户授予一个受限角色（不鼓励）&lt;/p&gt;

    &lt;p&gt;如果您不想管理每个命名空间的权限，则可以将集群范围角色授予所有服务帐户。&lt;/p&gt;

    &lt;p&gt;下面的例子将所有命名空间中的只读权限授予集群中的所有服务账户：&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create clusterrolebinding serviceaccounts-view &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --clusterrole&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;view &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --group&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;system:serviceaccounts
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;授予超级用户访问权限给集群范围内的所有服务帐户（强烈不鼓励）&lt;/p&gt;

    &lt;p&gt;如果您根本不关心权限分块，您可以对所有服务账户授予超级用户访问权限。&lt;/p&gt;

    &lt;p&gt;警告：这种做法将允许任何具有读取权限的用户访问secret或者通过创建一个容器的方式来访问超级用户的凭据。&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create clusterrolebinding serviceaccounts-cluster-admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --clusterrole&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster-admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --group&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;system:serviceaccounts
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;从版本15升级&quot;&gt;从版本1.5升级&lt;/h2&gt;

&lt;p&gt;在Kubernetes 1.6之前，许多部署使用非常宽泛的ABAC策略，包括授予对所有服务帐户的完整API访问权限。&lt;/p&gt;

&lt;p&gt;默认的RBAC策略将授予控制平面组件（control-plane components）、节点（nodes）和控制器（controller）一组范围受限的权限， 但对于”kube-system”命名空间以外的服务账户，则&lt;em&gt;不授予任何权限&lt;/em&gt;（超出授予所有认证用户的发现权限）。&lt;/p&gt;

&lt;p&gt;虽然安全性更高，但这可能会影响到期望自动接收API权限的现有工作负载。 以下是管理此转换的两种方法：&lt;/p&gt;

&lt;h3 id=&quot;并行授权器authorizer&quot;&gt;并行授权器（authorizer）&lt;/h3&gt;

&lt;p&gt;同时运行RBAC和ABAC授权器，并包括旧版ABAC策略：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;--authorization-mode=RBAC,ABAC --authorization-policy-file=mypolicy.jsonl

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;RBAC授权器将尝试首先授权请求。如果RBAC授权器拒绝API请求，则ABAC授权器将被运行。这意味着RBAC策略&lt;em&gt;或者&lt;/em&gt;ABAC策略所允许的任何请求都是可通过的。&lt;/p&gt;

&lt;p&gt;当以日志级别为2或更高（&lt;code class=&quot;highlighter-rouge&quot;&gt;--v = 2&lt;/code&gt;）运行时，您可以在API Server日志中看到RBAC拒绝请求信息（以&lt;code class=&quot;highlighter-rouge&quot;&gt;RBAC DENY:&lt;/code&gt;为前缀）。 您可以使用该信息来确定哪些角色需要授予哪些用户，用户组或服务帐户。 一旦&lt;a href=&quot;https://k8smeetup.github.io/docs/admin/authorization/rbac/#service-account-permissions&quot;&gt;授予服务帐户角色&lt;/a&gt;，并且服务器日志中没有RBAC拒绝消息的工作负载正在运行，您可以删除ABAC授权器。&lt;/p&gt;

&lt;h3 id=&quot;宽泛的rbac权限&quot;&gt;宽泛的RBAC权限&lt;/h3&gt;

&lt;p&gt;您可以使用RBAC角色绑定来复制一个宽泛的策略。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;警告：以下政策略允许所有服务帐户作为集群管理员。 运行在容器中的任何应用程序都会自动接收服务帐户凭据，并且可以对API执行任何操作，包括查看secret和修改权限。 因此，并不推荐使用这种策略。&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create clusterrolebinding permissive-binding &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --clusterrole&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster-admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --user&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --user&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubelet &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --group&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;system:serviceaccounts
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name>KevinGuo</name></author><summary type="html">注：全文转载于https://jimmysong.io/kubernetes-handbook/guide/rbac.html 主要是为了避免以后想查看概念的时候找不到位置，望作者见谅 以下所有内容是 xingzhou 对 kubernetes 官方文档的翻译，原文地址 https://k8smeetup.github.io/docs/admin/authorization/rbac/ RBAC——基于角色的访问控制 基于角色的访问控制（Role-Based Access Control, 即”RBAC”）使用”rbac.authorization.k8s.io” API Group实现授权决策，允许管理员通过Kubernetes API动态配置策略。 截至Kubernetes 1.6，RBAC模式处于beta版本。 要启用RBAC，请使用--authorization-mode=RBAC启动API Server。 API概述 本节将介绍RBAC API所定义的四种顶级类型。用户可以像使用其他Kubernetes API资源一样 （例如通过kubectl、API调用等）与这些资源进行交互。例如，命令kubectl create -f (resource).yml 可以被用于以下所有的例子，当然，读者在尝试前可能需要先阅读以下相关章节的内容。 Role与ClusterRole 在RBAC API中，一个角色包含了一套表示一组权限的规则。 权限以纯粹的累加形式累积（没有”否定”的规则）。 角色可以由命名空间（namespace）内的Role对象定义，而整个Kubernetes集群范围内有效的角色则通过ClusterRole对象实现。 一个Role对象只能用于授予对某一单一命名空间中资源的访问权限。 以下示例描述了”default”命名空间中的一个Role对象的定义，用于授予对pod的读访问权限： kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: default name: pod-reader rules: - apiGroups: [&quot;&quot;] # 空字符串&quot;&quot;表明使用core API group resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] ClusterRole对象可以授予与Role对象相同的权限，但由于它们属于集群范围对象， 也可以使用它们授予对以下几种资源的访问权限： 集群范围资源（例如节点，即node） 非资源类型endpoint（例如”/healthz”） 跨所有命名空间的命名空间范围资源（例如pod，需要运行命令kubectl get pods --all-namespaces来查询集群中所有的pod） 下面示例中的ClusterRole定义可用于授予用户对某一特定命名空间，或者所有命名空间中的secret（取决于其绑定方式）的读访问权限： kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: # 鉴于ClusterRole是集群范围对象，所以这里不需要定义&quot;namespace&quot;字段 name: secret-reader rules: - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] RoleBinding与ClusterRoleBinding 角色绑定将一个角色中定义的各种权限授予一个或者一组用户。 角色绑定包含了一组相关主体（即subject, 包括用户——User、用户组——Group、或者服务账户——Service Account）以及对被授予角色的引用。 在命名空间中可以通过RoleBinding对象授予权限，而集群范围的权限授予则通过ClusterRoleBinding对象完成。 RoleBinding可以引用在同一命名空间内定义的Role对象。 下面示例中定义的RoleBinding对象在”default”命名空间中将”pod-reader”角色授予用户”jane”。 这一授权将允许用户”jane”从”default”命名空间中读取pod。 # 以下角色绑定定义将允许用户&quot;jane&quot;从&quot;default&quot;命名空间中读取pod。 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: read-pods namespace: default subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io RoleBinding对象也可以引用一个ClusterRole对象用于在RoleBinding所在的命名空间内授予用户对所引用的ClusterRole中 定义的命名空间资源的访问权限。这一点允许管理员在整个集群范围内首先定义一组通用的角色，然后再在不同的命名空间中复用这些角色。 例如，尽管下面示例中的RoleBinding引用的是一个ClusterRole对象，但是用户”dave”（即角色绑定主体）还是只能读取”development” 命名空间中的secret（即RoleBinding所在的命名空间）。 # 以下角色绑定允许用户&quot;dave&quot;读取&quot;development&quot;命名空间中的secret。 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: read-secrets namespace: development # 这里表明仅授权读取&quot;development&quot;命名空间中的资源。 subjects: - kind: User name: dave apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 最后，可以使用ClusterRoleBinding在集群级别和所有命名空间中授予权限。下面示例中所定义的ClusterRoleBinding 允许在用户组”manager”中的任何用户都可以读取集群中任何命名空间中的secret。 # 以下`ClusterRoleBinding`对象允许在用户组&quot;manager&quot;中的任何用户都可以读取集群中任何命名空间中的secret。 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: read-secrets-global subjects: - kind: Group name: manager apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 对资源的引用 大多数资源由代表其名字的字符串表示，例如”pods”，就像它们出现在相关API endpoint的URL中一样。然而，有一些Kubernetes API还 包含了”子资源”，比如pod的logs。在Kubernetes中，pod logs endpoint的URL格式为： GET /api/v1/namespaces/{namespace}/pods/{name}/log 在这种情况下，”pods”是命名空间资源，而”log”是pods的子资源。为了在RBAC角色中表示出这一点，我们需要使用斜线来划分资源 与子资源。如果需要角色绑定主体读取pods以及pod log，您需要定义以下角色： kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: default name: pod-and-pod-logs-reader rules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;, &quot;pods/log&quot;] verbs: [&quot;get&quot;, &quot;list&quot;] 通过resourceNames列表，角色可以针对不同种类的请求根据资源名引用资源实例。当指定了resourceNames列表时，不同动作 种类的请求的权限，如使用”get”、”delete”、”update”以及”patch”等动词的请求，将被限定到资源列表中所包含的资源实例上。 例如，如果需要限定一个角色绑定主体只能”get”或者”update”一个configmap时，您可以定义以下角色： kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: default name: configmap-updater rules: - apiGroups: [&quot;&quot;] resources: [&quot;configmap&quot;] resourceNames: [&quot;my-configmap&quot;] verbs: [&quot;update&quot;, &quot;get&quot;] 值得注意的是，如果设置了resourceNames，则请求所使用的动词不能是list、watch、create或者deletecollection。 由于资源名不会出现在create、list、watch和deletecollection等API请求的URL中，所以这些请求动词不会被设置了resourceNames 的规则所允许，因为规则中的resourceNames部分不会匹配这些请求。 一些角色定义的例子 在以下示例中，我们仅截取展示了rules部分的定义。 允许读取core API Group中定义的资源”pods”： rules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] 允许读写在”extensions”和”apps” API Group中定义的”deployments”： rules: - apiGroups: [&quot;extensions&quot;, &quot;apps&quot;] resources: [&quot;deployments&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;] 允许读取”pods”以及读写”jobs”： rules: - apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] - apiGroups: [&quot;batch&quot;, &quot;extensions&quot;] resources: [&quot;jobs&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;] 允许读取一个名为”my-config”的ConfigMap实例（需要将其通过RoleBinding绑定从而限制针对某一个命名空间中定义的一个ConfigMap实例的访问）： rules: - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] resourceNames: [&quot;my-config&quot;] verbs: [&quot;get&quot;] 允许读取core API Group中的”nodes”资源（由于Node是集群级别资源，所以此ClusterRole定义需要与一个ClusterRoleBinding绑定才能有效）： rules: - apiGroups: [&quot;&quot;] resources: [&quot;nodes&quot;] verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;] 允许对非资源endpoint “/healthz”及其所有子路径的”GET”和”POST”请求（此ClusterRole定义需要与一个ClusterRoleBinding绑定才能有效）： rules: - nonResourceURLs: [&quot;/healthz&quot;, &quot;/healthz/*&quot;] # 在非资源URL中，'*'代表后缀通配符 verbs: [&quot;get&quot;, &quot;post&quot;] 对角色绑定主体（Subject）的引用 RoleBinding或者ClusterRoleBinding将角色绑定到角色绑定主体（Subject）。 角色绑定主体可以是用户组（Group）、用户（User）或者服务账户（Service Accounts）。 用户由字符串表示。可以是纯粹的用户名，例如”alice”、电子邮件风格的名字，如 “bob@example.com” 或者是用字符串表示的数字id。由Kubernetes管理员配置认证模块 以产生所需格式的用户名。对于用户名，RBAC授权系统不要求任何特定的格式。然而，前缀system:是 为Kubernetes系统使用而保留的，所以管理员应该确保用户名不会意外地包含这个前缀。 Kubernetes中的用户组信息由授权模块提供。用户组与用户一样由字符串表示。Kubernetes对用户组 字符串没有格式要求，但前缀system:同样是被系统保留的。 服务账户拥有包含 system:serviceaccount:前缀的用户名，并属于拥有system:serviceaccounts:前缀的用户组。 角色绑定的一些例子 以下示例中，仅截取展示了RoleBinding的subjects字段。 一个名为”alice@example.com”的用户： subjects: - kind: User name: &quot;alice@example.com&quot; apiGroup: rbac.authorization.k8s.io 一个名为”frontend-admins”的用户组： subjects: - kind: Group name: &quot;frontend-admins&quot; apiGroup: rbac.authorization.k8s.io kube-system命名空间中的默认服务账户： subjects: - kind: ServiceAccount name: default namespace: kube-system 名为”qa”命名空间中的所有服务账户： subjects: - kind: Group name: system:serviceaccounts:qa apiGroup: rbac.authorization.k8s.io 在集群中的所有服务账户： subjects: - kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io 所有认证过的用户（version 1.5+）： subjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io 所有未认证的用户（version 1.5+）： subjects: - kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 所有用户（version 1.5+）： subjects: - kind: Group name: system:authenticated apiGroup: rbac.authorization.k8s.io - kind: Group name: system:unauthenticated apiGroup: rbac.authorization.k8s.io 默认角色与默认角色绑定 API Server会创建一组默认的ClusterRole和ClusterRoleBinding对象。 这些默认对象中有许多包含system:前缀，表明这些资源由Kubernetes基础组件”拥有”。 对这些资源的修改可能导致非功能性集群（non-functional cluster）。一个例子是system:node ClusterRole对象。 这个角色定义了kubelets的权限。如果这个角色被修改，可能会导致kubelets无法正常工作。 所有默认的ClusterRole和ClusterRoleBinding对象都会被标记为kubernetes.io/bootstrapping=rbac-defaults。 自动更新 每次启动时，API Server都会更新默认ClusterRole所缺乏的各种权限，并更新默认ClusterRoleBinding所缺乏的各个角色绑定主体。 这种自动更新机制允许集群修复一些意外的修改。由于权限和角色绑定主体在新的Kubernetes释出版本中可能变化，这也能够保证角色和角色 绑定始终保持是最新的。 如果需要禁用自动更新，请将默认ClusterRole以及ClusterRoleBinding的rbac.authorization.kubernetes.io/autoupdate 设置成为false。 请注意，缺乏默认权限和角色绑定主体可能会导致非功能性集群问题。 自Kubernetes 1.6+起，当集群RBAC授权器（RBAC Authorizer）处于开启状态时，可以启用自动更新功能. 发现类角色 默认ClusterRole 默认ClusterRoleBinding 描述 system:basic-user system:authenticated and system:unauthenticatedgroups 允许用户只读访问有关自己的基本信息。 system:discovery system:authenticated and system:unauthenticatedgroups 允许只读访问API discovery endpoints, 用于在API级别进行发现和协商。 面向用户的角色 一些默认角色并不包含system:前缀，它们是面向用户的角色。 这些角色包含超级用户角色（cluster-admin），即旨在利用ClusterRoleBinding（cluster-status）在集群范围内授权的角色， 以及那些使用RoleBinding（admin、edit和view）在特定命名空间中授权的角色。 默认ClusterRole 默认ClusterRoleBinding 描述 cluster-admin system:masters group 超级用户权限，允许对任何资源执行任何操作。 在ClusterRoleBinding中使用时，可以完全控制集群和所有命名空间中的所有资源。 在RoleBinding中使用时，可以完全控制RoleBinding所在命名空间中的所有资源，包括命名空间自己。 admin None 管理员权限，利用RoleBinding在某一命名空间内部授予。 在RoleBinding中使用时，允许针对命名空间内大部分资源的读写访问， 包括在命名空间内创建角色与角色绑定的能力。 但不允许对资源配额（resource quota）或者命名空间本身的写访问。 edit None 允许对某一个命名空间内大部分对象的读写访问，但不允许查看或者修改角色或者角色绑定。 view None 允许对某一个命名空间内大部分对象的只读访问。 不允许查看角色或者角色绑定。 由于可扩散性等原因，不允许查看secret资源。 Core Component Roles 核心组件角色 默认ClusterRole 默认ClusterRoleBinding 描述 system:kube-scheduler system:kube-scheduler user 允许访问kube-scheduler组件所需要的资源。 system:kube-controller-manager system:kube-controller-manager user 允许访问kube-controller-manager组件所需要的资源。 单个控制循环所需要的权限请参阅控制器（controller）角色. system:node system:nodes group (deprecated in 1.7) 允许对kubelet组件所需要的资源的访问，包括读取所有secret和对所有pod的写访问。 自Kubernetes 1.7开始, 相比较于这个角色，更推荐使用Node authorizer 以及NodeRestriction admission plugin， 并允许根据调度运行在节点上的pod授予kubelets API访问的权限。 自Kubernetes 1.7开始，当启用Node授权模式时，对system:nodes用户组的绑定将不会被自动创建。 system:node-proxier system:kube-proxy user 允许对kube-proxy组件所需要资源的访问。 其它组件角色 默认ClusterRole 默认ClusterRoleBinding 描述 system:auth-delegator None 允许委托认证和授权检查。 通常由附加API Server用于统一认证和授权。 system:heapster None Heapster组件的角色。 system:kube-aggregator None kube-aggregator组件的角色。 system:kube-dns kube-dns service account in the kube-systemnamespace kube-dns组件的角色。 system:node-bootstrapper None 允许对执行Kubelet TLS引导（Kubelet TLS bootstrapping）所需要资源的访问. system:node-problem-detector None node-problem-detector组件的角色。 system:persistent-volume-provisioner None 允许对大部分动态存储卷创建组件（dynamic volume provisioner）所需要资源的访问。 控制器（Controller）角色 Kubernetes controller manager负责运行核心控制循环。 当使用--use-service-account-credentials选项运行controller manager时，每个控制循环都将使用单独的服务账户启动。 而每个控制循环都存在对应的角色，前缀名为system:controller:。 如果不使用--use-service-account-credentials选项时，controller manager将会使用自己的凭证运行所有控制循环，而这些凭证必须被授予相关的角色。 这些角色包括： system:controller:attachdetach-controller system:controller:certificate-controller system:controller:cronjob-controller system:controller:daemon-set-controller system:controller:deployment-controller system:controller:disruption-controller system:controller:endpoint-controller system:controller:generic-garbage-collector system:controller:horizontal-pod-autoscaler system:controller:job-controller system:controller:namespace-controller system:controller:node-controller system:controller:persistent-volume-binder system:controller:pod-garbage-collector system:controller:replicaset-controller system:controller:replication-controller system:controller:resourcequota-controller system:controller:route-controller system:controller:service-account-controller system:controller:service-controller system:controller:statefulset-controller system:controller:ttl-controller 初始化与预防权限升级 RBAC API会阻止用户通过编辑角色或者角色绑定来升级权限。 由于这一点是在API级别实现的，所以在RBAC授权器（RBAC authorizer）未启用的状态下依然可以正常工作。 用户只有在拥有了角色所包含的所有权限的条件下才能创建／更新一个角色，这些操作还必须在角色所处的相同范围内进行（对于ClusterRole来说是集群范围，对于Role来说是在与角色相同的命名空间或者集群范围）。 例如，如果用户”user-1”没有权限读取集群范围内的secret列表，那么他也不能创建包含这种权限的ClusterRole。为了能够让用户创建／更新角色，需要： 授予用户一个角色以允许他们根据需要创建／更新Role或者ClusterRole对象。 授予用户一个角色包含他们在Role或者ClusterRole中所能够设置的所有权限。如果用户尝试创建或者修改Role或者ClusterRole以设置那些他们未被授权的权限时，这些API请求将被禁止。 用户只有在拥有所引用的角色中包含的所有权限时才可以创建／更新角色绑定（这些操作也必须在角色绑定所处的相同范围内进行）或者用户被明确授权可以在所引用的角色上执行绑定操作。 例如，如果用户”user-1”没有权限读取集群范围内的secret列表，那么他将不能创建ClusterRole来引用那些授予了此项权限的角色。为了能够让用户创建／更新角色绑定，需要： 授予用户一个角色以允许他们根据需要创建／更新RoleBinding或者ClusterRoleBinding对象。 授予用户绑定某一特定角色所需要的权限： 隐式地，通过授予用户所有所引用的角色中所包含的权限 显式地，通过授予用户在特定Role（或者ClusterRole）对象上执行bind操作的权限 例如，下面例子中的ClusterRole和RoleBinding将允许用户”user-1”授予其它用户”user-1-namespace”命名空间内的admin、edit和view等角色和角色绑定。 apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: role-grantor rules: - apiGroups: [&quot;rbac.authorization.k8s.io&quot;] resources: [&quot;rolebindings&quot;] verbs: [&quot;create&quot;] - apiGroups: [&quot;rbac.authorization.k8s.io&quot;] resources: [&quot;clusterroles&quot;] verbs: [&quot;bind&quot;] resourceNames: [&quot;admin&quot;,&quot;edit&quot;,&quot;view&quot;] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: role-grantor-binding namespace: user-1-namespace roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: role-grantor subjects: - apiGroup: rbac.authorization.k8s.io kind: User name: user-1 当初始化第一个角色和角色绑定时，初始用户需要能够授予他们尚未拥有的权限。 初始化初始角色和角色绑定时需要： 使用包含system：masters用户组的凭证，该用户组通过默认绑定绑定到cluster-admin超级用户角色。 如果您的API Server在运行时启用了非安全端口（--insecure-port），您也可以通过这个没有施行认证或者授权的端口发送角色或者角色绑定请求。 一些命令行工具 有两个kubectl命令可以用于在命名空间内或者整个集群内授予角色。 kubectl create rolebinding 在某一特定命名空间内授予Role或者ClusterRole。示例如下： 在名为”acme”的命名空间中将admin ClusterRole授予用户”bob”： kubectl create rolebinding bob-admin-binding --clusterrole=admin --user=bob --namespace=acme 在名为”acme”的命名空间中将view ClusterRole授予服务账户”myapp”： kubectl create rolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp --namespace=acme kubectl create clusterrolebinding 在整个集群中授予ClusterRole，包括所有命名空间。示例如下： 在整个集群范围内将cluster-admin ClusterRole授予用户”root”： kubectl create clusterrolebinding root-cluster-admin-binding --clusterrole=cluster-admin --user=root 在整个集群范围内将system:node ClusterRole授予用户”kubelet”： kubectl create clusterrolebinding kubelet-node-binding --clusterrole=system:node --user=kubelet 在整个集群范围内将view ClusterRole授予命名空间”acme”内的服务账户”myapp”： kubectl create clusterrolebinding myapp-view-binding --clusterrole=view --serviceaccount=acme:myapp 请参阅CLI帮助文档以获得上述命令的详细用法 服务账户（Service Account）权限 默认的RBAC策略将授予控制平面组件（control-plane component）、节点（node）和控制器（controller）一组范围受限的权限， 但对于”kube-system”命名空间以外的服务账户，则不授予任何权限（超出授予所有认证用户的发现权限）。 这一点允许您根据需要向特定服务账号授予特定权限。 细粒度的角色绑定将提供更好的安全性，但需要更多精力管理。 更粗粒度的授权可能授予服务账号不需要的API访问权限（甚至导致潜在授权扩散），但更易于管理。 从最安全到最不安全可以排序以下方法： 对某一特定应用程序的服务账户授予角色（最佳实践） 要求应用程序在其pod规范（pod spec）中指定serviceAccountName字段，并且要创建相应服务账户（例如通过API、应用程序清单或者命令kubectl create serviceaccount等）。 例如，在”my-namespace”命名空间中授予服务账户”my-sa”只读权限： kubectl create rolebinding my-sa-view \ --clusterrole=view \ --serviceaccount=my-namespace:my-sa \ --namespace=my-namespace 在某一命名空间中授予”default”服务账号一个角色 如果一个应用程序没有在其pod规范中指定serviceAccountName，它将默认使用”default”服务账号。 注意：授予”default”服务账号的权限将可用于命名空间内任何没有指定serviceAccountName的pod。 下面的例子将在”my-namespace”命名空间内授予”default”服务账号只读权限： kubectl create rolebinding default-view \ --clusterrole=view \ --serviceaccount=my-namespace:default \ --namespace=my-namespace 目前，许多[加载项（addon）]（/ docs / concepts / cluster-administration / addons /）作为”kube-system”命名空间中的”default”服务帐户运行。 要允许这些加载项使用超级用户访问权限，请将cluster-admin权限授予”kube-system”命名空间中的”default”服务帐户。 注意：启用上述操作意味着”kube-system”命名空间将包含允许超级用户访问API的秘钥。 kubectl create clusterrolebinding add-on-cluster-admin \ --clusterrole=cluster-admin \ --serviceaccount=kube-system:default 为命名空间中所有的服务账号授予角色 如果您希望命名空间内的所有应用程序都拥有同一个角色，无论它们使用什么服务账户，您可以为该命名空间的服务账户用户组授予角色。 下面的例子将授予”my-namespace”命名空间中的所有服务账户只读权限： kubectl create rolebinding serviceaccounts-view \ --clusterrole=view \ --group=system:serviceaccounts:my-namespace \ --namespace=my-namespace 对集群范围内的所有服务账户授予一个受限角色（不鼓励） 如果您不想管理每个命名空间的权限，则可以将集群范围角色授予所有服务帐户。 下面的例子将所有命名空间中的只读权限授予集群中的所有服务账户： kubectl create clusterrolebinding serviceaccounts-view \ --clusterrole=view \ --group=system:serviceaccounts 授予超级用户访问权限给集群范围内的所有服务帐户（强烈不鼓励） 如果您根本不关心权限分块，您可以对所有服务账户授予超级用户访问权限。 警告：这种做法将允许任何具有读取权限的用户访问secret或者通过创建一个容器的方式来访问超级用户的凭据。 kubectl create clusterrolebinding serviceaccounts-cluster-admin \ --clusterrole=cluster-admin \ --group=system:serviceaccounts 从版本1.5升级 在Kubernetes 1.6之前，许多部署使用非常宽泛的ABAC策略，包括授予对所有服务帐户的完整API访问权限。 默认的RBAC策略将授予控制平面组件（control-plane components）、节点（nodes）和控制器（controller）一组范围受限的权限， 但对于”kube-system”命名空间以外的服务账户，则不授予任何权限（超出授予所有认证用户的发现权限）。 虽然安全性更高，但这可能会影响到期望自动接收API权限的现有工作负载。 以下是管理此转换的两种方法： 并行授权器（authorizer） 同时运行RBAC和ABAC授权器，并包括旧版ABAC策略： --authorization-mode=RBAC,ABAC --authorization-policy-file=mypolicy.jsonl RBAC授权器将尝试首先授权请求。如果RBAC授权器拒绝API请求，则ABAC授权器将被运行。这意味着RBAC策略或者ABAC策略所允许的任何请求都是可通过的。 当以日志级别为2或更高（--v = 2）运行时，您可以在API Server日志中看到RBAC拒绝请求信息（以RBAC DENY:为前缀）。 您可以使用该信息来确定哪些角色需要授予哪些用户，用户组或服务帐户。 一旦授予服务帐户角色，并且服务器日志中没有RBAC拒绝消息的工作负载正在运行，您可以删除ABAC授权器。 宽泛的RBAC权限 您可以使用RBAC角色绑定来复制一个宽泛的策略。 警告：以下政策略允许所有服务帐户作为集群管理员。 运行在容器中的任何应用程序都会自动接收服务帐户凭据，并且可以对API执行任何操作，包括查看secret和修改权限。 因此，并不推荐使用这种策略。 kubectl create clusterrolebinding permissive-binding \ --clusterrole=cluster-admin \ --user=admin \ --user=kubelet \ --group=system:serviceaccounts</summary></entry><entry><title type="html">手动搭建kubernetes HA集群(二)</title><link href="https://kevinguo.me/2017/11/28/manual-deploy-kubernetes-2/" rel="alternate" type="text/html" title="手动搭建kubernetes HA集群(二)" /><published>2017-11-28T00:00:00+08:00</published><updated>2017-11-28T00:00:00+08:00</updated><id>https://kevinguo.me/2017/11/28/manual-deploy-kubernetes-2</id><content type="html" xml:base="https://kevinguo.me/2017/11/28/manual-deploy-kubernetes-2/">&lt;blockquote&gt;
  &lt;p&gt;我们在第一章的时候，通过手动的方式搭建好了kubernetes集群，并且在上面跑了一些基础的服务，那么我们要如何将这些服务暴露出来呢，这一章重点介绍关于kubernetes的服务暴露&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;一kubernetes-服务暴露介绍&quot;&gt;一、Kubernetes 服务暴露介绍&lt;/h3&gt;

&lt;p&gt;关于服务暴露，常见的有如下几种方式:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LoadBlancer Service&lt;/li&gt;
  &lt;li&gt;NodePort Service&lt;/li&gt;
  &lt;li&gt;Ingress&lt;/li&gt;
  &lt;li&gt;traefik&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;11loadblancer-service&quot;&gt;1.1、LoadBlancer Service&lt;/h4&gt;

&lt;p&gt;LoandBlancer Service 是kubernetes深度结合云平台的一个组件；当使用LoandBlancer Service暴露服务时，实际上是通过向底层云平台申请创建一个负载均衡器来向外暴露服务；目前LoadBlancer Service支持的云平台已经相对完善，比如国外的GCE、DigitalOcean，国内的阿里云，私有云Openstack等等，由于LoadBlancer Service深度结合了云平台，所以只能在一些云平台使用。&lt;/p&gt;

&lt;h4 id=&quot;12nodeport-service&quot;&gt;1.2、NodePort Service&lt;/h4&gt;

&lt;p&gt;NodePort Service 顾名思义，实质上就是通过在集群的每个node上暴露一个端口，然后将这个端口隐射到某个具体的service来实现，虽然每个node的端口有很多(0~65535)，但由于安全性和易用性(服务多了，端口记不住，容易混乱)，实际上使用的可能并不多&lt;/p&gt;

&lt;h4 id=&quot;13ingress&quot;&gt;1.3、Ingress&lt;/h4&gt;

&lt;p&gt;ingress 这东西在1.2后才出现的，大致原理就是通过一个ingress controller来实时感知service、pod的变化，然后结合ingress生成配置，更新内部的反代，刷新配置，达到服务暴露的目的&lt;/p&gt;

&lt;h4 id=&quot;14traefik&quot;&gt;1.4、traefik&lt;/h4&gt;

&lt;p&gt;traefik 笔者并没有使用过，大致意思是抛弃了ingress controller，因为traefik本身就能和kubernetes API交互，感知后端变化，再根据ingress生成规则，暴露服务。&lt;/p&gt;

&lt;h3 id=&quot;二fabioconsulregistrator-实现服务暴露&quot;&gt;二、fabio+consul+registrator 实现服务暴露&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;前面简单的介绍了几种kubernetes中的服务暴露方式，但是我这里一种都没有使用，为什么呢，&lt;strong&gt;因为通过service或者Nodeport来实现服务发现都是使用的iptables来进行负载的，性能上总是有些损耗的&lt;/strong&gt;，所以这里我使用consul+registrator+fabio来实现kubernetes内部服务的暴露&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;组件介绍：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;fabio&lt;/li&gt;
  &lt;li&gt;registrator&lt;/li&gt;
  &lt;li&gt;consul&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;21fabio&quot;&gt;2.1、fabio&lt;/h4&gt;

&lt;p&gt;fabio 是ebay团队用golang开发的一个快速、简单零配置就能够让consul部署的应用快速支持http(s)的负载均衡路由器，支持服务发现，自动生成路由
我们只需要在consul注册服务，提供一个健康检查，fabio就会将流量路由到这些服务上&lt;/p&gt;

&lt;h4 id=&quot;22registrator&quot;&gt;2.2、registrator&lt;/h4&gt;

&lt;p&gt;registrator(注册器)，能够实时的监听docker的event，动态的注册docker 容器服务到consul、etcd或zookeeper中&lt;/p&gt;

&lt;h4 id=&quot;23consul&quot;&gt;2.3、consul&lt;/h4&gt;

&lt;p&gt;Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对&lt;/p&gt;

&lt;h4 id=&quot;24具体部署过程&quot;&gt;2.4、具体部署过程&lt;/h4&gt;

&lt;p&gt;具体架构图如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/fabio-consul-registrator.png&quot; alt=&quot;fabio-consul-registrator&quot; /&gt;&lt;/p&gt;

&lt;p&gt;首先我们需要部署一套&lt;code class=&quot;highlighter-rouge&quot;&gt;consul server cluster&lt;/code&gt;，具体部署过程，这里就不再演示了，请参考&lt;a href=&quot;https://github.com/kaizamm/consul/blob/master/consul%2Bdocker%2Bregistrator.md&quot;&gt;Consul 集群搭建&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;然后，我们所有的节点都需要包含在&lt;code class=&quot;highlighter-rouge&quot;&gt;calico-network&lt;/code&gt;范围之内，calico网络部署请参考&lt;a href=&quot;https://kevinguo.me/2017/09/22/manual-deploy-kubernetes/&quot;&gt;第一章&lt;/a&gt;,将所有fabio所在的节点配置为noscheduler&lt;/p&gt;

&lt;p&gt;1.consul client 部署&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我们在每个节点上跑一个consul-client，你可以以daemonset的方式部署，也可以直接以二进制的方式部署，用systemd管理起来，这里用二进制的方式&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;/usr/lib/systemd/system/consul.service&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Consul service discovery agent
&lt;span class=&quot;nv&quot;&gt;Requires&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;User&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;consul
&lt;span class=&quot;nv&quot;&gt;Group&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;consul
&lt;span class=&quot;nv&quot;&gt;EnvironmentFile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/etc/default/consul
&lt;span class=&quot;nv&quot;&gt;Environment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GOMAXPROCS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;ExecStartPre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=[&lt;/span&gt; -f &lt;span class=&quot;s2&quot;&gt;&quot;/opt/consul/run/consul.pid&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; /usr/bin/rm -f /opt/consul/run/consul.pid
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/bin/consul agent &lt;span class=&quot;nv&quot;&gt;$CONSUL_FLAGS&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ExecReload&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/bin/kill -HUP &lt;span class=&quot;nv&quot;&gt;$MAINPID&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;KillSignal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;SIGTERM
&lt;span class=&quot;nv&quot;&gt;TimeoutStopSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;/etc/default/consul&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;CONSUL_FLAGS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;-ui -data-dir=/opt/consul/data -config-dir=/opt/consul/conf -pid-file=/opt/consul/run/consul.pid -client=0.0.0.0 -bind=172.29.151.4 -node=consul-client04 -retry-join=172.30.33.39 -retry-join=172.30.33.40 -retry-join=172.30.33.41 -retry-interval=3s&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.registrator 部署&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我们在每个node 节点上跑一个registrator，同样可以以daemonset的方式部署，或者使用docker container的方式部署，用systemd管理，这里我们通过daemonset的方式部署&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;registrator.yaml&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;DaemonSet&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;creationTimestamp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;null&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;registrator&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;registrator&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;registrator&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;creationTimestamp&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;null&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;registrator&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker-sock&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;hostPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/var/run/docker.sock&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;hostNetwork&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;true&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;registrator&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp/docker.sock&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;docker-sock&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ganeshkaila/registrator:v7&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/bin/sh&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-c&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;registrator&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-useIpFromEnv=POD_IP&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-internal&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;consul://localhost:8500&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IfNotPresent&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NODE_NAME&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;fieldRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;s&quot;&gt;fieldPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spec.nodeName&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.fabio 部署&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我们在对应的节点上部署fabio，二进制文件部署，通过systemd管理&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;fabio.service&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;fabio
&lt;span class=&quot;nv&quot;&gt;Requires&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;Environment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;GOMAXPROCS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/bin/fabio
&lt;span class=&quot;nv&quot;&gt;ExecReload&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/bin/kill -HUP &lt;span class=&quot;nv&quot;&gt;$MAINPID&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;KillSignal&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;SIGTERM

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;至此，我们的fabio+consul+registrator就算是部署完成了，那么我们怎么使用呢，这里，我们以&lt;code class=&quot;highlighter-rouge&quot;&gt;kubernetes-dashboard&lt;/code&gt;为例&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;kubernetes-dashboard.yaml&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Secret&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;k8s-app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard-certs&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Opaque&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ServiceAccount&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;k8s-app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# ------------------- Dashboard Role &amp;amp; Role Binding ------------------- #&lt;/span&gt;

&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Role&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard-minimal&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# Allow Dashboard to create and watch for changes of 'kubernetes-dashboard-key-holder' secret.&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;secrets&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;create&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;watch&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Allow Dashboard to get, update and delete Dashboard exclusive secrets.&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;secrets&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;resourceNames&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kubernetes-dashboard-key-holder&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;update&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;delete&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map.&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;configmaps&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;resourceNames&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;kubernetes-dashboard-settings&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;list&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;get&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;update&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Allow Dashboard to get metrics from heapster.&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apiGroups&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;services&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;resourceNames&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;heapster&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;verbs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;proxy&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;

  &lt;span class=&quot;s&quot;&gt;---&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io/v1beta1&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RoleBinding&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard-minimal&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;roleRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;apiGroup&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;rbac.authorization.k8s.io&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Role&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard-minimal&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;subjects&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ServiceAccount&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;

  &lt;span class=&quot;s&quot;&gt;---&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# ------------------- Dashboard Deployment ------------------- #&lt;/span&gt;

  &lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;k8s-app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kube-system&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;revisionHistoryLimit&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;10&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;k8s-app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;k8s-app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gcr.io/google_containers/kubernetes-dashboard-amd64:v1.7.1&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;9090&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# 这里个人添加一些必要的env&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;SERVICE_9090_CHECK_HTTP&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;SERVICE_9090_CHECK_INTERVAL&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;15s&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;SERVICE_9090_CHECK_TIME&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1s&quot;&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;SERVICE_NAME&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kubernetes-dashboard&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;SERVICE_TAGS&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;urlprefix-dashboard.quark.com/&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# 指定获取pod ip&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;POD_IP&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;fieldRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;s&quot;&gt;fieldPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;status.podIP&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# Uncomment the following line to manually specify Kubernetes API server Host&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# If not specified, Dashboard will attempt to auto discover the API server and connect&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# to it. Uncomment only if the default does not work.&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# - --apiserver-host=http://my-address:port&lt;/span&gt;
              &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--authentication-mode=basic&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# 这里添加一个连接heapster&lt;/span&gt;
              &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;--heapster-host=http://heapster.kube-system.svc.cluster.local&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;c1&quot;&gt;# Create on-disk volume to store exec logs&lt;/span&gt;
              &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/tmp&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;tmp-volume&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;livenessProbe&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;httpGet&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;s&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/&lt;/span&gt;
                  &lt;span class=&quot;s&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;9090&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;initialDelaySeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;30&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;timeoutSeconds&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;30&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;tmp-volume&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;emptyDir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;{}&lt;/span&gt;
              &lt;span class=&quot;c1&quot;&gt;# 这里的serviceAccountName改成default&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;serviceAccountName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# Comment the following tolerations if Dashboard must not be deployed on master&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;tolerations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;node-role.kubernetes.io/master&lt;/span&gt;
              &lt;span class=&quot;s&quot;&gt;effect&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NoSchedule&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;前端LB配置&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;server &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    listen       80;
    server_name  dashboard.quark.com;
    client_max_body_size 0;

    location / &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        proxy_pass                          http://fabio-server;
        proxy_set_header  Host              dashboard.quark.com;   &lt;span class=&quot;c&quot;&gt;# 这个位置一定是注册到consul里面的tags部分&lt;/span&gt;
        proxy_set_header  X-Real-IP         &lt;span class=&quot;nv&quot;&gt;$remote_addr&lt;/span&gt;; &lt;span class=&quot;c&quot;&gt;# pass on real client's IP&lt;/span&gt;


        proxy_set_header  X-Forwarded-For   &lt;span class=&quot;nv&quot;&gt;$proxy_add_x_forwarded_for&lt;/span&gt;;
        proxy_set_header  X-Forwarded-Proto &lt;span class=&quot;nv&quot;&gt;$scheme&lt;/span&gt;;
        proxy_read_timeout                  900;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;upstream fabio-server &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        server 172.29.151.4:9999;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;最后看看结果&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;fabio&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/fabio.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;kubernetes-dashboard&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/dashboard.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name>KevinGuo</name></author><summary type="html">我们在第一章的时候，通过手动的方式搭建好了kubernetes集群，并且在上面跑了一些基础的服务，那么我们要如何将这些服务暴露出来呢，这一章重点介绍关于kubernetes的服务暴露 一、Kubernetes 服务暴露介绍 关于服务暴露，常见的有如下几种方式: LoadBlancer Service NodePort Service Ingress traefik 1.1、LoadBlancer Service LoandBlancer Service 是kubernetes深度结合云平台的一个组件；当使用LoandBlancer Service暴露服务时，实际上是通过向底层云平台申请创建一个负载均衡器来向外暴露服务；目前LoadBlancer Service支持的云平台已经相对完善，比如国外的GCE、DigitalOcean，国内的阿里云，私有云Openstack等等，由于LoadBlancer Service深度结合了云平台，所以只能在一些云平台使用。 1.2、NodePort Service NodePort Service 顾名思义，实质上就是通过在集群的每个node上暴露一个端口，然后将这个端口隐射到某个具体的service来实现，虽然每个node的端口有很多(0~65535)，但由于安全性和易用性(服务多了，端口记不住，容易混乱)，实际上使用的可能并不多 1.3、Ingress ingress 这东西在1.2后才出现的，大致原理就是通过一个ingress controller来实时感知service、pod的变化，然后结合ingress生成配置，更新内部的反代，刷新配置，达到服务暴露的目的 1.4、traefik traefik 笔者并没有使用过，大致意思是抛弃了ingress controller，因为traefik本身就能和kubernetes API交互，感知后端变化，再根据ingress生成规则，暴露服务。 二、fabio+consul+registrator 实现服务暴露 前面简单的介绍了几种kubernetes中的服务暴露方式，但是我这里一种都没有使用，为什么呢，因为通过service或者Nodeport来实现服务发现都是使用的iptables来进行负载的，性能上总是有些损耗的，所以这里我使用consul+registrator+fabio来实现kubernetes内部服务的暴露 组件介绍： fabio registrator consul 2.1、fabio fabio 是ebay团队用golang开发的一个快速、简单零配置就能够让consul部署的应用快速支持http(s)的负载均衡路由器，支持服务发现，自动生成路由 我们只需要在consul注册服务，提供一个健康检查，fabio就会将流量路由到这些服务上 2.2、registrator registrator(注册器)，能够实时的监听docker的event，动态的注册docker 容器服务到consul、etcd或zookeeper中 2.3、consul Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件,由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源. Consul 支持健康检查,并允许 HTTP 和 DNS 协议调用 API 存储键值对 2.4、具体部署过程 具体架构图如下： 首先我们需要部署一套consul server cluster，具体部署过程，这里就不再演示了，请参考Consul 集群搭建 然后，我们所有的节点都需要包含在calico-network范围之内，calico网络部署请参考第一章,将所有fabio所在的节点配置为noscheduler 1.consul client 部署 我们在每个节点上跑一个consul-client，你可以以daemonset的方式部署，也可以直接以二进制的方式部署，用systemd管理起来，这里用二进制的方式 /usr/lib/systemd/system/consul.service [Unit] Description=Consul service discovery agent Requires=network-online.target After=network-online.target [Service] User=consul Group=consul EnvironmentFile=-/etc/default/consul Environment=GOMAXPROCS=2 Restart=on-failure ExecStartPre=[ -f &quot;/opt/consul/run/consul.pid&quot; ] &amp;amp;&amp;amp; /usr/bin/rm -f /opt/consul/run/consul.pid ExecStart=/usr/local/bin/consul agent $CONSUL_FLAGS ExecReload=/bin/kill -HUP $MAINPID KillSignal=SIGTERM TimeoutStopSec=5 [Install] WantedBy=multi-user.target /etc/default/consul CONSUL_FLAGS=&quot;-ui -data-dir=/opt/consul/data -config-dir=/opt/consul/conf -pid-file=/opt/consul/run/consul.pid -client=0.0.0.0 -bind=172.29.151.4 -node=consul-client04 -retry-join=172.30.33.39 -retry-join=172.30.33.40 -retry-join=172.30.33.41 -retry-interval=3s&quot; 2.registrator 部署 我们在每个node 节点上跑一个registrator，同样可以以daemonset的方式部署，或者使用docker container的方式部署，用systemd管理，这里我们通过daemonset的方式部署 registrator.yaml apiVersion: extensions/v1beta1 kind: DaemonSet metadata: creationTimestamp: null labels: run: registrator name: registrator spec: selector: matchLabels: run: registrator template: metadata: creationTimestamp: null labels: run: registrator spec: volumes: - name: docker-sock hostPath: path: /var/run/docker.sock hostNetwork: true containers: - name: registrator volumeMounts: - mountPath: /tmp/docker.sock name: docker-sock image: ganeshkaila/registrator:v7 command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;, &quot;registrator -useIpFromEnv=POD_IP -internal consul://localhost:8500&quot;] imagePullPolicy: IfNotPresent env: - name: NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName 3.fabio 部署 我们在对应的节点上部署fabio，二进制文件部署，通过systemd管理 fabio.service [Unit] Description=fabio Requires=network-online.target After=network-online.target [Service] Environment=GOMAXPROCS=2 Restart=on-failure ExecStart=/usr/local/bin/fabio ExecReload=/bin/kill -HUP $MAINPID KillSignal=SIGTERM [Install] WantedBy=multi-user.target 至此，我们的fabio+consul+registrator就算是部署完成了，那么我们怎么使用呢，这里，我们以kubernetes-dashboard为例 kubernetes-dashboard.yaml apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kube-system type: Opaque --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Role &amp;amp; Role Binding ------------------- # kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: kubernetes-dashboard-minimal namespace: kube-system rules: # Allow Dashboard to create and watch for changes of 'kubernetes-dashboard-key-holder' secret. - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] verbs: [&quot;create&quot;, &quot;watch&quot;] # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [&quot;&quot;] resources: [&quot;secrets&quot;] resourceNames: [&quot;kubernetes-dashboard-key-holder&quot;] verbs: [&quot;get&quot;, &quot;update&quot;, &quot;delete&quot;] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [&quot;&quot;] resources: [&quot;configmaps&quot;] resourceNames: [&quot;kubernetes-dashboard-settings&quot;] verbs: [&quot;list&quot;,&quot;get&quot;, &quot;update&quot;] # Allow Dashboard to get metrics from heapster. - apiGroups: [&quot;&quot;] resources: [&quot;services&quot;] resourceNames: [&quot;heapster&quot;] verbs: [&quot;proxy&quot;] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: kubernetes-dashboard-minimal namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard-minimal subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kube-system --- # ------------------- Dashboard Deployment ------------------- # kind: Deployment apiVersion: extensions/v1beta1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.7.1 ports: - containerPort: 9090 protocol: TCP # 这里个人添加一些必要的env env: - name: SERVICE_9090_CHECK_HTTP value: &quot;/&quot; - name: SERVICE_9090_CHECK_INTERVAL value: &quot;15s&quot; - name: SERVICE_9090_CHECK_TIME value: &quot;1s&quot; - name: SERVICE_NAME value: kubernetes-dashboard - name: SERVICE_TAGS value: urlprefix-dashboard.quark.com/ # 指定获取pod ip - name: POD_IP valueFrom: fieldRef: fieldPath: status.podIP args: # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port - --authentication-mode=basic # 这里添加一个连接heapster - --heapster-host=http://heapster.kube-system.svc.cluster.local volumeMounts: # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: path: / port: 9090 initialDelaySeconds: 30 timeoutSeconds: 30 volumes: - name: tmp-volume emptyDir: {} # 这里的serviceAccountName改成default serviceAccountName: default # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule 前端LB配置 server { listen 80; server_name dashboard.quark.com; client_max_body_size 0; location / { proxy_pass http://fabio-server; proxy_set_header Host dashboard.quark.com; # 这个位置一定是注册到consul里面的tags部分 proxy_set_header X-Real-IP $remote_addr; # pass on real client's IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_read_timeout 900; } } upstream fabio-server { server 172.29.151.4:9999; } 最后看看结果 fabio kubernetes-dashboard</summary></entry><entry><title type="html">Harbor+etcd+docker结合Ceph搭建高可用集群</title><link href="https://kevinguo.me/2017/11/22/harbor-etcd-ceph/" rel="alternate" type="text/html" title="Harbor+etcd+docker结合Ceph搭建高可用集群" /><published>2017-11-22T00:00:00+08:00</published><updated>2017-11-22T00:00:00+08:00</updated><id>https://kevinguo.me/2017/11/22/harbor-etcd-ceph</id><content type="html" xml:base="https://kevinguo.me/2017/11/22/harbor-etcd-ceph/">&lt;blockquote&gt;
  &lt;p&gt;由于原有的etcd一直是以单机的环境运行，不仅没有共享存储，也没有集群环境，而且生产上的私有image仓库也是使用的docker private registry，没有任何高可用，存在很大的隐患，所以，这里我搭建了一个套由ceph fs作为共享存储，为harbor和etcd集群提供存储服务的环境，特意在此记录下来，免得以后忘记了。整体架构图如下:&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/overall-structure.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;docker-安装&quot;&gt;docker 安装&lt;/h3&gt;

&lt;p&gt;1.安装依赖包&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo yum install -y yum-utils device-mapper-persistent-data lvm2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.添加docker stable 库&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.关闭edge和test库&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum-config-manager --disable docker-ce-edge
yum-config-manager --disable docker-ce-test
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.安装docker-ce&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yum install docker-ce -y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;5.配置docker graph driver&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim /etc/docker/daemon.json

&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;graph&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/data_docker/docker&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;6.配置成开机启动，这时候先别启动docker&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ceph-搭建&quot;&gt;ceph 搭建&lt;/h3&gt;

&lt;h4 id=&quot;在管理节点上操作&quot;&gt;在管理节点上操作&lt;/h4&gt;

&lt;p&gt;1.添加ceph源&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ceph-noarch]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Ceph noarch packages
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/rpm/el7/noarch
&lt;span class=&quot;nv&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rpm-md
&lt;span class=&quot;nv&quot;&gt;gpgkey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/keys/release.asc
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.更新并安装&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph-deploy&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo yum update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo yum install ceph-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.配置从部署机器到所有其他节点的免密钥登录，具体参考&lt;a href=&quot;https://kevinguo.me/2017/07/06/ansible-client/&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;在节点上操作&quot;&gt;在节点上操作&lt;/h4&gt;

&lt;p&gt;1.安装epel源&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum install yum-plugin-priorities -y
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum install epel-release -y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.校对时间，由于ceph使用Paxos算法保证数据一致性，所以安装前要先保证各个节点的时间同步&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo yum install ntp ntpdate ntp-doc

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ntpdate 0.cn.pool.ntp.org
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.开放所需端口或关闭防火墙&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;systemctl stop firewalld
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo firewall-cmd --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;public --add-port&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6789/tcp --permanent
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.关闭selinux&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo setenforce 0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;创建集群&quot;&gt;创建集群&lt;/h4&gt;

&lt;p&gt;1.由于ceph-deploy工具部署集群前需要创建一些集群配置信息，其保存在&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph.conf&lt;/code&gt;文件中，这个文件将来会被复制到每个节点的 &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/ceph/ceph.conf&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 创建集群配置目录&lt;/span&gt;
mkdir ceph-cluster &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;ceph-cluster
&lt;span class=&quot;c&quot;&gt;# 创建 monitor-node&lt;/span&gt;
ceph-deploy new i711-ustorage-1 i711-ustorage-2 i711-ustorage-3
&lt;span class=&quot;c&quot;&gt;# 追加 OSD 副本数量(测试虚拟机总共有3台)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;osd pool default size = 3&quot;&lt;/span&gt; &amp;gt;&amp;gt; ceph.conf
&lt;span class=&quot;c&quot;&gt;# 追加时间ceph允许的误差时间范围到ceph.conf&lt;/span&gt;
mon_clock_drift_allowed &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 5
mon_clock_drift_warn_backoff &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 30
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.创建集群使用 &lt;code class=&quot;highlighter-rouge&quot;&gt;ceph-deploy&lt;/code&gt;工具在部署节点上执行即可&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 安装ceph&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy install i711-ustorage-1 i711-ustorage-2 i711-ustorage-3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;注意：在部署节点部署的时候，可能会因为网络原因导致无法安装ceph和ceph-radosgw，这时候，我们在各个节点上手动安装一下&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 添加ceph 源&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Ceph]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Ceph packages &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$basearch&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://download.ceph.com/rpm-jewel/el7/&lt;span class=&quot;nv&quot;&gt;$basearch&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rpm-md
&lt;span class=&quot;nv&quot;&gt;gpgkey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/keys/release.asc
&lt;span class=&quot;nv&quot;&gt;priority&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Ceph-noarch]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Ceph noarch packages
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://download.ceph.com/rpm-jewel/el7/noarch
&lt;span class=&quot;nv&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rpm-md
&lt;span class=&quot;nv&quot;&gt;gpgkey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/keys/release.asc
&lt;span class=&quot;nv&quot;&gt;priority&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ceph-source]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Ceph &lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;packages
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://download.ceph.com/rpm-jewel/el7/SRPMS
&lt;span class=&quot;nv&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rpm-md
&lt;span class=&quot;nv&quot;&gt;gpgkey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/keys/release.asc
&lt;span class=&quot;nv&quot;&gt;priority&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1


&lt;span class=&quot;c&quot;&gt;# 执行安装&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum install ceph ceph-radosgw -y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.初始化monitor node 和密钥文件&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy mon create-initial
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.在管理节点上初始化osd&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy osd prepare i711-ustorage-1:/dev/sdb i711-ustorage-2:/dev/sdb i711-ustorage-3:/dev/sdb
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;5.在管理节点上激活osd&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy osd activate i711-ustorage-1:/dev/sdb1 i711-ustorage-2:/dev/sdb1 i711-ustorage-3:/dev/sdb1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;6.在管理节点上部署 ceph cli 工具和密钥文件&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy admin i711-ustorage-1 i711-ustorage-2 i711-ustorage-3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;7.确保你对 &lt;code class=&quot;highlighter-rouge&quot;&gt;ceph.client.admin.keyring&lt;/code&gt;有正确的操作权限，在每个节点上执行&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo chmod +r /etc/ceph/ceph.client.admin.keyring
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;8.最后检查集群状态&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph health
HEALTH_OK

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph osd tree
ID WEIGHT  TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 1.44955 root default                                              
-2 0.48318     host i711-ustorage-1                                   
 0 0.48318         osd.0                up  1.00000          1.00000
-3 0.48318     host i711-ustorage-2                                   
 1 0.48318         osd.1                up  1.00000          1.00000
-4 0.48318     host i711-ustorage-3                                   
 2 0.48318         osd.2                up  1.00000          1.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;ceph-rados创建&quot;&gt;Ceph rados创建&lt;/h4&gt;

&lt;p&gt;1.创建pool&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rados mkpool docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.创建image&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd create docker1 --size 100G -p docker
rbd create docker2 --size 100G -p docker
rbd create docker3 --size 100G -p docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.关闭不支持的特性&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd feature disable docker1 exclusive-lock, object-map, fast-diff, deep-flatten -p docker
rbd feature disable docker2 exclusive-lock, object-map, fast-diff, deep-flatten -p docker
rbd feature disable docker3 exclusive-lock, object-map, fast-diff, deep-flatten -p docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.隐射image到块设备(依次在每个节点映射)&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# docker1&lt;/span&gt;
rbd map docker1 --name client.admin -p docker
&lt;span class=&quot;c&quot;&gt;# docker2&lt;/span&gt;
rbd map docker2 --name client.admin -p docker
&lt;span class=&quot;c&quot;&gt;# docker3&lt;/span&gt;
rbd map docker3 --name client.admin -p docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;5.格式化设备&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里我们为了满足docker overlay2的需求，格式化的时候需要指定&lt;code class=&quot;highlighter-rouge&quot;&gt;-n ftype=1&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkfs.xfs -n &lt;span class=&quot;nv&quot;&gt;ftype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 /dev/rbd0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;6.创建docker root 目录，进行挂载，并添加到fstab中&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# docker1&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir -p /data_docker/docker1

&lt;span class=&quot;c&quot;&gt;# 将下面的内容添加到/etc/fstab中&lt;/span&gt;
/dev/rbd0       /data_docker/docker1    xfs     noauto  0 0

&lt;span class=&quot;c&quot;&gt;# docker2&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir -p /data_docker/docker2

&lt;span class=&quot;c&quot;&gt;# 将下面的内容添加到/etc/fstab中&lt;/span&gt;
/dev/rbd0       /data_docker/docker2    xfs     noauto  0 0

&lt;span class=&quot;c&quot;&gt;# docker3&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir -p /data_docker/docker3

&lt;span class=&quot;c&quot;&gt;# 将下面的内容添加到/etc/fstab中&lt;/span&gt;
/dev/rbd0       /data_docker/docker3    xfs     noauto  0 0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;因为ceph在每次重启的时候都需要去重新map，所以这里，我们需要配置下rbdmap.service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;7.配置&lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/ceph/rbdmap&lt;/code&gt;(依次在每个节点上操作)&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# docker1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# RbdDevice             Parameters&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#poolname/imagename     id=client,keyring=/etc/ceph/ceph.client.keyring&lt;/span&gt;
docker/docker1           &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin,keyring&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/ceph/ceph.client.admin.keyring

&lt;span class=&quot;c&quot;&gt;# docker2&lt;/span&gt;
docker/docker2           &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin,keyring&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/ceph/ceph.client.admin.keyring

&lt;span class=&quot;c&quot;&gt;# docker3&lt;/span&gt;
docker/docker3           &lt;span class=&quot;nv&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin,keyring&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/ceph/ceph.client.admin.keyring
&lt;span class=&quot;c&quot;&gt;# 配置成开机启动&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;rbdmap.service
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;8.修改docker 的service文件，将rbdmap添加到after后面，保证rbdmap先运行挂载成功之后，再启动docker&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim /usr/lib/systemd/system/docker.service

&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target firewalld.service rbdmap.service
&lt;span class=&quot;nv&quot;&gt;Wants&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;9.重启下机器，看看重启之后，docker是否启动成功，rbd image时候隐射成功&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker info
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;ceph-fs-创建&quot;&gt;Ceph FS 创建&lt;/h4&gt;

&lt;p&gt;1.创建MDS&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy mds create i711-ustorage-1 i711-ustorage-2 i711-ustorage-3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.创建pool和fs，创建pool需要指定PG数量&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph osd pool create data_data 32
ceph osd pool create data_metadata 32
ceph fs new data data_metadata data_data
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.复制密钥到文件中保存，将该文件复制到每个节点上的/etc/ceph下，并保证其权限&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;AQCF8QxaBIkrCxAAt12YUP+NzLv0TB5XHeJ4xQ==&quot;&lt;/span&gt; &amp;gt; ceph-key
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.将挂载添加到每个节点的fstab中&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;172.30.33.39:6789,172.30.33.40,172.30.33.41:6789:/      /data_harbor_etcd   ceph &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin,secretfile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/ceph/ceph-key,noatime,_netdev        0 2
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;5.重启机器后查看挂载&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;df -Th
Filesystem                                         Type      Size  Used Avail Use% Mounted on
172.30.33.39:6789,172.30.33.40,172.30.33.41:6789:/ ceph      1.5T  436M  1.5T   1% /data_harbor_etcd
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;至此，我们的共享存储就算是创建好了&lt;/p&gt;

&lt;h3 id=&quot;etcd-集群搭建&quot;&gt;etcd 集群搭建&lt;/h3&gt;

&lt;p&gt;etcd 集群的搭建很简单，我们只需要执行yum 安装即可，重点在后面的配置文件修改&lt;/p&gt;

&lt;p&gt;1.安装etcd&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum install etcd -y

&lt;span class=&quot;c&quot;&gt;# 在data目录下创建etcd目录，并修改其权限&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir -p /data/etcd
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;chown etcd.etcd  /data/etcd
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.修改配置文件,其他几个节点同理&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 本member名字&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_NAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd1

&lt;span class=&quot;c&quot;&gt;# 存放数据的位置&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_DATA_DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/data_harbor_etcd/etcd/etcd1.etcd&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 监听其他etcd实例的地址&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_LISTEN_PEER_URLS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://172.30.33.39:2380&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#监听客户端地址&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_LISTEN_CLIENT_URLS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://127.0.0.1:2379,http://172.30.33.39:2379&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 通知其他etcd实例地址&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_INITIAL_ADVERTISE_PEER_URLS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://172.30.33.39:2380&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 初始化集群内节点地址&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_INITIAL_CLUSTER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;etcd1=http://172.30.33.39:2380,etcd2=http://172.30.33.40:2380,etcd3=http://172.30.33.41:2380&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 初始化集群状态，new 表示新建&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_INITIAL_CLUSTER_STATE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;new&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 通知客户端地址&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_ADVERTISE_CLIENT_URLS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://172.30.33.39:2379&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.启动集群，然后查看&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;systemctl start etcd
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;etcd
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;etcdctl member list

186fb106f678cc55: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd3 &lt;span class=&quot;nv&quot;&gt;peerURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://172.30.33.41:2380 &lt;span class=&quot;nv&quot;&gt;clientURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://172.30.33.41:2379 &lt;span class=&quot;nv&quot;&gt;isLeader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;bbe26c67e852d6f9: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd1 &lt;span class=&quot;nv&quot;&gt;peerURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://172.30.33.39:2380 &lt;span class=&quot;nv&quot;&gt;clientURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://172.30.33.39:2379 &lt;span class=&quot;nv&quot;&gt;isLeader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;d4155475d1205f97: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd2 &lt;span class=&quot;nv&quot;&gt;peerURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://172.30.33.40:2380 &lt;span class=&quot;nv&quot;&gt;clientURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://172.30.33.40:2379 &lt;span class=&quot;nv&quot;&gt;isLeader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;至此，etcd集群，也算是搭建完成了，这里我们没有使用域名和https，如果需要使用https的话，则需要证书制作，可参考&lt;a href=&quot;https://kevinguo.me/2017/09/22/manual-deploy-kubernetes/#24-%E5%88%9B%E5%BB%BAetcd%E8%AF%81%E4%B9%A6%E9%85%8D%E7%BD%AE%E7%94%9F%E6%88%90-etcd-%E8%AF%81%E4%B9%A6%E5%92%8C%E7%A7%81%E9%92%A5&quot;&gt;k8s-manual&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;harbor-集群搭建&quot;&gt;harbor 集群搭建&lt;/h3&gt;

&lt;p&gt;1.安装docker-compose&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo curl -L https://github.com/docker/compose/releases/download/1.17.0/docker-compose-&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;uname -s&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;-&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;uname -m&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt; -o /usr/local/bin/docker-compose
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.配置docker网络&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 安装flannel网络&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum install flannel -y

&lt;span class=&quot;c&quot;&gt;# 配置flannel&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;FLANNEL_ETCD_ENDPOINTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://10.19.65.27:2379,http://10.19.65.28:2379,http://10.19.65.29:2379&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;FLANNEL_ETCD_PREFIX&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/quarkfinance.com/network&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;FLANNEL_OPTIONS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;--iface=eth0&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 在etcd中添加网络记录&lt;/span&gt;
etcdctl &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; /quarkfinance.com/network/config &lt;span class=&quot;s1&quot;&gt;'{ &quot;Network&quot;: &quot;10.1.0.0/16&quot; }'&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 在/usr/lib/systemd/system/docker.service中添加 $DOCKER_NETWORK_OPTIONS&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/bin/dockerd &lt;span class=&quot;nv&quot;&gt;$DOCKER_NETWORK_OPTIONS&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 配置docker用非root用户启动&lt;/span&gt;
groupadd docker
gpasswd -a &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;USER&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; docker
systemctl restart docker

&lt;span class=&quot;c&quot;&gt;# 启动docker，并将各服务配置成开机启动&lt;/span&gt;
systemctl start docker
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;flanneld
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;docker
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.查看docker网络是否生效&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;3: docker0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc noqueue state DOWN
    link/ether 02:42:90:6e:ac:b3 brd ff:ff:ff:ff:ff:ff
    inet 10.1.82.1/24 scope global docker0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:90ff:fe6e:acb3/64 scope link
       valid_lft forever preferred_lft forever
9: flannel0: &amp;lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1472 qdisc pfifo_fast state UNKNOWN qlen 500
    link/none
    inet 10.1.82.0/16 scope global flannel0
       valid_lft forever preferred_lft forever
    inet6 fe80::ff4f:42ff:391f:2a9d/64 scope link flags 800
       valid_lft forever preferred_lft forever
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.万事俱备，只欠harbor，下面，我们就来搭建我们的harbor&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;从git上下载最新的harbor包，git上有online和offlinle两个版，这里我选择online版，image都从外网拉取&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://github.com/vmware/harbor/releases/download/v1.2.2/harbor-online-installer-v1.2.2.tgz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;5.修改harbor配置文件&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1.修改挂载位置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;将&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;docker-compose.yml&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;和&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;harbor.cfg&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;文件中所有的&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;/data&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;都修改成&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;/data_harbor_etcd/harbor&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;2.由于使用外部mysql，所以删除&lt;code class=&quot;highlighter-rouge&quot;&gt;mysql&lt;/code&gt;service，并且删除掉其他service对mysql的依赖(depends_on)
&lt;strong&gt;如果你的harbor中已经有数据，那么请先导出mysql的数据，然后导入到你外部的mysql中&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -ti &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;containerID&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; /bin/bash
mysqldump -u root -proot123 --databases registry &amp;gt; registry.dump

docker cp &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;containerID&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;:/root/registry.dump ./
mysql -h &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;your_mysql_host&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; -u &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;your_mysql_user&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; -p
&lt;span class=&quot;gp&quot;&gt;mysql&amp;gt; &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt; ./registry.dump
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;3.在./common/templates/adminserver/env中添加如下内容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;MYSQL_HOST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;your_mysql_host&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;MYSQL_PORT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3306
&lt;span class=&quot;nv&quot;&gt;MYSQL_USR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;your_mysql_user&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;MYSQL_PWD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;your_mysql_passwd&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;4.修改./common/templates/adminserver/env文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;将&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RESET&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;改为&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RESET&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;5.由于要使用redis共享session，所以在./common/templates/ui/env中添加如下内容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;_REDIS_URL&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;redis_ip:6379,100,password,0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;6.关闭其他两个节点上的crt生成功能，留一个生成一套数字证书和私钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;custom_crt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;7.修改common/templates/nginx/nginx.http.conf，找到location /, location /v2/ and location /service/这3个配置块， 将这三个配置块中的proxy_set_header X-Forwarded-Proto $scheme;配置移除&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# When setting up Harbor behind other proxy, such as an Nginx instance, remove the below line if the proxy already has similar settings.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#proxy_set_header X-Forwarded-Proto $$scheme;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;8.修改&lt;code class=&quot;highlighter-rouge&quot;&gt;common/templates/registry/config.yml&lt;/code&gt;，修改&lt;code class=&quot;highlighter-rouge&quot;&gt;auth.token.realm&lt;/code&gt;的地址&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auth:
  token:
    issuer: harbor-token-issuer
    realm: https://harbor.quark.com/service/token
    &lt;span class=&quot;c&quot;&gt;# realm: $ui_url/service/token&lt;/span&gt;
    rootcertbundle: /etc/registry/root.crt
    service: harbor-registry
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;9.在各个节点上执行prepare脚本生成harbor各容器服务器的配置，然后启动容器&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./prepare
docker-compose up -d
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;注意：如果你启动之后，提示你无法连接远程数据库，请重启网络和docker daemon&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;6.前端nginx LB https配置&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1.新建一个证书脚本gencert.sh，内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# create self-signed server certificate:&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;read&lt;/span&gt; -p &lt;span class=&quot;s2&quot;&gt;&quot;Enter your domain [www.example.com]: &quot;&lt;/span&gt; DOMAIN

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Create server key...&quot;&lt;/span&gt;

openssl genrsa -des3 -out &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.key 1024

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Create server certificate signing request...&quot;&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;SUBJECT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/C=CN/ST=Hubei/L=Wuhan/O=quark/OU=devops/CN=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

openssl req -new -subj &lt;span class=&quot;nv&quot;&gt;$SUBJECT&lt;/span&gt; -key &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.key -out &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.csr

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Remove password...&quot;&lt;/span&gt;

mv &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.key &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.origin.key
openssl rsa -in &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.origin.key -out &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.key

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Sign SSL certificate...&quot;&lt;/span&gt;

openssl x509 -req -days 3650 -in &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.csr -signkey &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.key -out &lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;.crt

&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;TODO:&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Copy &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.crt to /etc/nginx/ssl/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.crt&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Copy &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.key to /etc/nginx/ssl/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.key&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Add configuration in nginx:&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;server {&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;    ...&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;    listen 443 ssl;&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;    ssl_certificate     /etc/nginx/ssl/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.crt;&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;    ssl_certificate_key /etc/nginx/ssl/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DOMAIN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.key;&quot;&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;}&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;2.生成证书，按提示输入&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./gencert.sh
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;3.生成内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@i612-devopsyw-1 ssl]# ll
total 20
-rwxr-xr-x 1 root root 949 Nov 22 16:28 gencert.sh
-rw-r--r-- 1 root root 887 Nov 22 17:40 harbor.quark.com.crt
-rw-r--r-- 1 root root 672 Nov 22 16:28 harbor.quark.com.csr
-rw-r--r-- 1 root root 887 Nov 22 17:40 harbor.quark.com.key
-rw-r--r-- 1 root root 963 Nov 22 16:28 harbor.quark.com.origin.key
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;4.将&lt;code class=&quot;highlighter-rouge&quot;&gt;harbor.quark.com.crt&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;harbor.quark.com.key&lt;/code&gt; 复制到你自己的nginx的ssl目录下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cp harbor.quark.com.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /data/bkv2.0.1/common/nginx/ssl
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;5.配置nginx的upstream 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;.conf&lt;/code&gt;文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;harbor-upstream.conf&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;upstream harbor-server &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        ip_hash;
        server 172.30.33.40:80;
        server 172.30.33.39:80;
        server 172.30.33.41:80;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
map &lt;span class=&quot;nv&quot;&gt;$http_upgrade&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$connection_upgrade&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    default Upgrade;
    &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt;      close;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;harbor.conf&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;server &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    listen 80;
    server_name harbor.quark.com;
    &lt;span class=&quot;c&quot;&gt;# Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response.&lt;/span&gt;
    rewrite ^&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt; https://&lt;span class=&quot;nv&quot;&gt;$host$1&lt;/span&gt; permanent;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

server &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    listen 443 ssl;
    server_name harbor.quark.com;
    ssl_certificate /data/bkv2.0.1/common/nginx/ssl/harbor.quark.com.crt;
    ssl_certificate_key /data/bkv2.0.1/common/nginx/ssl/harbor.quark.com.key;
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;
    access_log logs/harbor_access.log ;
    error_log logs/harbor_error.log ;
    location / &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        proxy_pass   http://harbor-server;
        proxy_set_header   Host             &lt;span class=&quot;nv&quot;&gt;$host&lt;/span&gt;;
        proxy_set_header   X-Real-IP        &lt;span class=&quot;nv&quot;&gt;$remote_addr&lt;/span&gt;;
        proxy_set_header   X-Forwarded-For  &lt;span class=&quot;nv&quot;&gt;$proxy_add_x_forwarded_for&lt;/span&gt;;
        proxy_set_header   X-Forwarded-Proto https;
        client_max_body_size 300M;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;6.重新加载nginx&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;nginx reload
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;7.访问试试效果&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/harbor-ui.png&quot; alt=&quot;harbor-ui&quot; /&gt;&lt;/p&gt;

&lt;p&gt;七.配置docker，让docker可以访问自签名证书的harbor&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;1.在每个docker主机上创建&lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/docker/certs.d/harbor.quark.com&lt;/code&gt;目录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir -p /etc/docker/certs.d/harbor.quark.com
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;2.将&lt;code class=&quot;highlighter-rouge&quot;&gt;harbor.quark.com.crt&lt;/code&gt; 复制到每个docker主机上的 &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/docker/certs.d/harbor.quark.com/&lt;/code&gt;目录下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;seq &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;39 31&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;scp harbor.quark.com.crt root@172.30.33.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/etc/docker/certs.d/habor.quark.com/
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;3.我们login试试，然后push一个image&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;docker login harbor.quark.com
Username: &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;your_ldap_user&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
Password:
Login Succeeded

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;docker tag vmware/harbor-adminserver:v1.2.2 harbor.quark.com/quark/harbor-adminserver:v1.2.2
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;docker push harbor.quark.com/quark/harbor-adminserver:v1.2.2
The push refers to a repository &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;harbor.quark.com/quark/harbor-adminserver]
4fe250d3c912: Pushed
2202528221a2: Pushed
abf0579c40fd: Pushed
dd60b611baaa: Pushed
v1.2.2: digest: sha256:80bfbc20a1ee2bc6b05dfe31f1e082c08961a0f62e94089ef952800e92a1fc4c size: 1157
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;看看harbor是否已经有了这个image呢&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/harbor-image.png&quot; alt=&quot;harbor-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;有了，至此，我们的harbor就算是搭建完成，因为我们是在内网使用，所以使用自签名证书无所谓，如果要上到公网，那么就必须使用可信任的证书机构颁发的证书了&lt;/p&gt;</content><author><name>KevinGuo</name></author><summary type="html">由于原有的etcd一直是以单机的环境运行，不仅没有共享存储，也没有集群环境，而且生产上的私有image仓库也是使用的docker private registry，没有任何高可用，存在很大的隐患，所以，这里我搭建了一个套由ceph fs作为共享存储，为harbor和etcd集群提供存储服务的环境，特意在此记录下来，免得以后忘记了。整体架构图如下: docker 安装 1.安装依赖包 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 2.添加docker stable 库 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 3.关闭edge和test库 yum-config-manager --disable docker-ce-edge yum-config-manager --disable docker-ce-test 4.安装docker-ce yum install docker-ce -y 5.配置docker graph driver vim /etc/docker/daemon.json { &quot;graph&quot;: &quot;/data_docker/docker&quot; } 6.配置成开机启动，这时候先别启动docker systemctl enable docker ceph 搭建 在管理节点上操作 1.添加ceph源 [ceph-noarch] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc 2.更新并安装ceph-deploy $ sudo yum update &amp;amp;&amp;amp; sudo yum install ceph-deploy 2.配置从部署机器到所有其他节点的免密钥登录，具体参考这里 在节点上操作 1.安装epel源 $ yum install yum-plugin-priorities -y $ yum install epel-release -y 2.校对时间，由于ceph使用Paxos算法保证数据一致性，所以安装前要先保证各个节点的时间同步 $ sudo yum install ntp ntpdate ntp-doc $ ntpdate 0.cn.pool.ntp.org 3.开放所需端口或关闭防火墙 $ systemctl stop firewalld $ sudo firewall-cmd --zone=public --add-port=6789/tcp --permanent 4.关闭selinux $ sudo setenforce 0 创建集群 1.由于ceph-deploy工具部署集群前需要创建一些集群配置信息，其保存在ceph.conf文件中，这个文件将来会被复制到每个节点的 /etc/ceph/ceph.conf # 创建集群配置目录 mkdir ceph-cluster &amp;amp;&amp;amp; cd ceph-cluster # 创建 monitor-node ceph-deploy new i711-ustorage-1 i711-ustorage-2 i711-ustorage-3 # 追加 OSD 副本数量(测试虚拟机总共有3台) echo &quot;osd pool default size = 3&quot; &amp;gt;&amp;gt; ceph.conf # 追加时间ceph允许的误差时间范围到ceph.conf mon_clock_drift_allowed = 5 mon_clock_drift_warn_backoff = 30 2.创建集群使用 ceph-deploy工具在部署节点上执行即可 # 安装ceph $ ceph-deploy install i711-ustorage-1 i711-ustorage-2 i711-ustorage-3 注意：在部署节点部署的时候，可能会因为网络原因导致无法安装ceph和ceph-radosgw，这时候，我们在各个节点上手动安装一下 # 添加ceph 源 [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [ceph-source] name=Ceph source packages baseurl=http://download.ceph.com/rpm-jewel/el7/SRPMS enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 # 执行安装 $ yum install ceph ceph-radosgw -y 3.初始化monitor node 和密钥文件 $ ceph-deploy mon create-initial 4.在管理节点上初始化osd $ ceph-deploy osd prepare i711-ustorage-1:/dev/sdb i711-ustorage-2:/dev/sdb i711-ustorage-3:/dev/sdb 5.在管理节点上激活osd $ ceph-deploy osd activate i711-ustorage-1:/dev/sdb1 i711-ustorage-2:/dev/sdb1 i711-ustorage-3:/dev/sdb1 6.在管理节点上部署 ceph cli 工具和密钥文件 $ ceph-deploy admin i711-ustorage-1 i711-ustorage-2 i711-ustorage-3 7.确保你对 ceph.client.admin.keyring有正确的操作权限，在每个节点上执行 $ sudo chmod +r /etc/ceph/ceph.client.admin.keyring 8.最后检查集群状态 $ ceph health HEALTH_OK $ ceph osd tree ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 1.44955 root default -2 0.48318 host i711-ustorage-1 0 0.48318 osd.0 up 1.00000 1.00000 -3 0.48318 host i711-ustorage-2 1 0.48318 osd.1 up 1.00000 1.00000 -4 0.48318 host i711-ustorage-3 2 0.48318 osd.2 up 1.00000 1.00000 Ceph rados创建 1.创建pool $ rados mkpool docker 2.创建image rbd create docker1 --size 100G -p docker rbd create docker2 --size 100G -p docker rbd create docker3 --size 100G -p docker 3.关闭不支持的特性 rbd feature disable docker1 exclusive-lock, object-map, fast-diff, deep-flatten -p docker rbd feature disable docker2 exclusive-lock, object-map, fast-diff, deep-flatten -p docker rbd feature disable docker3 exclusive-lock, object-map, fast-diff, deep-flatten -p docker 4.隐射image到块设备(依次在每个节点映射) # docker1 rbd map docker1 --name client.admin -p docker # docker2 rbd map docker2 --name client.admin -p docker # docker3 rbd map docker3 --name client.admin -p docker 5.格式化设备 这里我们为了满足docker overlay2的需求，格式化的时候需要指定-n ftype=1 mkfs.xfs -n ftype=1 /dev/rbd0 6.创建docker root 目录，进行挂载，并添加到fstab中 # docker1 $ mkdir -p /data_docker/docker1 # 将下面的内容添加到/etc/fstab中 /dev/rbd0 /data_docker/docker1 xfs noauto 0 0 # docker2 $ mkdir -p /data_docker/docker2 # 将下面的内容添加到/etc/fstab中 /dev/rbd0 /data_docker/docker2 xfs noauto 0 0 # docker3 $ mkdir -p /data_docker/docker3 # 将下面的内容添加到/etc/fstab中 /dev/rbd0 /data_docker/docker3 xfs noauto 0 0 因为ceph在每次重启的时候都需要去重新map，所以这里，我们需要配置下rbdmap.service 7.配置/etc/ceph/rbdmap(依次在每个节点上操作) # docker1 # RbdDevice Parameters #poolname/imagename id=client,keyring=/etc/ceph/ceph.client.keyring docker/docker1 id=admin,keyring=/etc/ceph/ceph.client.admin.keyring # docker2 docker/docker2 id=admin,keyring=/etc/ceph/ceph.client.admin.keyring # docker3 docker/docker3 id=admin,keyring=/etc/ceph/ceph.client.admin.keyring # 配置成开机启动 $ systemctl enable rbdmap.service 8.修改docker 的service文件，将rbdmap添加到after后面，保证rbdmap先运行挂载成功之后，再启动docker vim /usr/lib/systemd/system/docker.service After=network-online.target firewalld.service rbdmap.service Wants=network-online.target 9.重启下机器，看看重启之后，docker是否启动成功，rbd image时候隐射成功 docker info Ceph FS 创建 1.创建MDS $ ceph-deploy mds create i711-ustorage-1 i711-ustorage-2 i711-ustorage-3 2.创建pool和fs，创建pool需要指定PG数量 ceph osd pool create data_data 32 ceph osd pool create data_metadata 32 ceph fs new data data_metadata data_data 3.复制密钥到文件中保存，将该文件复制到每个节点上的/etc/ceph下，并保证其权限 echo &quot;AQCF8QxaBIkrCxAAt12YUP+NzLv0TB5XHeJ4xQ==&quot; &amp;gt; ceph-key 4.将挂载添加到每个节点的fstab中 172.30.33.39:6789,172.30.33.40,172.30.33.41:6789:/ /data_harbor_etcd ceph name=admin,secretfile=/etc/ceph/ceph-key,noatime,_netdev 0 2 5.重启机器后查看挂载 $ df -Th Filesystem Type Size Used Avail Use% Mounted on 172.30.33.39:6789,172.30.33.40,172.30.33.41:6789:/ ceph 1.5T 436M 1.5T 1% /data_harbor_etcd 至此，我们的共享存储就算是创建好了 etcd 集群搭建 etcd 集群的搭建很简单，我们只需要执行yum 安装即可，重点在后面的配置文件修改 1.安装etcd $ yum install etcd -y # 在data目录下创建etcd目录，并修改其权限 $ mkdir -p /data/etcd $ chown etcd.etcd /data/etcd 2.修改配置文件,其他几个节点同理 # 本member名字 ETCD_NAME=etcd1 # 存放数据的位置 ETCD_DATA_DIR=&quot;/data_harbor_etcd/etcd/etcd1.etcd&quot; # 监听其他etcd实例的地址 ETCD_LISTEN_PEER_URLS=&quot;http://172.30.33.39:2380&quot; #监听客户端地址 ETCD_LISTEN_CLIENT_URLS=&quot;http://127.0.0.1:2379,http://172.30.33.39:2379&quot; # 通知其他etcd实例地址 ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;http://172.30.33.39:2380&quot; # 初始化集群内节点地址 ETCD_INITIAL_CLUSTER=&quot;etcd1=http://172.30.33.39:2380,etcd2=http://172.30.33.40:2380,etcd3=http://172.30.33.41:2380&quot; # 初始化集群状态，new 表示新建 ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot; # 通知客户端地址 ETCD_ADVERTISE_CLIENT_URLS=&quot;http://172.30.33.39:2379&quot; 3.启动集群，然后查看 $ systemctl start etcd $ systemctl enable etcd $ etcdctl member list 186fb106f678cc55: name=etcd3 peerURLs=http://172.30.33.41:2380 clientURLs=http://172.30.33.41:2379 isLeader=false bbe26c67e852d6f9: name=etcd1 peerURLs=http://172.30.33.39:2380 clientURLs=http://172.30.33.39:2379 isLeader=true d4155475d1205f97: name=etcd2 peerURLs=http://172.30.33.40:2380 clientURLs=http://172.30.33.40:2379 isLeader=false 至此，etcd集群，也算是搭建完成了，这里我们没有使用域名和https，如果需要使用https的话，则需要证书制作，可参考k8s-manual harbor 集群搭建 1.安装docker-compose sudo curl -L https://github.com/docker/compose/releases/download/1.17.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose 2.配置docker网络 # 安装flannel网络 $ yum install flannel -y # 配置flannel FLANNEL_ETCD_ENDPOINTS=&quot;http://10.19.65.27:2379,http://10.19.65.28:2379,http://10.19.65.29:2379&quot; FLANNEL_ETCD_PREFIX=&quot;/quarkfinance.com/network&quot; FLANNEL_OPTIONS=&quot;--iface=eth0&quot; # 在etcd中添加网络记录 etcdctl set /quarkfinance.com/network/config '{ &quot;Network&quot;: &quot;10.1.0.0/16&quot; }' # 在/usr/lib/systemd/system/docker.service中添加 $DOCKER_NETWORK_OPTIONS ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS # 配置docker用非root用户启动 groupadd docker gpasswd -a ${USER} docker systemctl restart docker # 启动docker，并将各服务配置成开机启动 systemctl start docker systemctl enable flanneld systemctl enable docker 3.查看docker网络是否生效 3: docker0: &amp;lt;NO-CARRIER,BROADCAST,MULTICAST,UP&amp;gt; mtu 1500 qdisc noqueue state DOWN link/ether 02:42:90:6e:ac:b3 brd ff:ff:ff:ff:ff:ff inet 10.1.82.1/24 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:90ff:fe6e:acb3/64 scope link valid_lft forever preferred_lft forever 9: flannel0: &amp;lt;POINTOPOINT,MULTICAST,NOARP,UP,LOWER_UP&amp;gt; mtu 1472 qdisc pfifo_fast state UNKNOWN qlen 500 link/none inet 10.1.82.0/16 scope global flannel0 valid_lft forever preferred_lft forever inet6 fe80::ff4f:42ff:391f:2a9d/64 scope link flags 800 valid_lft forever preferred_lft forever 4.万事俱备，只欠harbor，下面，我们就来搭建我们的harbor 从git上下载最新的harbor包，git上有online和offlinle两个版，这里我选择online版，image都从外网拉取 wget https://github.com/vmware/harbor/releases/download/v1.2.2/harbor-online-installer-v1.2.2.tgz 5.修改harbor配置文件 1.修改挂载位置 将`docker-compose.yml`和`harbor.cfg`文件中所有的`/data`都修改成`/data_harbor_etcd/harbor` 2.由于使用外部mysql，所以删除mysqlservice，并且删除掉其他service对mysql的依赖(depends_on) 如果你的harbor中已经有数据，那么请先导出mysql的数据，然后导入到你外部的mysql中 docker exec -ti ${containerID} /bin/bash mysqldump -u root -proot123 --databases registry &amp;gt; registry.dump docker cp ${containerID}:/root/registry.dump ./ mysql -h ${your_mysql_host} -u ${your_mysql_user} -p mysql&amp;gt; source ./registry.dump 3.在./common/templates/adminserver/env中添加如下内容 MYSQL_HOST=${your_mysql_host} MYSQL_PORT=3306 MYSQL_USR=${your_mysql_user} MYSQL_PWD=${your_mysql_passwd} 4.修改./common/templates/adminserver/env文件 将`RESET=false`改为`RESET=true` 5.由于要使用redis共享session，所以在./common/templates/ui/env中添加如下内容 _REDIS_URL=redis_ip:6379,100,password,0 6.关闭其他两个节点上的crt生成功能，留一个生成一套数字证书和私钥 custom_crt=false 7.修改common/templates/nginx/nginx.http.conf，找到location /, location /v2/ and location /service/这3个配置块， 将这三个配置块中的proxy_set_header X-Forwarded-Proto $scheme;配置移除 # When setting up Harbor behind other proxy, such as an Nginx instance, remove the below line if the proxy already has similar settings. #proxy_set_header X-Forwarded-Proto $$scheme; 8.修改common/templates/registry/config.yml，修改auth.token.realm的地址 auth: token: issuer: harbor-token-issuer realm: https://harbor.quark.com/service/token # realm: $ui_url/service/token rootcertbundle: /etc/registry/root.crt service: harbor-registry 9.在各个节点上执行prepare脚本生成harbor各容器服务器的配置，然后启动容器 ./prepare docker-compose up -d 注意：如果你启动之后，提示你无法连接远程数据库，请重启网络和docker daemon 6.前端nginx LB https配置 1.新建一个证书脚本gencert.sh，内容如下 #!/bin/sh # create self-signed server certificate: read -p &quot;Enter your domain [www.example.com]: &quot; DOMAIN echo &quot;Create server key...&quot; openssl genrsa -des3 -out $DOMAIN.key 1024 echo &quot;Create server certificate signing request...&quot; SUBJECT=&quot;/C=CN/ST=Hubei/L=Wuhan/O=quark/OU=devops/CN=$DOMAIN&quot; openssl req -new -subj $SUBJECT -key $DOMAIN.key -out $DOMAIN.csr echo &quot;Remove password...&quot; mv $DOMAIN.key $DOMAIN.origin.key openssl rsa -in $DOMAIN.origin.key -out $DOMAIN.key echo &quot;Sign SSL certificate...&quot; openssl x509 -req -days 3650 -in $DOMAIN.csr -signkey $DOMAIN.key -out $DOMAIN.crt echo &quot;TODO:&quot; echo &quot;Copy $DOMAIN.crt to /etc/nginx/ssl/$DOMAIN.crt&quot; echo &quot;Copy $DOMAIN.key to /etc/nginx/ssl/$DOMAIN.key&quot; echo &quot;Add configuration in nginx:&quot; echo &quot;server {&quot; echo &quot; ...&quot; echo &quot; listen 443 ssl;&quot; echo &quot; ssl_certificate /etc/nginx/ssl/$DOMAIN.crt;&quot; echo &quot; ssl_certificate_key /etc/nginx/ssl/$DOMAIN.key;&quot; echo &quot;}&quot; 2.生成证书，按提示输入 ./gencert.sh 3.生成内容如下 [root@i612-devopsyw-1 ssl]# ll total 20 -rwxr-xr-x 1 root root 949 Nov 22 16:28 gencert.sh -rw-r--r-- 1 root root 887 Nov 22 17:40 harbor.quark.com.crt -rw-r--r-- 1 root root 672 Nov 22 16:28 harbor.quark.com.csr -rw-r--r-- 1 root root 887 Nov 22 17:40 harbor.quark.com.key -rw-r--r-- 1 root root 963 Nov 22 16:28 harbor.quark.com.origin.key 4.将harbor.quark.com.crt 和 harbor.quark.com.key 复制到你自己的nginx的ssl目录下 cp harbor.quark.com.* /data/bkv2.0.1/common/nginx/ssl 5.配置nginx的upstream 和 .conf文件 harbor-upstream.conf upstream harbor-server { ip_hash; server 172.30.33.40:80; server 172.30.33.39:80; server 172.30.33.41:80; } map $http_upgrade $connection_upgrade { default Upgrade; '' close; } harbor.conf server { listen 80; server_name harbor.quark.com; # Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response. rewrite ^(.*)$ https://$host$1 permanent; } server { listen 443 ssl; server_name harbor.quark.com; ssl_certificate /data/bkv2.0.1/common/nginx/ssl/harbor.quark.com.crt; ssl_certificate_key /data/bkv2.0.1/common/nginx/ssl/harbor.quark.com.key; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; access_log logs/harbor_access.log ; error_log logs/harbor_error.log ; location / { proxy_pass http://harbor-server; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto https; client_max_body_size 300M; } } 6.重新加载nginx $ nginx reload 7.访问试试效果 七.配置docker，让docker可以访问自签名证书的harbor 1.在每个docker主机上创建/etc/docker/certs.d/harbor.quark.com目录 $ mkdir -p /etc/docker/certs.d/harbor.quark.com 2.将harbor.quark.com.crt 复制到每个docker主机上的 /etc/docker/certs.d/harbor.quark.com/目录下 $ for IP in seq `39 31`;do scp harbor.quark.com.crt root@172.30.33.$IP:/etc/docker/certs.d/habor.quark.com/ done 3.我们login试试，然后push一个image $ docker login harbor.quark.com Username: ${your_ldap_user} Password: Login Succeeded $ docker tag vmware/harbor-adminserver:v1.2.2 harbor.quark.com/quark/harbor-adminserver:v1.2.2 $ docker push harbor.quark.com/quark/harbor-adminserver:v1.2.2 The push refers to a repository [harbor.quark.com/quark/harbor-adminserver] 4fe250d3c912: Pushed 2202528221a2: Pushed abf0579c40fd: Pushed dd60b611baaa: Pushed v1.2.2: digest: sha256:80bfbc20a1ee2bc6b05dfe31f1e082c08961a0f62e94089ef952800e92a1fc4c size: 1157 看看harbor是否已经有了这个image呢 有了，至此，我们的harbor就算是搭建完成，因为我们是在内网使用，所以使用自签名证书无所谓，如果要上到公网，那么就必须使用可信任的证书机构颁发的证书了</summary></entry><entry><title type="html">手动搭建kubernetes HA集群(一)</title><link href="https://kevinguo.me/2017/09/22/manual-deploy-kubernetes/" rel="alternate" type="text/html" title="手动搭建kubernetes HA集群(一)" /><published>2017-09-22T00:00:00+08:00</published><updated>2017-09-22T00:00:00+08:00</updated><id>https://kevinguo.me/2017/09/22/manual-deploy-kubernetes</id><content type="html" xml:base="https://kevinguo.me/2017/09/22/manual-deploy-kubernetes/">&lt;blockquote&gt;
  &lt;p&gt;原有的环境需要迁移，现在需要重新搭建一套kubernetes，而且原来一直是用kargo来搭建，所有组件都是基于docker容器的，感觉有点不稳妥，所以正好这个时候有机会，可以纯手动部署一下，所有的关键组件都以二进制形式部署，并添加为系统服务，这里记录一下。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;该文档参考了诸多大神的文档,&lt;a href=&quot;https://mritd.me/2017/07/21/set-up-kubernetes-ha-cluster-by-binary/&quot;&gt;漠然&lt;/a&gt;、&lt;a href=&quot;http://blog.frognew.com/2017/04/install-ha-kubernetes-1.6-cluster.html&quot;&gt;青蛙小白&lt;/a&gt;，谨请原谅&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;一环境准备&quot;&gt;一.环境准备&lt;/h3&gt;

&lt;h5 id=&quot;11-系统环境&quot;&gt;1.1 系统环境&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;IP&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;HostName&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;节点&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;OS&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;172.29.151.1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;k8s-mon-master01&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;master MON etcd&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;centOS7.4.1708&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;172.29.151.2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;k8s-mon-master02&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;master MON etcd&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;centOS7.4.1708&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;172.29.151.3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;k8s-mon-master03&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;master MON etcd&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;centOS7.4.1708&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;172.29.151.4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;k8s-harbor&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;harbor&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;centOS7.4.1708&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;172.29.151.5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;k8s-mds-node01&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;node MDS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;centOS7.4.1708&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;172.29.151.6&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;k8s-mds-node02&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;node MDS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;centOS7.4.1708&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;172.29.151.7&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;k8s-mds-node03&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;node MDS&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;centOS7.4.1708&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;172.29.151.8&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;k8s-console&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;console&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;centOS7.4.1708&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;12-系统组件&quot;&gt;1.2 系统组件&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在安装之前，我们要确认，我们具体需要准备哪些系统组件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;docker&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/coreos/etcd/releases/download/v3.2.7/etcd-v3.2.7-linux-amd64.tar.gz&quot;&gt;etcd-3.2.7&lt;/a&gt;(etcd、etcdctl)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.k8s.io/v1.7.6/kubernetes-server-linux-amd64.tar.gz&quot;&gt;kubernetes-server-1.7.6&lt;/a&gt;(kube-apiserver、kube-controller-manager、kube-scheduler)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.k8s.io/v1.7.6/kubernetes-node-linux-amd64.tar.gz&quot;&gt;kubernetes-node-1.7.6&lt;/a&gt;(kube-proxy、kubelet、kubectl)&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;13-自签名证书&quot;&gt;1.3 自签名证书&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;因为所有的组件之间都是通过证书认证的方式来进行通信的，所以我们还得确认下，我们到底需要哪些证书&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;CA&lt;/li&gt;
  &lt;li&gt;etcd&lt;/li&gt;
  &lt;li&gt;kube-apiserver&lt;/li&gt;
  &lt;li&gt;kube-controller-manager&lt;/li&gt;
  &lt;li&gt;kube-scheduler&lt;/li&gt;
  &lt;li&gt;kube-proxy&lt;/li&gt;
  &lt;li&gt;kubelet&lt;/li&gt;
  &lt;li&gt;kube-admin&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;14-系统配置&quot;&gt;1.4 系统配置&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;关闭所有节点的selinux、iptables、firewalld&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl stop iptables
systemctl stop firewalld
systemctl disable iptables
systemctl disable firewalld

vi /etc/selinux/config
&lt;span class=&quot;nv&quot;&gt;SELINUX&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;disable
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;del&gt;如果你想使用flanel网络，还记得在所有节点上创建 &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/sysctl.d/k8s.conf&lt;/code&gt;文件，添加如下内容，如果是calico网络，请忽略这步&lt;/del&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;net.bridge.bridge-nf-call-ip6tables &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1
net.bridge.bridge-nf-call-iptables &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1

&lt;span class=&quot;c&quot;&gt;# 执行命令使其生效&lt;/span&gt;
sysctl -p /etc/sysctl.d/k8s.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;在所有节点上编辑 &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/hosts&lt;/code&gt;文件，配置host通信&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi /etc/hosts

172.29.151.1 k8s-mon-master01
172.29.151.2 k8s-mon-master02
172.29.151.3 k8s-mon-master03
172.29.151.4 k8s-harbor
172.29.151.5 k8s-mds-node01
172.29.151.6 k8s-mds-node02
172.29.151.7 k8s-mds-node03
172.29.151.8 k8s-console
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;15-创建基本用户&quot;&gt;1.5 创建基本用户&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 在master节点上创建etcd用户&lt;/span&gt;
useradd etcd -d /var/lib/etcd -c &lt;span class=&quot;s2&quot;&gt;&quot;Etcd user&quot;&lt;/span&gt; -r -s /sbin/nologin

&lt;span class=&quot;c&quot;&gt;# 在maaster节点和node节点上创建kube用户&lt;/span&gt;
useradd kube  -M -c &lt;span class=&quot;s2&quot;&gt;&quot;Kubernetes user&quot;&lt;/span&gt; -r -s /sbin/nologin
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;16-在console上配置免密钥登录&quot;&gt;1.6 在console上配置免密钥登录&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;所有证书分发，二进制文件分发，配置文件分发，都将在 &lt;code class=&quot;highlighter-rouge&quot;&gt;k8s-console&lt;/code&gt; 上执行，所以该节点主机对集群内所有节点设置了免密钥登录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;具体过程可参考&lt;a href=&quot;https://kevinguo.me/2017/07/06/ansible-client/&quot;&gt;免密钥登录&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;二创建验证&quot;&gt;二.创建验证&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;因为所有组件和apiserver进行通信，都需要使用证书来进行认证，所以这里我们使用CloudFlare的PKI工具集 cfssl 来生成CA证书和其密钥文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;如果你的kube-controller-manager、kube-scheduler同apiserver之间通信不需要进行证书认证(毕竟他们都在同一台机器上)，那么下面有关kube-controller-manager、kube-scheduler的证书步骤可以忽略；而在该实验中，我考虑到后面假若它们不在同一台机器上，所以也记录了kube-controller-manager、kube-scheduler的证书创建配置过程&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;21-安装cfssl&quot;&gt;2.1 安装cfssl&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64

wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64

chmod +x cfssl_linux-amd64 cfssljson_linux-amd64

mv cfssl_linux-amd64 /usr/local/bin/cfssl
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;22-创建ca证书配置生成ca证书和私钥&quot;&gt;2.2 创建CA证书配置，生成CA证书和私钥&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;先用 &lt;code class=&quot;highlighter-rouge&quot;&gt;cfssl&lt;/code&gt; 命令生成包含默认配置的 &lt;code class=&quot;highlighter-rouge&quot;&gt;config.json&lt;/code&gt;和 &lt;code class=&quot;highlighter-rouge&quot;&gt;csr.json&lt;/code&gt;文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir /opt/ssl
&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cfssl print-defaults config &amp;gt; config.json
cfssl print-defaults csr &amp;gt; csr.json
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;然后分别修改这两个文件为如下内容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;config.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;signing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;default&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;expiry&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;87600h&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;profiles&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;kubernetes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;usages&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;signing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;key encipherment&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;server auth&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
            &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;client auth&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;expiry&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;87600h&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile；&lt;/li&gt;
  &lt;li&gt;signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE；&lt;/li&gt;
  &lt;li&gt;server auth：表示client可以用该 CA 对server提供的证书进行验证；&lt;/li&gt;
  &lt;li&gt;client auth：表示server可以用该CA对client提供的证书进行验证；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;csr.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kubernetes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;algo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rsa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;names&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Wuhan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;L&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hubei&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;O&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;k8s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;OU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;System&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法；&lt;/li&gt;
  &lt;li&gt;O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)；&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;生成CA 证书和私钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cfssl gencert -initca csr.json | cfssljson -bare ca

&lt;span class=&quot;c&quot;&gt;# CA有关证书列表如下&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@k8s-console ssl]# tree
.
├── ca.csr
├── ca-key.pem
├── ca.pem
├── config.json
└── csr.json
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;24-创建etcd证书配置生成-etcd-证书和私钥&quot;&gt;2.4 创建etcd证书配置，生成 etcd 证书和私钥&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/ssl&lt;/code&gt; 下添加文件 &lt;code class=&quot;highlighter-rouge&quot;&gt;etcd-csr.json&lt;/code&gt;，内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;etcd-csr.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;etcd&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;hosts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;127.0.0.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.3&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;algo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rsa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;names&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Wuhan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;L&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hubei&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;O&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;k8s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;OU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;System&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;生成etcd证书和密钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cfssl gencert -ca&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-ca-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-config&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/config.json &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-profile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes etcd-csr.json | cfssljson -bare etcd

&lt;span class=&quot;c&quot;&gt;# etcd 有关证书证书列表如下&lt;/span&gt;
ls etcd&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
etcd.csr  etcd-csr.json  etcd-key.pem  etcd.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;25-创建kube-apiserver证书配置生成kube-apiserver证书和私钥&quot;&gt;2.5 创建kube-apiserver证书配置，生成kube-apiserver证书和私钥&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/ssl&lt;/code&gt; 下添加文件 &lt;code class=&quot;highlighter-rouge&quot;&gt;kube-apiserver-csr.json&lt;/code&gt;，内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;kube-apiserver-csr.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kubernetes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;hosts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;127.0.0.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10.254.0.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;localhost&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kubernetes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kubernetes.default&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kubernetes.default.svc&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kubernetes.default.svc.cluster&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kubernetes.default.svc.cluster.local&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;algo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rsa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;names&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Wuhan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;L&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hubei&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;O&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;k8s&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;OU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;System&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP 和 kubernetes 服务的服务 IP（一般是 kue-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;生成kube-apiserver证书和私钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cfssl gencert -ca&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-ca-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-config&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/config.json &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-profile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver

&lt;span class=&quot;c&quot;&gt;# 列出kube-apiserver有关证书&lt;/span&gt;
ls kube-apiserver&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
kube-apiserver.csr  kube-apiserver-csr.json  kube-apiserver-key.pem  kube-apiserver.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;26-创建kube-controller-manager证书配置生成kube-controller-manager证书和私钥&quot;&gt;2.6 创建kube-controller-manager证书配置，生成kube-controller-manager证书和私钥&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/ssl&lt;/code&gt; 下添加文件 &lt;code class=&quot;highlighter-rouge&quot;&gt;kube-controller-manager-csr.json&lt;/code&gt;，内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;kube-controller-manager-csr.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;system:kube-controller-manager&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;hosts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.3&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;algo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rsa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;names&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Wuhan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;L&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hubei&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;O&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;system:kube-controller-manager&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;OU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;System&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;CN 指定该证书的 User 为 system:kube-controller-manager&lt;/li&gt;
  &lt;li&gt;kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-controller-manager 与 ClusterRole system:kube-controller-manager 绑定，该 ClusterRole 授予了调用 kube-apiserver kube-controller-manager 相关 API 的权限&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;生成kube-controller-manager证书和私钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cfssl gencert -ca&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-ca-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-config&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/config.json &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-profile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager

&lt;span class=&quot;c&quot;&gt;# 列出kube-controller-manager有关证书&lt;/span&gt;
ls kube-controller-manager&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
kube-controller-manager.csr  kube-controller-manager-csr.json  kube-controller-manager-key.pem  kube-controller-manager.pem

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;27-创建kube-scheduler证书配置生成kube-scheduler证书和私钥&quot;&gt;2.7 创建kube-scheduler证书配置，生成kube-scheduler证书和私钥&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/ssl&lt;/code&gt; 下添加文件 &lt;code class=&quot;highlighter-rouge&quot;&gt;kube-scheduler-csr.json&lt;/code&gt;，内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;kube-scheduler-csr.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;system:kube-scheduler&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;hosts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.1&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.2&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.3&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;algo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rsa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;names&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Wuhan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;L&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hubei&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;O&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;system:kube-scheduler&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;OU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;System&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;CN 指定该证书的 User 为 system:kube-scheduler&lt;/li&gt;
  &lt;li&gt;kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-scheduler 与 ClusterRole system:kube-scheduler 绑定，该 ClusterRole 授予了调用 kube-apiserver kube-scheduler 相关 API 的权限&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;生成kube-scheduler证书和私钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cfssl gencert -ca&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-ca-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-config&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/config.json &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-profile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler

&lt;span class=&quot;c&quot;&gt;# 列出kube-scheduler有关证书&lt;/span&gt;
ls kube-scheduler&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
kube-scheduler.csr  kube-scheduler-csr.json  kube-scheduler-key.pem  kube-scheduler.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;28-创建kube-admin证书配置生成kube-admin证书和私钥&quot;&gt;2.8 创建kube-admin证书配置，生成kube-admin证书和私钥&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/ssl&lt;/code&gt; 下添加文件 &lt;code class=&quot;highlighter-rouge&quot;&gt;kube-admin-csr.json&lt;/code&gt;，内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;kube-admin-csr.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;kube-admin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;hosts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.8&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;algo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rsa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;names&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Wuhan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;L&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hubei&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;O&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;system:masters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;OU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;System&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权&lt;/li&gt;
  &lt;li&gt;kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限&lt;/li&gt;
  &lt;li&gt;OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;生成kube-admin证书和私钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cfssl gencert -ca&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-ca-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-config&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/config.json &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-profile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes kube-admin-csr.json | cfssljson -bare kube-admin

&lt;span class=&quot;c&quot;&gt;# 列出kube-admin有关证书&lt;/span&gt;
ls kube-admin&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
kube-admin.csr  kube-admin-csr.json  kube-admin-key.pem  kube-admin.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;29-创建kube-proxy证书配置生成kube-proxy证书和私钥&quot;&gt;2.9 创建kube-proxy证书配置，生成kube-proxy证书和私钥&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在&lt;code class=&quot;highlighter-rouge&quot;&gt;/opt/ssl&lt;/code&gt; 下添加文件 &lt;code class=&quot;highlighter-rouge&quot;&gt;kube-proxy-csr.json&lt;/code&gt;，内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;kube-proxy-csr.json&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;system:kube-proxy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;hosts&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;key&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;algo&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rsa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;size&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2048&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;names&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;C&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CN&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ST&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Wuhan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;L&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Hubei&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;O&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;system:kube-proxy&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;OU&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;System&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;CN 指定该证书的 User 为 system:kube-proxy&lt;/li&gt;
  &lt;li&gt;kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 ClusterRole system:node-proxier 绑定，该 ClusterRole 授予了调用 kube-apiserver Proxy 相关 API 的权限&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;生成kube-proxy证书和私钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cfssl gencert -ca&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-ca-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-config&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/config.json &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-profile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy

&lt;span class=&quot;c&quot;&gt;# 列出kube-proxy有关证书&lt;/span&gt;
ls kube-proxy&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;210-kubelet-证书和私钥&quot;&gt;2.10 kubelet 证书和私钥&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;kubelet 其实也可以手动通过CA来进行签发，但是这只能针对少数机器，毕竟我们在进行证书签发的时候，是需要绑定对应Node的IP的，如果node太多了，加IP就会很幸苦， 所以这里我们使用TLS 认证，由apiserver自动给符合条件的node签发证书，允许节点加入集群。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;kubelet 首次启动时想kube-apiserver发送TLS Bootstrapping请求，kube-apiserver验证kubelet请求中的token是否与它配置的token一致，如果一致则自动为kubelet生成证书和密钥。具体参考&lt;a href=&quot;https://k8smeetup.github.io/docs/admin/kubelet-tls-bootstrapping/&quot;&gt;kubelet-tls-bootstrapping&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;我们在k8s-console上生成token并分发到所有的master节点&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

head -c 16 /dev/urandom | od -An -t x | tr -d &lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;
04d9b6c6fd3ed8a3488b3b0913e87d64

vim token.csv
04d9b6c6fd3ed8a3488b3b0913e87d64,kubelet-bootstrap,10001,&lt;span class=&quot;s2&quot;&gt;&quot;system:kubelet-bootstrap&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;211-证书分发&quot;&gt;2.11 证书分发&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;既然证书都创建好了，那么这时候，我们就需要将对应的证书分发到对应的节点上去&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;master&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 1 3&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; mkdir -p /etc/kubernetes/ssl
  ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; mkdir -p /etc/etcd/ssl
  scp ca&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem kube-apiserver&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem kube-controller-manager&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem kube-scheduler&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/etc/kubernetes/ssl/
  ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; chown -R kube:kube /etc/kubernetes/ssl
  scp ca&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem etcd&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/etc/etcd/ssl
  ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; chown -R etcd:etcd /etc/etcd/ssl
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;node&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 5 7&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; mkdir -p /etc/kubernetes/ssl
  scp ca&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem kube-proxy&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/etc/kubernetes/ssl/
  ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; chown -R kube:kube /etc/kubernetes/ssl
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;console&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

cp kube-admin&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem /etc/kubernetes/ssl/
chown -R kube:kube /etc/kubernetes/ssl/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;211-token分发&quot;&gt;2.11 token分发&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl

&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 1 3&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; mkdir -p /etc/kubernetes/known_token
  scp token.csv root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/etc/kubernetes/known_token/
  ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; chown -R kube:kube /etc/kubernetes/known_token
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;h3 id=&quot;三分发二进制文件&quot;&gt;三.分发二进制文件&lt;/h3&gt;

&lt;h5 id=&quot;分发etcd&quot;&gt;分发etcd&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar zxvf etcd-v3.2.7-linux-amd64.tar.gz
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;etcd-v3.2.7-linux-amd64

&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 1 3&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;scp etcd etcdctl root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/usr/bin/
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;分发kubernetes-master&quot;&gt;分发kubernetes master&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar zxvf kubernetes-server-linux-amd64.tar.gz
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;kubernetes/server/bin

&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 1 3&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;scp kube-apiserver kube-controller-manager kube-scheduler root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/usr/local/bin/
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;分发kubernetes-node&quot;&gt;分发kubernetes node&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;tar zxvf kubernetes-node-linux-amd64.tar.gz
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;kubernetes/node/bin

&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 5 7&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;scp kubelet kube-proxy root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/usr/local/bin/
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;分发kubectletcdctl&quot;&gt;分发kubectl,etcdctl&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;kubernetes/node/bin
cp kubectl /usr/local/bin/

&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;etcd-v3.2.7-linux-amd64
cp etcdctl /usr/bin/
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;四etcd集群部署&quot;&gt;四.etcd集群部署&lt;/h3&gt;

&lt;h5 id=&quot;添加etcd为系统服务&quot;&gt;添加etcd为系统服务&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个master节点上添加etcd启动文件/usr/lib/systemd/system/etcd.service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# etcd1&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd server
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network.target
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target
&lt;span class=&quot;nv&quot;&gt;Wants&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;notify
&lt;span class=&quot;nv&quot;&gt;User&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd
&lt;span class=&quot;nv&quot;&gt;WorkingDirectory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/lib/etcd/
&lt;span class=&quot;nv&quot;&gt;EnvironmentFile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/etc/etcd/etcd.conf
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/bin/etcd &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --name etcd1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --trusted-ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-trusted-ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-advertise-peer-urls https://172.29.151.1:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --listen-peer-urls https://172.29.151.1:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --listen-client-urls https://172.29.151.1:2379,https://127.0.0.1:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --advertise-client-urls https://172.29.151.1:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster-token k8s_etcd &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster &lt;span class=&quot;nv&quot;&gt;etcd1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.1:2380,etcd2&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.2:2380,etcd3&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.3:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster-state new &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --data-dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/lib/etcd
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;RestartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# etcd2&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd server
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network.target
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target
&lt;span class=&quot;nv&quot;&gt;Wants&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;notify
&lt;span class=&quot;nv&quot;&gt;User&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd
&lt;span class=&quot;nv&quot;&gt;WorkingDirectory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/lib/etcd/
&lt;span class=&quot;nv&quot;&gt;EnvironmentFile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/etc/etcd/etcd.conf
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/bin/etcd &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --name etcd2 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --trusted-ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-trusted-ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-advertise-peer-urls https://172.29.151.2:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --listen-peer-urls https://172.29.151.2:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --listen-client-urls https://172.29.151.2:2379,https://127.0.0.1:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --advertise-client-urls https://172.29.151.2:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster-token k8s_etcd &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster &lt;span class=&quot;nv&quot;&gt;etcd1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.1:2380,etcd2&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.2:2380,etcd3&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.3:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster-state new &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --data-dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/lib/etcd
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;RestartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# etcd3&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd server
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network.target
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target
&lt;span class=&quot;nv&quot;&gt;Wants&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network-online.target

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;notify
&lt;span class=&quot;nv&quot;&gt;User&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd
&lt;span class=&quot;nv&quot;&gt;WorkingDirectory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/lib/etcd/
&lt;span class=&quot;nv&quot;&gt;EnvironmentFile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/etc/etcd/etcd.conf
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/bin/etcd &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --name etcd3 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --trusted-ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --peer-trusted-ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-advertise-peer-urls https://172.29.151.3:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --listen-peer-urls https://172.29.151.3:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --listen-client-urls https://172.29.151.3:2379,https://127.0.0.1:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --advertise-client-urls https://172.29.151.3:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster-token k8s_etcd &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster &lt;span class=&quot;nv&quot;&gt;etcd1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.1:2380,etcd2&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.2:2380,etcd3&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.3:2380 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --initial-cluster-state new &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --data-dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/lib/etcd
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;RestartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;验证etcd-集群状态&quot;&gt;验证etcd 集群状态&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看etcd集群状态&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;etcdctl &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --endpoints&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.1:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  cluster-health

member 31a0c451ae46a2d0 is healthy: got healthy result from https://172.29.151.1:2379
member 72b18fe792c0a463 is healthy: got healthy result from https://172.29.151.3:2379
member d0c073403f6edbd3 is healthy: got healthy result from https://172.29.151.2:2379
cluster is healthy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看etcd 集群成员&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;etcdctl &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --endpoints&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.1:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  member list

31a0c451ae46a2d0: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd1 &lt;span class=&quot;nv&quot;&gt;peerURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.1:2380 &lt;span class=&quot;nv&quot;&gt;clientURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.1:2379 &lt;span class=&quot;nv&quot;&gt;isLeader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;72b18fe792c0a463: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd3 &lt;span class=&quot;nv&quot;&gt;peerURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.3:2380 &lt;span class=&quot;nv&quot;&gt;clientURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.3:2379 &lt;span class=&quot;nv&quot;&gt;isLeader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;d0c073403f6edbd3: &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd2 &lt;span class=&quot;nv&quot;&gt;peerURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.2:2380 &lt;span class=&quot;nv&quot;&gt;clientURLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.2:2379 &lt;span class=&quot;nv&quot;&gt;isLeader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;五kube-apiserver部署&quot;&gt;五.kube-apiserver部署&lt;/h3&gt;

&lt;h5 id=&quot;添加kube-apiserver为系统服务&quot;&gt;添加kube-apiserver为系统服务&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个master节点上添加/usr/lib/systemd/system/kube-apiserver.service，注意修改为各自节点的ip地址&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 创建日志目录文件&lt;/span&gt;
mkdir -p /var/log/kubernetes
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;kube-apiserver.service文件内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;注意：安全端口监听在172.29.151.1，提供给node节点访问，非安全端口监听在127.0.0.1，只提供给同一台机器上的kube-controller-manager和kube-scheduler访问，这样就保证了安全性和稳定性&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Kubernetes API Server
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network.target
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd.service

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;EnvironmentFile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/etc/kubernetes/apiserver
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/bin/kube-apiserver &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --admission-control&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --advertise-address&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;172.29.151.1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --bind-address&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;172.29.151.1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --insecure-bind-address&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;127.0.0.1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --service-cluster-ip-range&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10.254.0.0/16 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --service-node-port-range&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;30000-32000 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --allow-privileged&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --apiserver-count&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --logtostderr&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --v&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --audit-log-maxage&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;30 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --audit-log-maxbackup&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --audit-log-maxsize&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;100 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --audit-log-path&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/log/kubernetes/audit.log &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --authorization-mode&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;RBAC &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --enable-swagger-ui&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --event-ttl&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1h &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --secure-port&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6443 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --insecure-port&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8080 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --etcd-servers&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://172.29.151.1:2379,https://172.29.151.2:2379,https://172.29.151.3:2379 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --etcd-cafile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --etcd-certfile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --etcd-keyfile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/etcd/ssl/etcd-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --storage-backend&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;etcd3 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --tls-cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/kube-apiserver.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --tls-private-key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/kube-apiserver-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --client-ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --service-account-key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/kube-apiserver-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --token-auth-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/known_token/token.csv &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --experimental-bootstrap-token-auth&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubelet-https&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --anonymous-auth&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;False
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;notify
&lt;span class=&quot;nv&quot;&gt;LimitNOFILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;65536

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;启动kube-apiserver&quot;&gt;启动kube-apiserver&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;kube-apiserver
systemctl start kube-apiserver
systemctl status kube-apiserver
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;六kube-controller-manager部署&quot;&gt;六.kube-controller-manager部署&lt;/h3&gt;

&lt;h5 id=&quot;添加kube-controller-manager为系统服务&quot;&gt;添加kube-controller-manager为系统服务&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个master节点上添加/usr/lib/systemd/system/kube-controller-manager.service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Kubernetes Controller Manager
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network.target
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-apiserver.service

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/bin/kube-controller-manager &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --address&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;127.0.0.1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --master&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://127.0.0.1:8080 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --allocate-node-cidrs&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --service-cluster-ip-range&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10.254.0.0/16 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster-cidr&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10.233.0.0/16 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster-name&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster-signing-cert-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster-signing-key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/ca-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --service-account-private-key-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/kube-apiserver-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --root-ca-file&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --leader-elect&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --node-monitor-grace-period&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;40s &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --node-monitor-period&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5s &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --pod-eviction-timeout&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5m0s &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --v&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;RestartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;启动kube-controller-manager&quot;&gt;启动kube-controller-manager&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;kube-controller-manager
systemctl start kube-controller-manager
systemctl status kube-controller-manager
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;七kube-scheduler部署&quot;&gt;七.kube-scheduler部署&lt;/h3&gt;

&lt;h5 id=&quot;添加kube-scheduler为系统服务&quot;&gt;添加kube-scheduler为系统服务&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个master节点上添加/usr/lib/systemd/system/kube-scheduler.service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-scheduler
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network.target
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-apiserver.service

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;EnvironmentFile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/etc/kubernetes/scheduler
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/bin/kube-scheduler &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
      --address&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;127.0.0.1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	    --logtostderr&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	    --v&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	    --master&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;127.0.0.1:8080 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
	    --leader-elect&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;simple
&lt;span class=&quot;nv&quot;&gt;LimitNOFILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;65536

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;启动kube-scheduler&quot;&gt;启动kube-scheduler&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;kube-scheduler
systemctl start kube-scheduler
systemctl status kube-scheduler
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;八master-ha配置&quot;&gt;八.Master HA配置&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;目前所谓的kubernetes HA 其实主要是API Server的HA，master上的其他组件，比如kube-controller-manager、kube-scheduler都是通过etcd做选举。而API Server一般有两种方式做HA；一种是多个API Server 做聚合为 VIP，另一种使用nginx反向代理，这里我们采用nginx的方式，如下图&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/apiserver-ha.png&quot; alt=&quot;apiserver-ha&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;kube-controller-manager、kube-scheduler通过etcd选举，而且与master直接通过127.0.0.1:8080通信，而其他node，则需要在每个node上启动一个nginx，每个nginx反代所有apiserver，node上的kubelet、kube-proxy、kubectl连接本地nginx代理端口，当nginx发现无法连接后端时会自动踢掉出问题的apiserver，从而实现api server的HA&lt;/strong&gt;&lt;/p&gt;

&lt;h5 id=&quot;在每个node节点和k8s-console上创建nginx代理&quot;&gt;在每个node节点和k8s-console上创建nginx代理&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个节点上新建配置目录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir -p /etc/nginx
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;在配置文件/etc/nginx/nginx.conf中下写入代理配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;error_log stderr notice;

worker_processes auto;
events &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  multi_accept on;
  use epoll;
  worker_connections 1024;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

stream &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    upstream kube_apiserver &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        least_conn;
        server 172.29.151.1:6443;
        server 172.29.151.2:6443;
        server 172.29.151.3:6443;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

    server &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        listen        0.0.0.0:6443;
        proxy_pass    kube_apiserver;
        proxy_timeout 10m;
        proxy_connect_timeout 1s;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;更新权限&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;chmod +r /etc/nginx/nginx.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;将nginx配置为docker启动同时用systemd来进行守护&quot;&gt;将nginx配置为docker启动，同时用systemd来进行守护&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个node节点上添加/etc/systemd/system/nginx-proxy.service&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes apiserver docker wrapper
&lt;span class=&quot;nv&quot;&gt;Wants&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker.socket
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker.service

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;User&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;root
&lt;span class=&quot;nv&quot;&gt;PermissionsStartOnly&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/bin/docker run -p 127.0.0.1:6443:6443 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
                              -v /etc/nginx:/etc/nginx &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
                              --name nginx-proxy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
                              --net&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;host &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
                              --restart&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure:5 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
                              --memory&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;512M &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
                              nginx:1.13.3-alpine
&lt;span class=&quot;nv&quot;&gt;ExecStartPre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/usr/bin/docker rm -f nginx-proxy
&lt;span class=&quot;nv&quot;&gt;ExecStop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/bin/docker stop nginx-proxy
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;always
&lt;span class=&quot;nv&quot;&gt;RestartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;15s
&lt;span class=&quot;nv&quot;&gt;TimeoutStartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;30s

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;配置开机启动&quot;&gt;配置开机启动&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl start nginx-proxy
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;nginx-proxy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;最后我们在k8s-console上执行kubectl试试&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl --server&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://127.0.0.1:6443 --certificate-authority&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/ca.pem --client-certificate&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/kube-admin.pem --client-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl/kube-admin-key.pem get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-1               Healthy   &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;health&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;   
etcd-0               Healthy   &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;health&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;   
etcd-2               Healthy   &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;health&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;   
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;九-配置kubectl访问apiserver&quot;&gt;九. 配置kubectl访问apiserver&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;前面我们使用kubect打印除了kubernetes核心组件的状态，但是每次使用的时候都需要指定apiserver的地址以及证书之类的，实在是有点繁琐，接下来，我们在k8s-console上创建kubeconfig文件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /etc/kubernetes
&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBE_APISERVER&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://127.0.0.1:6443&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 设置集群参数&lt;/span&gt;
kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-cluster kubernetes &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 --certificate-authority&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 --embed-certs&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 --server&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;KUBE_APISERVER&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin.conf

&lt;span class=&quot;c&quot;&gt;# 设置客户端认证参数&lt;/span&gt;
kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-credentials kubernetes-admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --client-certificate&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/kube-admin.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --embed-certs&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --client-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/kube-admin-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin.conf

&lt;span class=&quot;c&quot;&gt;# 设置上下文参数&lt;/span&gt;
kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-context kubernetes-admin@kubernetes &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --user&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes-admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin.conf

&lt;span class=&quot;c&quot;&gt;# 设置默认上下文&lt;/span&gt;
kubectl config use-context kubernetes-admin@kubernetes --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin.conf

&lt;span class=&quot;c&quot;&gt;# cp成~/.kube/config&lt;/span&gt;
cp /etc/kubernetes/ssl/admin.conf ~/.kube/config
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;试试看是否生效&quot;&gt;试试看是否生效&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
scheduler            Healthy   ok                   
controller-manager   Healthy   ok                   
etcd-2               Healthy   &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;health&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;   
etcd-1               Healthy   &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;health&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;   
etcd-0               Healthy   &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;health&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;   
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;十kubelet配置&quot;&gt;十.kubelet配置&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;kubelet启动时向kube-apiserver发送 TLS bootstrapping请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap用户赋予system:node-bootstrapper角色，然后kubelet才有权限创建认证请求。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h5 id=&quot;kubelet角色授权&quot;&gt;kubelet角色授权&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 在k8s-console上执行绑定操作&lt;/span&gt;
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;system:node-bootstrapper --user&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubelet-bootstrap
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;在k8s-console上生成kubelet-kubeconfig文件&quot;&gt;在k8s-console上生成kubelet kubeconfig文件&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;配置集群&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-cluster kubernetes &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --certificate-authority&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --embed-certs&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --server&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://127.0.0.1:6443 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;配置客户端认证参数&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-credentials kubelet-bootstrap &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --token&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;04d9b6c6fd3ed8a3488b3b0913e87d64 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;配置上下文关联&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-context default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --user&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubelet-bootstrap &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;配置默认上下文&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl config use-context default --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;bootstrap.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;分发bootstrap.kubeconfig文件到每个node节点&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /etc/kubernetes
&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 5 7&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;scp bootstrap.kubeconfig root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/etc/kubernetes/
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;添加kubelet为系统服务&quot;&gt;添加kubelet为系统服务&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;创建kubelet工作目录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir /var/lib/kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;添加/usr/lib/systemd/system/kubelet.service,注意修改你成你自己节点的ip&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Kubernetes Kubelet
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker.service
&lt;span class=&quot;nv&quot;&gt;Requires&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker.service

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;WorkingDirectory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/lib/kubelet
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/bin/kubelet &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cgroup-driver&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;systemd &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --address&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;172.29.151.5 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --hostname-override&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;172.29.151.5 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --pod-infra-container-image&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;gcr.io/google_containers/pause-amd64:3.0 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --experimental-bootstrap-kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/bootstrap.kubeconfig &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/kubelet.kubeconfig &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --require-kubeconfig &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cert-dir&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/ssl &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster_dns&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10.254.0.2 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster_domain&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cluster.local. &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --hairpin-mode promiscuous-bridge &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --allow-privileged&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --serialize-image-pulls&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --logtostderr&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --max-pods&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;512 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --v&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;RestartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;启动kubelet&quot;&gt;启动kubelet&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;kubelet
systemctl start kubelet
systemctl status kubelet
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;签发证书验证nodes&quot;&gt;签发证书，验证nodes&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看csr，我们发现状态为Pending&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get csr
NAME                                                   AGE       REQUESTOR           CONDITION
node-csr-MIqovZHmrYMe1Y6AspcfU6_keLdSWfbUqg4pcK-Hb9w   2m        kubelet-bootstrap   Pending
node-csr-el6foG3yw6_9xCu1vC_upuT-xLR9Z9ASBNj5isBFcsY   2m        kubelet-bootstrap   Pending
node-csr-oPWmprgtrRixLZXUvEFKnHI2qZEGorzHKZ1ktLMdGS8   5m        kubelet-bootstrap   Pending
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;签发证书&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl certificate approve node-csr-oPWmprgtrRixLZXUvEFKnHI2qZEGorzHKZ1ktLMdGS8 node-csr-el6foG3yw6_9xCu1vC_upuT-xLR9Z9ASBNj5isBFcsY node-csr-MIqovZHmrYMe1Y6AspcfU6_keLdSWfbUqg4pcK-Hb9w
certificatesigningrequest &lt;span class=&quot;s2&quot;&gt;&quot;node-csr-oPWmprgtrRixLZXUvEFKnHI2qZEGorzHKZ1ktLMdGS8&quot;&lt;/span&gt; approved
certificatesigningrequest &lt;span class=&quot;s2&quot;&gt;&quot;node-csr-el6foG3yw6_9xCu1vC_upuT-xLR9Z9ASBNj5isBFcsY&quot;&lt;/span&gt; approved
certificatesigningrequest &lt;span class=&quot;s2&quot;&gt;&quot;node-csr-MIqovZHmrYMe1Y6AspcfU6_keLdSWfbUqg4pcK-Hb9w&quot;&lt;/span&gt; approved
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看node&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get nodes
NAME           STATUS    AGE       VERSION
172.29.151.5   Ready     3m        v1.7.6
172.29.151.6   Ready     45s       v1.7.6
172.29.151.7   Ready     12s       v1.7.6
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;成功后会自动生成配置文件和密钥&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ll /etc/kubernetes/ssl
-rw-r--r-- 1 root root 1042 Oct  9 17:46 kubelet-client.crt
-rw------- 1 root root  227 Oct  9 17:18 kubelet-client.key
-rw-r--r-- 1 root root 1111 Oct  9 17:46 kubelet.crt
-rw------- 1 root root 1675 Oct  9 17:46 kubelet.key


&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ll /etc/kubernetes/kubelet.kubeconfig
-rw------- 1 root root 2260 Oct  9 17:46 /etc/kubernetes/kubelet.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;十一kube-proxy-配置&quot;&gt;十一.kube-proxy 配置&lt;/h3&gt;

&lt;h5 id=&quot;在k8s-console上生成kube-proxy-kubeconfig文件&quot;&gt;在k8s-console上生成kube-proxy kubeconfig文件&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;配置集群&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-cluster kubernetes &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --certificate-authority&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/ca.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --embed-certs&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --server&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://127.0.0.1:6443 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;配置客户端认证&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-credentials kube-proxy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --client-certificate&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/kube-proxy.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --client-key&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/opt/ssl/kube-proxy-key.pem &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --embed-certs&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;配置上下文&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl config &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;-context default &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kubernetes &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --user&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-proxy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;配置默认上下文&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl config use-context default --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;分发到各个节点的/etc/kubernetes 目录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 5 7&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;scp kube-proxy.kubeconfig root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/etc/kubernetes/
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;添加kube-proxy为系统服务&quot;&gt;添加kube-proxy为系统服务&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;创建 kube-proxy目录&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir -p /var/lib/kube-proxy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;添加/usr/lib/systemd/system/kube-proxy.service，注意修改为自己的节点ip&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Kubernetes Kube-Proxy Server
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;network.target

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;WorkingDirectory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/lib/kube-proxy
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/bin/kube-proxy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --bind-address&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;172.29.151.5 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --hostname-override&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;172.29.151.5 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --cluster-cidr&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10.254.0.0/16 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --kubeconfig&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/kubernetes/kube-proxy.kubeconfig &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --logtostderr&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  --v&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2
&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;on-failure
&lt;span class=&quot;nv&quot;&gt;RestartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5
&lt;span class=&quot;nv&quot;&gt;LimitNOFILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;65536

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;启动kube-proxy&quot;&gt;启动kube-proxy&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;kube-proxy
systemctl start kube-proxy
systemctl status kube-proxy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;十二calico配置&quot;&gt;十二.calico配置&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;网络组件采用calico，calico部署比较简单，只需要create 一下yml文件即可，具体参考&lt;a href=&quot;https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/&quot;&gt;calico官方文档&lt;/a&gt; ，在使用calico网络的时候，官方的要求如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;kubelet 必须配置使用CNI插件&lt;code class=&quot;highlighter-rouge&quot;&gt;--network-plugin=cni&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;kube-proxy 必须以&lt;code class=&quot;highlighter-rouge&quot;&gt;iptables&lt;/code&gt;的模式启动&lt;/li&gt;
  &lt;li&gt;kube-proxy 不能使用&lt;code class=&quot;highlighter-rouge&quot;&gt;--masquerade-all&lt;/code&gt;启动(会与calico policy冲突)&lt;/li&gt;
  &lt;li&gt;kubernetes networkpolicy api 至少需要kubernetes 1.3 版本以上&lt;/li&gt;
  &lt;li&gt;如果开启了RBAC，那么需要注意需要创建clusterrole和clusterrolebinding&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;在每个节点上修改kubeletservice&quot;&gt;在每个节点上修改kubelet.service&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;修改kubelet配置，增加&lt;code class=&quot;highlighter-rouge&quot;&gt;--network-plugin=cni&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi /usr/lib/systemd/system/kubelet.service

--network-plugin&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cni
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;重启kubelet&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl restart kubelet.service
systemctl status kubelet.service
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;准备依赖包和文件&quot;&gt;准备依赖包和文件&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;下载calico.yaml 和rbac.yaml&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/calico.yaml
wget https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/rbac.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;下载镜像&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;quay.io/calico/node:v2.6.1
quay.io/calico/cni:v1.11.0
quay.io/calico/kube-controllers:v1.0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;修改配置文件&quot;&gt;修改配置文件&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;修改etcd_endpoints&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;etcd_endpoints: &lt;span class=&quot;s2&quot;&gt;&quot;https://172.29.151.1:2379,https://172.29.151.2:2379,https://172.29.151.3:2379&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;修改calico所需的etcd密钥信息&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;etcd_ca: &lt;span class=&quot;s2&quot;&gt;&quot;/calico-secrets/etcd-ca&quot;&lt;/span&gt;
etcd_cert: &lt;span class=&quot;s2&quot;&gt;&quot;/calico-secrets/etcd-cert&quot;&lt;/span&gt;
etcd_key: &lt;span class=&quot;s2&quot;&gt;&quot;/calico-secrets/etcd-key&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;写入etcd-key、etcd-cert、etcd-ca的base64信息，将括号里面命令执行的结果填入即可&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data:
  etcd-key: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;cat /opt/ssl/etcd-key.pem | base64 | tr -d &lt;span class=&quot;s1&quot;&gt;'\n'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  etcd-cert: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;cat /opt/ssl/etcd.pem | base64 | tr -d &lt;span class=&quot;s1&quot;&gt;'\n'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
  etcd-ca: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;cat /opt/ssl/ca.pem | base64 | tr -d &lt;span class=&quot;s1&quot;&gt;'\n'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;修改calico的网络段&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- name: CALICO_IPV4POOL_CIDR
      value: &lt;span class=&quot;s2&quot;&gt;&quot;10.233.0.0/16&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;注释掉calico-node 部分，这部分用systemctl来进行管理，因为用官方文档可能会出现无法获取到IP的情况&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Calico Version v2.6.1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# https://docs.projectcalico.org/v2.6/releases#v2.6.1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This manifest includes the following component versions:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   calico/node:v2.6.1&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   calico/cni:v1.11.0&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#   calico/kube-controllers:v1.0.0&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# This ConfigMap is used to configure a self-hosted Calico installation.&lt;/span&gt;
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  &lt;span class=&quot;c&quot;&gt;# Configure this with the location of your etcd cluster.&lt;/span&gt;
  etcd_endpoints: &lt;span class=&quot;s2&quot;&gt;&quot;https://172.29.151.1:2379,https://172.29.151.2:2379,https://172.29.151.3:2379&quot;&lt;/span&gt;

  &lt;span class=&quot;c&quot;&gt;# Configure the Calico backend to use.&lt;/span&gt;
  calico_backend: &lt;span class=&quot;s2&quot;&gt;&quot;bird&quot;&lt;/span&gt;

  &lt;span class=&quot;c&quot;&gt;# The CNI network configuration to install on each node.&lt;/span&gt;
  cni_network_config: |-
    &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;k8s-pod-network&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;cniVersion&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;0.1.0&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;calico&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;etcd_endpoints&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;__ETCD_ENDPOINTS__&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;etcd_key_file&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;__ETCD_KEY_FILE__&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;etcd_cert_file&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;__ETCD_CERT_FILE__&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;etcd_ca_cert_file&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;__ETCD_CA_CERT_FILE__&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;log_level&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;info&quot;&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;mtu&quot;&lt;/span&gt;: 1500,
        &lt;span class=&quot;s2&quot;&gt;&quot;ipam&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;calico-ipam&quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;policy&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;k8s&quot;&lt;/span&gt;,
            &lt;span class=&quot;s2&quot;&gt;&quot;k8s_api_root&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__&quot;&lt;/span&gt;,
            &lt;span class=&quot;s2&quot;&gt;&quot;k8s_auth_token&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;__SERVICEACCOUNT_TOKEN__&quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
        &lt;span class=&quot;s2&quot;&gt;&quot;kubernetes&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s2&quot;&gt;&quot;kubeconfig&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;__KUBECONFIG_FILEPATH__&quot;&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;c&quot;&gt;# If you're using TLS enabled etcd uncomment the following.&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# You must also populate the Secret below with these files.&lt;/span&gt;
  etcd_ca: &lt;span class=&quot;s2&quot;&gt;&quot;/calico-secrets/etcd-ca&quot;&lt;/span&gt;
  etcd_cert: &lt;span class=&quot;s2&quot;&gt;&quot;/calico-secrets/etcd-cert&quot;&lt;/span&gt;
  etcd_key: &lt;span class=&quot;s2&quot;&gt;&quot;/calico-secrets/etcd-key&quot;&lt;/span&gt;

---

&lt;span class=&quot;c&quot;&gt;# The following contains k8s Secrets for use with a TLS enabled etcd cluster.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/&lt;/span&gt;
apiVersion: v1
kind: Secret
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: Opaque
metadata:
  name: calico-etcd-secrets
  namespace: kube-system
data:
  &lt;span class=&quot;c&quot;&gt;# Populate the following files with etcd TLS configuration if desired, but leave blank if&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# not using TLS for etcd.&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# This self-hosted install expects three files with the following names.  The values&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# should be base64 encoded strings of the entire contents of each file.&lt;/span&gt;
  etcd-key: 这块自己对 etcd 相关证书做 base64
  etcd-cert: 这块自己对 etcd 相关证书做 base64
  etcd-ca: 这块自己对 etcd 相关证书做 base64

---

&lt;span class=&quot;c&quot;&gt;# This manifest installs the calico/node container, as well&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# as the Calico CNI plugins and network config on&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# each master and worker node in a Kubernetes cluster.&lt;/span&gt;
kind: DaemonSet
apiVersion: extensions/v1beta1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
  template:
    metadata:
      labels:
        k8s-app: calico-node
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt;
        scheduler.alpha.kubernetes.io/tolerations: |
          &lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;key&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;dedicated&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;master&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;effect&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;NoSchedule&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
           &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;key&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;CriticalAddonsOnly&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;operator&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Exists&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
    spec:
      hostNetwork: &lt;span class=&quot;nb&quot;&gt;true
      &lt;/span&gt;serviceAccountName: calico-node
      containers:
        &lt;span class=&quot;c&quot;&gt;# Runs calico/node container on each Kubernetes node.  This&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# container programs network policy and routes on each&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# host.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 从这里开始注释掉calico-node的部分&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# - name: calico-node&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#   image: quay.io/calico/node:v2.6.1&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#   env:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # The location of the Calico etcd cluster.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: ETCD_ENDPOINTS&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       valueFrom:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#         configMapKeyRef:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           name: calico-config&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           key: etcd_endpoints&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Choose the backend to use.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: CALICO_NETWORKING_BACKEND&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       valueFrom:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#         configMapKeyRef:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           name: calico-config&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           key: calico_backend&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Cluster type to identify the deployment type&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: CLUSTER_TYPE&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;k8s,bgp&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Disable file logging so `kubectl logs` works.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: CALICO_DISABLE_FILE_LOGGING&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;true&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Set Felix endpoint to host default action to ACCEPT.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: FELIX_DEFAULTENDPOINTTOHOSTACTION&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;ACCEPT&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Configure the IP Pool from which Pod IPs will be chosen.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: CALICO_IPV4POOL_CIDR&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;10.233.0.0/16&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: CALICO_IPV4POOL_IPIP&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;always&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Disable IPv6 on Kubernetes.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: FELIX_IPV6SUPPORT&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;false&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Set Felix logging to &quot;info&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: FELIX_LOGSEVERITYSCREEN&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;info&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Set MTU for tunnel device used if ipip is enabled&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: FELIX_IPINIPMTU&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;1440&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Location of the CA certificate for etcd.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: ETCD_CA_CERT_FILE&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       valueFrom:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#         configMapKeyRef:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           name: calico-config&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           key: etcd_ca&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Location of the client key for etcd.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: ETCD_KEY_FILE&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       valueFrom:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#         configMapKeyRef:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           name: calico-config&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           key: etcd_key&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Location of the client certificate for etcd.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: ETCD_CERT_FILE&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       valueFrom:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#         configMapKeyRef:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           name: calico-config&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#           key: etcd_cert&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     # Auto-detect the BGP IP address.&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: IP&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - name: FELIX_HEALTHENABLED&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       value: &quot;true&quot;&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#   securityContext:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     privileged: true&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#   resources:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     requests:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       cpu: 250m&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#   livenessProbe:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     httpGet:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       path: /liveness&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       port: 9099&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     periodSeconds: 10&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     initialDelaySeconds: 10&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     failureThreshold: 6&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#   readinessProbe:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     httpGet:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       path: /readiness&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       port: 9099&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     periodSeconds: 10&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#   volumeMounts:&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - mountPath: /lib/modules&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       name: lib-modules&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       readOnly: true&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - mountPath: /var/run/calico&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       name: var-run-calico&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       readOnly: false&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#     - mountPath: /calico-secrets&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#       name: etcd-certs&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# This container installs the Calico CNI binaries&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# and CNI network config file on each node.&lt;/span&gt;
        - name: install-cni
          image: quay.io/calico/cni:v1.11.0
          &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/install-cni.sh&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
          env:
            &lt;span class=&quot;c&quot;&gt;# The location of the Calico etcd cluster.&lt;/span&gt;
            - name: ETCD_ENDPOINTS
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_endpoints
            &lt;span class=&quot;c&quot;&gt;# The CNI network config to install on each node.&lt;/span&gt;
            - name: CNI_NETWORK_CONFIG
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: cni_network_config
          volumeMounts:
            - mountPath: /host/opt/cni/bin
              name: cni-bin-dir
            - mountPath: /host/etc/cni/net.d
              name: cni-net-dir
            - mountPath: /calico-secrets
              name: etcd-certs
      volumes:
        &lt;span class=&quot;c&quot;&gt;# Used by calico/node.&lt;/span&gt;
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: var-run-calico
          hostPath:
            path: /var/run/calico
        &lt;span class=&quot;c&quot;&gt;# Used to install CNI.&lt;/span&gt;
        - name: cni-bin-dir
          hostPath:
            path: /opt/cni/bin
        - name: cni-net-dir
          hostPath:
            path: /etc/cni/net.d
        &lt;span class=&quot;c&quot;&gt;# Mount in the etcd TLS secrets.&lt;/span&gt;
        - name: etcd-certs
          secret:
            secretName: calico-etcd-secrets

---

&lt;span class=&quot;c&quot;&gt;# This manifest deploys the Calico Kubernetes controllers.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# See https://github.com/projectcalico/kube-controllers&lt;/span&gt;
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: calico-kube-controllers
  namespace: kube-system
  labels:
    k8s-app: calico-kube-controllers
  annotations:
    scheduler.alpha.kubernetes.io/critical-pod: &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt;
    scheduler.alpha.kubernetes.io/tolerations: |
      &lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;key&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;dedicated&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;value&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;master&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;effect&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;NoSchedule&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
       &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;key&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;CriticalAddonsOnly&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;operator&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Exists&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
spec:
  &lt;span class=&quot;c&quot;&gt;# The controllers can only have a single active instance.&lt;/span&gt;
  replicas: 1
  strategy:
    &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: Recreate
  template:
    metadata:
      name: calico-kube-controllers
      namespace: kube-system
      labels:
        k8s-app: calico-kube-controllers
    spec:
      &lt;span class=&quot;c&quot;&gt;# The controllers must run in the host network namespace so that&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# it isn't governed by policy that would prevent it from working.&lt;/span&gt;
      hostNetwork: &lt;span class=&quot;nb&quot;&gt;true
      &lt;/span&gt;serviceAccountName: calico-kube-controllers
      containers:
        - name: calico-kube-controllers
          image: quay.io/calico/kube-controllers:v1.0.0
          env:
            &lt;span class=&quot;c&quot;&gt;# The location of the Calico etcd cluster.&lt;/span&gt;
            - name: ETCD_ENDPOINTS
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_endpoints
            &lt;span class=&quot;c&quot;&gt;# Location of the CA certificate for etcd.&lt;/span&gt;
            - name: ETCD_CA_CERT_FILE
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_ca
            &lt;span class=&quot;c&quot;&gt;# Location of the client key for etcd.&lt;/span&gt;
            - name: ETCD_KEY_FILE
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_key
            &lt;span class=&quot;c&quot;&gt;# Location of the client certificate for etcd.&lt;/span&gt;
            - name: ETCD_CERT_FILE
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_cert
          volumeMounts:
            &lt;span class=&quot;c&quot;&gt;# Mount in the etcd TLS secrets.&lt;/span&gt;
            - mountPath: /calico-secrets
              name: etcd-certs
      volumes:
        &lt;span class=&quot;c&quot;&gt;# Mount in the etcd TLS secrets.&lt;/span&gt;
        - name: etcd-certs
          secret:
            secretName: calico-etcd-secrets

---

&lt;span class=&quot;c&quot;&gt;# This deployment turns off the old &quot;policy-controller&quot;. It should remain at 0 replicas, and then&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# be removed entirely once the new kube-controllers deployment has been deployed above.&lt;/span&gt;
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: calico-policy-controller
  namespace: kube-system
  labels:
    k8s-app: calico-policy
spec:
  &lt;span class=&quot;c&quot;&gt;# Turn this deployment off in favor of the kube-controllers deployment above.&lt;/span&gt;
  replicas: 0
  strategy:
    &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: Recreate
  template:
    metadata:
      name: calico-policy-controller
      namespace: kube-system
      labels:
        k8s-app: calico-policy
    spec:
      hostNetwork: &lt;span class=&quot;nb&quot;&gt;true
      &lt;/span&gt;serviceAccountName: calico-kube-controllers
      containers:
        - name: calico-policy-controller
          image: quay.io/calico/kube-controllers:v1.0.0
          env:
            &lt;span class=&quot;c&quot;&gt;# The location of the Calico etcd cluster.&lt;/span&gt;
            - name: ETCD_ENDPOINTS
              valueFrom:
                configMapKeyRef:
                  name: calico-config
                  key: etcd_endpoints

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-kube-controllers
  namespace: kube-system

---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-node
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;安装calico&quot;&gt;安装calico&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个node节点上创建calico所需的目录，并分发证书&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /opt/ssl
&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 5 7&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;ssh root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt; mkdir -p /etc/calico/certs
  scp ca.pem etcd&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.pem root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/etc/calico/certs/
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个node节点上添加/etc/calico/calico.env文件,注意修改你自己的ip和hostname&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;ETCD_ENDPOINTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;https://172.29.151.1:2379,https://172.29.151.2:2379,https://172.29.151.3:2379&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_CA_CERT_FILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/etc/calico/certs/ca.pem&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_CERT_FILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/etc/calico/certs/etcd.pem&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;ETCD_KEY_FILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/etc/calico/certs/etcd-key.pem&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CALICO_IP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;172.29.151.5&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CALICO_IP6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CALICO_LIBNETWORK_ENABLED&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CALICO_IPV4POOL_CIDR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;10.233.0.0/16&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CALICO_IPV4POOL_IPIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;always&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CALICO_HOSTNAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;k8s-mds-node01&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;在每个node节点上添加calico.service为系统服务&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Unit]
&lt;span class=&quot;nv&quot;&gt;Description&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;calico-node
&lt;span class=&quot;nv&quot;&gt;After&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker.service
&lt;span class=&quot;nv&quot;&gt;Requires&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker.service

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Service]
&lt;span class=&quot;nv&quot;&gt;EnvironmentFile&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/calico/calico.env
&lt;span class=&quot;nv&quot;&gt;ExecStartPre&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/usr/bin/docker rm -f calico-node
&lt;span class=&quot;nv&quot;&gt;ExecStart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/bin/docker run --net&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;host --privileged &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 --name&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;calico-node &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;ETCD_ENDPOINTS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ETCD_ENDPOINTS&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;ETCD_CA_CERT_FILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ETCD_CA_CERT_FILE&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;ETCD_CERT_FILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ETCD_CERT_FILE&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;ETCD_KEY_FILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;ETCD_KEY_FILE&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;HOSTNAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_HOSTNAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;IP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_IP&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;IP6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_IP6&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;CALICO_NETWORKING_BACKEND&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_NETWORKING_BACKEND&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;CALICO_LIBNETWORK_ENABLED&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_LIBNETWORK_ENABLED&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;CALICO_IPV4POOL_CIDR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_IPV4POOL_CIDR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;CALICO_IPV4POOL_IPIP&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_IPV4POOL_IPIP&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;CALICO_DISABLE_FILE_LOGGING&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_DISABLE_FILE_LOGGING&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;FELIX_DEFAULTENDPOINTTOHOSTACTION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;RETURN &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;FELIX_IPV6SUPPORT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;FELIX_LOGSEVERITYSCREEN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;info &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -e &lt;span class=&quot;nv&quot;&gt;AS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CALICO_AS&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -v /var/log/calico:/var/log/calico &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -v /run/docker/plugins:/run/docker/plugins &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -v /lib/modules:/lib/modules &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -v /var/run/calico:/var/run/calico &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -v /var/run/docker.sock:/var/run/docker.sock &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 -v /etc/calico/certs:/etc/calico/certs:ro &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 --memory&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;500M --cpu-shares&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;300 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 quay.io/calico/node:v2.6.1

&lt;span class=&quot;nv&quot;&gt;Restart&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;always
&lt;span class=&quot;nv&quot;&gt;RestartSec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10s

&lt;span class=&quot;nv&quot;&gt;ExecStop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;-/usr/bin/docker stop calico-node

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Install]
&lt;span class=&quot;nv&quot;&gt;WantedBy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;multi-user.target
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;启动calico-node&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
systemctl &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;calico-node
systemctl start calico-node
systemctl status calico-node
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;在k8s-console上执行calico安装&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl apply -f calico.yaml
configmap &lt;span class=&quot;s2&quot;&gt;&quot;calico-config&quot;&lt;/span&gt; created
secret &lt;span class=&quot;s2&quot;&gt;&quot;calico-etcd-secrets&quot;&lt;/span&gt; created
daemonset &lt;span class=&quot;s2&quot;&gt;&quot;calico-node&quot;&lt;/span&gt; created
deployment &lt;span class=&quot;s2&quot;&gt;&quot;calico-policy-controller&quot;&lt;/span&gt; created
serviceaccount &lt;span class=&quot;s2&quot;&gt;&quot;calico-policy-controller&quot;&lt;/span&gt; created
serviceaccount &lt;span class=&quot;s2&quot;&gt;&quot;calico-node&quot;&lt;/span&gt; created


&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl apply -f rbac.yaml
clusterrole &lt;span class=&quot;s2&quot;&gt;&quot;calico-policy-controller&quot;&lt;/span&gt; created
clusterrolebinding &lt;span class=&quot;s2&quot;&gt;&quot;calico-policy-controller&quot;&lt;/span&gt; created
clusterrole &lt;span class=&quot;s2&quot;&gt;&quot;calico-node&quot;&lt;/span&gt; created
clusterrolebinding &lt;span class=&quot;s2&quot;&gt;&quot;calico-node&quot;&lt;/span&gt; created
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;验证calico&quot;&gt;验证calico&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get pods -n kube-system
NAME                                       READY     STATUS    RESTARTS   AGE
calico-kube-controllers-3994748863-0dpcp   1/1       Running   0          1h
calico-node-74d64                          1/1       Running   0          14h
calico-node-rbrw3                          1/1       Running   0          14h
calico-node-vtcrs                          1/1       Running   0          14h
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;安装calicoctl&quot;&gt;安装calicoctl&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;在k8s-console上下载calicoctl并分发到各个node节点&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://github.com/projectcalico/calicoctl/releases/download/v1.6.1/calicoctl
chmod +x calicoctl

&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;IP &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;seq 5 7&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;;&lt;span class=&quot;k&quot;&gt;do
  &lt;/span&gt;scp calicoctl root@172.29.151.&lt;span class=&quot;nv&quot;&gt;$IP&lt;/span&gt;:/usr/local/bin/
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;在节点上看看calico的状态&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;calicoctl node status
Calico process is running.

IPv4 BGP status
+--------------+-------------------+-------+----------+-------------+
| PEER ADDRESS |     PEER TYPE     | STATE |  SINCE   |    INFO     |
+--------------+-------------------+-------+----------+-------------+
| 172.29.151.6 | node-to-node mesh | up    | 12:40:50 | Established |
| 172.29.151.7 | node-to-node mesh | up    | 12:40:50 | Established |
+--------------+-------------------+-------+----------+-------------+

IPv6 BGP status
No IPv6 peers found.

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;测试跨主机通信&quot;&gt;测试跨主机通信&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;创建一个nginxdeployment&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-dm&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx:alpine&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;imagePullPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;IfNotPresent&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;

&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;s&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx-svc&lt;/span&gt;
&lt;span class=&quot;s&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看创建结果&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get pods -o wide
NAME                        READY     STATUS    RESTARTS   AGE       IP             NODE
nginx-dm-2214564181-bplwr   1/1       Running   0          3m        10.233.136.3   172.29.151.7
nginx-dm-2214564181-qsl5c   1/1       Running   0          3m        10.233.203.2   172.29.151.6

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get deployment
NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
nginx-dm   2         2         2            2           4m

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get svc
NAME         CLUSTER-IP       EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;   AGE
kubernetes   10.254.0.1       &amp;lt;none&amp;gt;        443/TCP   14h
nginx-svc    10.254.149.124   &amp;lt;none&amp;gt;        80/TCP    4m
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;在pod里ping另一个pod&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -ti nginx-dm-2214564181-bplwr /bin/sh
/ &lt;span class=&quot;c&quot;&gt;# ping 10.233.203.2&lt;/span&gt;
PING 10.233.203.2 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;10.233.203.2&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;: 56 data bytes
64 bytes from 10.233.203.2: &lt;span class=&quot;nv&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &lt;span class=&quot;nv&quot;&gt;ttl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;62 &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.592 ms
64 bytes from 10.233.203.2: &lt;span class=&quot;nv&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1 &lt;span class=&quot;nv&quot;&gt;ttl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;62 &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.894 ms
64 bytes from 10.233.203.2: &lt;span class=&quot;nv&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2 &lt;span class=&quot;nv&quot;&gt;ttl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;62 &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.559 ms
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;在node节点上curl测试一下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;curl 10.254.149.124
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a &lt;span class=&quot;nv&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://nginx.org/&quot;&lt;/span&gt;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a &lt;span class=&quot;nv&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://nginx.com/&quot;&lt;/span&gt;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;十三部署dns&quot;&gt;十三.部署DNS&lt;/h3&gt;

&lt;h5 id=&quot;部署集群dns&quot;&gt;部署集群dns&lt;/h5&gt;
&lt;blockquote&gt;
  &lt;p&gt;获取对应的yaml文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kube-dns.yaml.sed
mv kube-dns.yaml.sed kube-dns.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;修改如下配置&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sed -i &lt;span class=&quot;s1&quot;&gt;'s/$DNS_DOMAIN/cluster.local/gi'&lt;/span&gt; kube-dns.yaml
sed -i &lt;span class=&quot;s1&quot;&gt;'s/$DNS_SERVER_IP/10.254.0.2/gi'&lt;/span&gt; kube-dns.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f kube-dns.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看创建结果&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get pods -n kube-system |grep kube-dns
kube-dns-3468831164-2kl0h                  3/3       Running   0          14m
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;进入刚刚创建的nginx pod中访问nginx-svc测试&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -ti nginx-dm-2214564181-bplwr /bin/sh
/ &lt;span class=&quot;c&quot;&gt;# curl nginx-svc&lt;/span&gt;
&amp;lt;!DOCTYPE html&amp;gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;
&amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt;
&amp;lt;style&amp;gt;
    body &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&amp;lt;/style&amp;gt;
&amp;lt;/head&amp;gt;
&amp;lt;body&amp;gt;
&amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt;
&amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;For online documentation and support please refer to
&amp;lt;a &lt;span class=&quot;nv&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://nginx.org/&quot;&lt;/span&gt;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt;
Commercial support is available at
&amp;lt;a &lt;span class=&quot;nv&quot;&gt;href&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;http://nginx.com/&quot;&lt;/span&gt;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt;

&amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;测试外网&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -ti nginx-dm-2214564181-bplwr /bin/sh
/ &lt;span class=&quot;c&quot;&gt;# curl https://baidu.com&lt;/span&gt;
&amp;lt;html&amp;gt;
&amp;lt;head&amp;gt;&amp;lt;title&amp;gt;302 Found&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt;
&amp;lt;body &lt;span class=&quot;nv&quot;&gt;bgcolor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;white&quot;&lt;/span&gt;&amp;gt;
&amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;302 Found&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt;
&amp;lt;hr&amp;gt;&amp;lt;center&amp;gt;bfe/1.0.8.18&amp;lt;/center&amp;gt;
&amp;lt;/body&amp;gt;
&amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;部署dns自动扩容&quot;&gt;部署dns自动扩容&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;下载对应的yaml文件，不需要任何修改&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;在1.7.6中rbac还是beta版本，所以，这里我们要修改文件中的authentication.k8s.io/v1 为 authentication.k8s.io/vibeta1&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sed -i &lt;span class=&quot;s1&quot;&gt;'s/rbac.authorization.k8s.io\/v1/rbac.authorization.k8s.io\/v1beta1/gi'&lt;/span&gt; dns-horizontal-autoscaler.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;blockquote&gt;
  &lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f dns-horizontal-autoscaler.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看创建结果&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get pods -n kube-system
NAME                                       READY     STATUS    RESTARTS   AGE
calico-kube-controllers-3994748863-0dpcp   1/1       Running   0          7h
calico-node-74d64                          1/1       Running   0          20h
calico-node-rbrw3                          1/1       Running   0          20h
calico-node-vtcrs                          1/1       Running   0          20h
kube-dns-3468831164-2kl0h                  3/3       Running   0          5h
kube-dns-3468831164-zjgzp                  3/3       Running   0          13m
kube-dns-autoscaler-244676396-bpfpw        1/1       Running   0          13m
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;十四kubernetes周边组件配置&quot;&gt;十四.kubernetes周边组件配置&lt;/h3&gt;

&lt;h5 id=&quot;kubernetes-dashboard配置&quot;&gt;kubernetes-dashboard配置&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;kubernetes基础环境搭建好之后，我们第一步要搭建的就是我们的kubernetes-dashboard&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;准备所需image&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcr.io/google_containers/kubernetes-dashboard-amd64:v1.7.1
gcr.io/google_containers/kubernetes-dashboard-init-amd64:v1.0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;下载所需yaml文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;为了方便测试，我们在最后添加NodePort，后期如果有了ingress或traffic,再将其去掉即可&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 30001
  selector:
    k8s-app: kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;创建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl create -f kubernetes-dashboard.yaml

secret &lt;span class=&quot;s2&quot;&gt;&quot;kubernetes-dashboard-certs&quot;&lt;/span&gt; created
serviceaccount &lt;span class=&quot;s2&quot;&gt;&quot;kubernetes-dashboard&quot;&lt;/span&gt; created
role &lt;span class=&quot;s2&quot;&gt;&quot;kubernetes-dashboard-minimal&quot;&lt;/span&gt; created
rolebinding &lt;span class=&quot;s2&quot;&gt;&quot;kubernetes-dashboard-minimal&quot;&lt;/span&gt; created
deployment &lt;span class=&quot;s2&quot;&gt;&quot;kubernetes-dashboard&quot;&lt;/span&gt; created
service &lt;span class=&quot;s2&quot;&gt;&quot;kubernetes-dashboard&quot;&lt;/span&gt; created

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看创建结果&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get pods -n kube-system -o wide
NAME                                       READY     STATUS    RESTARTS   AGE       IP              NODE
calico-kube-controllers-3994748863-0dpcp   1/1       Running   1          1d        172.29.151.6    172.29.151.6
calico-node-74d64                          1/1       Running   1          1d        172.29.151.6    172.29.151.6
calico-node-rbrw3                          1/1       Running   1          1d        172.29.151.5    172.29.151.5
calico-node-vtcrs                          1/1       Running   1          1d        172.29.151.7    172.29.151.7
kube-dns-3468831164-2kl0h                  3/3       Running   3          23h       10.233.136.10   172.29.151.7
kube-dns-3468831164-zjgzp                  3/3       Running   3          18h       10.233.161.7    172.29.151.5
kube-dns-autoscaler-244676396-bpfpw        1/1       Running   1          18h       10.233.136.9    172.29.151.7
kubernetes-dashboard-3625439193-tgtmm      1/1       Running   0          8s        10.233.136.15   172.29.151.7

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get svc  -n kube-system
NAME                   CLUSTER-IP      EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;         AGE
kube-dns               10.254.0.2      &amp;lt;none&amp;gt;        53/UDP,53/TCP   23h
kubernetes-dashboard   10.254.116.15   &amp;lt;nodes&amp;gt;       443:30001/TCP   1m
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;最后我们来访问 https://$NODEIP:30001试试，我们发现新的kubernetes提供了认证，就算是skip进去之后，也看不到啥东西&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/kubernetes-login.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里我们使用token认证，那么token来自于哪呢，我们创建一个kubernetes-dashboard-rbac.yaml,内容如下&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: dashboard-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: default
  namespace: kube-system
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;创建之后，我们来获取它的token值&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;我们看到这里的serviceaccount是在kube-system的default的，所以我们直接查看kube-system中的default secret就可以了&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f kubernetes-dashboard-rbac.yaml

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl describe secret default-token-d9jjg -n kube-system
Name:		default-token-d9jjg
Namespace:	kube-system
Labels:		&amp;lt;none&amp;gt;
Annotations:	kubernetes.io/service-account.name&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default
		kubernetes.io/service-account.uid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;458abfc9-aef6-11e7-aa7b-00155dfa7a1a

Type:	kubernetes.io/service-account-token

Data
&lt;span class=&quot;o&quot;&gt;====&lt;/span&gt;
ca.crt:		1346 bytes
namespace:	11 bytes
token:		eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLWQ5ampnIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI0NThhYmZjOS1hZWY2LTExZTctYWE3Yi0wMDE1NWRmYTdhMWEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.gRfeCeQSRPOP7yZ94STPZ8GLb77Gx2wAgyVmyATbyoYR7ZMOgqIOMX0lmgZIzCkA1hFnPHcQ863Q9lW_uvkDbHYWA2B2DrRrdkBOYnq_FF2RM09qrwqspS5u3L0w1vgo7S--Rs-mG-yYnMw0EwBtl9rd6Lx7q59sDvWzU47YoQD3HyYZNuIiaIhuZiugvpkJGeKrrsHpd-wh4_rMcTp0GnUKdqSoIpeth2jvudnu34Wv_Jh5q2rhvhMSgb-qEW7JqB5wnDzXLaxkdW7i5PVDZD5RGCQGDwxqr4opfg53JrJQ9ojEjmR7Q0GfgWyKkudwlBm9nPT0VaW4LJkaM37vpQ
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;我们输入token登录看看，发现可以看到内容了&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/kubernetes-dashboard-login-token.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;heapster&quot;&gt;heapster&lt;/h5&gt;

&lt;blockquote&gt;
  &lt;p&gt;kubernetes-dashboard搭建好之后，我们配套的搭建下heapster&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;准备所需镜像&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
gcr.io/google_containers/heapster-amd64:v1.3.0
gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;下载所需yaml文件&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://github.com/kubernetes/heapster/archive/v1.4.3.tar.gz
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;进入heapster-1.4.3/deploy/kube-config/influxdb，修改一下grafana.yaml里面的镜像版本，如果你想要通过NodePort查看下grafana的数据测试一下，可以注释掉service中的 type: NodePort&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gcr.io/google_containers/heapster-grafana-amd64:v4.2.0 -&amp;gt; gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: NodePort
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;执行构建&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;heapster-1.4.3/deploy/kube-config/influxdb
kubectl create -f .

&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;heapster-1.4.3/deploy/kube-config/rbac
kubectl create -f .
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;修改kubernetes-dashboard.yaml 文件，添加如下内容&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;c&quot;&gt;# - --apiserver-host=http://my-address:port&lt;/span&gt;
  - --heapster-host&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://heapster.kube-system.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;重新构建kubernetes-dashboard&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl create -f kubernetes-dashboard.yaml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;查看构建结果&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;kubectl get pods -n kube-system
NAME                                       READY     STATUS    RESTARTS   AGE
calico-kube-controllers-3994748863-0dpcp   1/1       Running   1          1d
calico-node-74d64                          1/1       Running   1          1d
calico-node-rbrw3                          1/1       Running   2          1d
calico-node-vtcrs                          1/1       Running   1          1d
heapster-84017538-54dkm                    1/1       Running   0          1h
kube-dns-3468831164-2kl0h                  3/3       Running   3          1d
kube-dns-3468831164-9hsbm                  3/3       Running   0          3h
kube-dns-autoscaler-244676396-bpfpw        1/1       Running   1          22h
kubernetes-dashboard-2923351285-pzgx5      1/1       Running   0          24m
monitoring-grafana-2115417091-lgqsc        1/1       Running   0          1h
monitoring-influxdb-3570645011-dp51l       1/1       Running   0          1h
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;登录kubernetes-dashboard，查看是否有数据了&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/kubernetes-dashboard-heapster.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;登录到grafana查看数据&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/grafana.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;ingress-配置&quot;&gt;ingress 配置&lt;/h5&gt;

&lt;p&gt;见&lt;a href=&quot;https://kevinguo.me/&quot;&gt;第二章&lt;/a&gt;&lt;/p&gt;</content><author><name>KevinGuo</name></author><summary type="html">原有的环境需要迁移，现在需要重新搭建一套kubernetes，而且原来一直是用kargo来搭建，所有组件都是基于docker容器的，感觉有点不稳妥，所以正好这个时候有机会，可以纯手动部署一下，所有的关键组件都以二进制形式部署，并添加为系统服务，这里记录一下。 该文档参考了诸多大神的文档,漠然、青蛙小白，谨请原谅 一.环境准备 1.1 系统环境 IP HostName 节点 OS 172.29.151.1 k8s-mon-master01 master MON etcd centOS7.4.1708 172.29.151.2 k8s-mon-master02 master MON etcd centOS7.4.1708 172.29.151.3 k8s-mon-master03 master MON etcd centOS7.4.1708 172.29.151.4 k8s-harbor harbor centOS7.4.1708 172.29.151.5 k8s-mds-node01 node MDS centOS7.4.1708 172.29.151.6 k8s-mds-node02 node MDS centOS7.4.1708 172.29.151.7 k8s-mds-node03 node MDS centOS7.4.1708 172.29.151.8 k8s-console console centOS7.4.1708 1.2 系统组件 在安装之前，我们要确认，我们具体需要准备哪些系统组件 docker etcd-3.2.7(etcd、etcdctl) kubernetes-server-1.7.6(kube-apiserver、kube-controller-manager、kube-scheduler) kubernetes-node-1.7.6(kube-proxy、kubelet、kubectl) 1.3 自签名证书 因为所有的组件之间都是通过证书认证的方式来进行通信的，所以我们还得确认下，我们到底需要哪些证书 CA etcd kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet kube-admin 1.4 系统配置 关闭所有节点的selinux、iptables、firewalld systemctl stop iptables systemctl stop firewalld systemctl disable iptables systemctl disable firewalld vi /etc/selinux/config SELINUX=disable 如果你想使用flanel网络，还记得在所有节点上创建 /etc/sysctl.d/k8s.conf文件，添加如下内容，如果是calico网络，请忽略这步 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 # 执行命令使其生效 sysctl -p /etc/sysctl.d/k8s.conf 在所有节点上编辑 /etc/hosts文件，配置host通信 vi /etc/hosts 172.29.151.1 k8s-mon-master01 172.29.151.2 k8s-mon-master02 172.29.151.3 k8s-mon-master03 172.29.151.4 k8s-harbor 172.29.151.5 k8s-mds-node01 172.29.151.6 k8s-mds-node02 172.29.151.7 k8s-mds-node03 172.29.151.8 k8s-console 1.5 创建基本用户 # 在master节点上创建etcd用户 useradd etcd -d /var/lib/etcd -c &quot;Etcd user&quot; -r -s /sbin/nologin # 在maaster节点和node节点上创建kube用户 useradd kube -M -c &quot;Kubernetes user&quot; -r -s /sbin/nologin 1.6 在console上配置免密钥登录 所有证书分发，二进制文件分发，配置文件分发，都将在 k8s-console 上执行，所以该节点主机对集群内所有节点设置了免密钥登录 具体过程可参考免密钥登录 二.创建验证 因为所有组件和apiserver进行通信，都需要使用证书来进行认证，所以这里我们使用CloudFlare的PKI工具集 cfssl 来生成CA证书和其密钥文件 如果你的kube-controller-manager、kube-scheduler同apiserver之间通信不需要进行证书认证(毕竟他们都在同一台机器上)，那么下面有关kube-controller-manager、kube-scheduler的证书步骤可以忽略；而在该实验中，我考虑到后面假若它们不在同一台机器上，所以也记录了kube-controller-manager、kube-scheduler的证书创建配置过程 2.1 安装cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 mv cfssl_linux-amd64 /usr/local/bin/cfssl mv cfssljson_linux-amd64 /usr/local/bin/cfssljson 2.2 创建CA证书配置，生成CA证书和私钥 先用 cfssl 命令生成包含默认配置的 config.json和 csr.json文件 mkdir /opt/ssl cd /opt/ssl cfssl print-defaults config &amp;gt; config.json cfssl print-defaults csr &amp;gt; csr.json 然后分别修改这两个文件为如下内容 config.json { &quot;signing&quot;: { &quot;default&quot;: { &quot;expiry&quot;: &quot;87600h&quot; }, &quot;profiles&quot;: { &quot;kubernetes&quot;: { &quot;usages&quot;: [ &quot;signing&quot;, &quot;key encipherment&quot;, &quot;server auth&quot;, &quot;client auth&quot; ], &quot;expiry&quot;: &quot;87600h&quot; } } } } ca-config.json：可以定义多个 profiles，分别指定不同的过期时间、使用场景等参数；后续在签名证书时使用某个 profile； signing：表示该证书可用于签名其它证书；生成的 ca.pem 证书中 CA=TRUE； server auth：表示client可以用该 CA 对server提供的证书进行验证； client auth：表示server可以用该CA对client提供的证书进行验证； csr.json { &quot;CN&quot;: &quot;kubernetes&quot;, &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Wuhan&quot;, &quot;L&quot;: &quot;Hubei&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } CN：Common Name，kube-apiserver 从证书中提取该字段作为请求的用户名 (User Name)；浏览器使用该字段验证网站是否合法； O：Organization，kube-apiserver 从证书中提取该字段作为请求用户所属的组 (Group)； 生成CA 证书和私钥 cd /opt/ssl cfssl gencert -initca csr.json | cfssljson -bare ca # CA有关证书列表如下 [root@k8s-console ssl]# tree . ├── ca.csr ├── ca-key.pem ├── ca.pem ├── config.json └── csr.json 2.4 创建etcd证书配置，生成 etcd 证书和私钥 在/opt/ssl 下添加文件 etcd-csr.json，内容如下 etcd-csr.json { &quot;CN&quot;: &quot;etcd&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;172.29.151.1&quot;, &quot;172.29.151.2&quot;, &quot;172.29.151.3&quot; ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Wuhan&quot;, &quot;L&quot;: &quot;Hubei&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } 生成etcd证书和密钥 cd /opt/ssl cfssl gencert -ca=/opt/ssl/ca.pem \ -ca-key=/opt/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes etcd-csr.json | cfssljson -bare etcd # etcd 有关证书证书列表如下 ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem 2.5 创建kube-apiserver证书配置，生成kube-apiserver证书和私钥 在/opt/ssl 下添加文件 kube-apiserver-csr.json，内容如下 kube-apiserver-csr.json { &quot;CN&quot;: &quot;kubernetes&quot;, &quot;hosts&quot;: [ &quot;127.0.0.1&quot;, &quot;172.29.151.1&quot;, &quot;172.29.151.2&quot;, &quot;172.29.151.3&quot;, &quot;10.254.0.1&quot;, &quot;localhost&quot;, &quot;kubernetes&quot;, &quot;kubernetes.default&quot;, &quot;kubernetes.default.svc&quot;, &quot;kubernetes.default.svc.cluster&quot;, &quot;kubernetes.default.svc.cluster.local&quot; ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Wuhan&quot;, &quot;L&quot;: &quot;Hubei&quot;, &quot;O&quot;: &quot;k8s&quot;, &quot;OU&quot;: &quot;System&quot; } ] } 如果 hosts 字段不为空则需要指定授权使用该证书的 IP 或域名列表，由于该证书被 etcd 集群和 kubernetes master 集群使用，所以上面分别指定了 etcd 集群、kubernetes master 集群的主机 IP 和 kubernetes 服务的服务 IP（一般是 kue-apiserver 指定的 service-cluster-ip-range 网段的第一个IP，如 10.254.0.1。 生成kube-apiserver证书和私钥 cd /opt/ssl cfssl gencert -ca=/opt/ssl/ca.pem \ -ca-key=/opt/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver # 列出kube-apiserver有关证书 ls kube-apiserver* kube-apiserver.csr kube-apiserver-csr.json kube-apiserver-key.pem kube-apiserver.pem 2.6 创建kube-controller-manager证书配置，生成kube-controller-manager证书和私钥 在/opt/ssl 下添加文件 kube-controller-manager-csr.json，内容如下 kube-controller-manager-csr.json { &quot;CN&quot;: &quot;system:kube-controller-manager&quot;, &quot;hosts&quot;: [ &quot;172.29.151.1&quot;, &quot;172.29.151.2&quot;, &quot;172.29.151.3&quot; ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Wuhan&quot;, &quot;L&quot;: &quot;Hubei&quot;, &quot;O&quot;: &quot;system:kube-controller-manager&quot;, &quot;OU&quot;: &quot;System&quot; } ] } CN 指定该证书的 User 为 system:kube-controller-manager kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-controller-manager 与 ClusterRole system:kube-controller-manager 绑定，该 ClusterRole 授予了调用 kube-apiserver kube-controller-manager 相关 API 的权限 生成kube-controller-manager证书和私钥 cd /opt/ssl cfssl gencert -ca=/opt/ssl/ca.pem \ -ca-key=/opt/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager # 列出kube-controller-manager有关证书 ls kube-controller-manager* kube-controller-manager.csr kube-controller-manager-csr.json kube-controller-manager-key.pem kube-controller-manager.pem 2.7 创建kube-scheduler证书配置，生成kube-scheduler证书和私钥 在/opt/ssl 下添加文件 kube-scheduler-csr.json，内容如下 kube-scheduler-csr.json { &quot;CN&quot;: &quot;system:kube-scheduler&quot;, &quot;hosts&quot;: [ &quot;172.29.151.1&quot;, &quot;172.29.151.2&quot;, &quot;172.29.151.3&quot; ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Wuhan&quot;, &quot;L&quot;: &quot;Hubei&quot;, &quot;O&quot;: &quot;system:kube-scheduler&quot;, &quot;OU&quot;: &quot;System&quot; } ] } CN 指定该证书的 User 为 system:kube-scheduler kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-scheduler 与 ClusterRole system:kube-scheduler 绑定，该 ClusterRole 授予了调用 kube-apiserver kube-scheduler 相关 API 的权限 生成kube-scheduler证书和私钥 cd /opt/ssl cfssl gencert -ca=/opt/ssl/ca.pem \ -ca-key=/opt/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler # 列出kube-scheduler有关证书 ls kube-scheduler* kube-scheduler.csr kube-scheduler-csr.json kube-scheduler-key.pem kube-scheduler.pem 2.8 创建kube-admin证书配置，生成kube-admin证书和私钥 在/opt/ssl 下添加文件 kube-admin-csr.json，内容如下 kube-admin-csr.json { &quot;CN&quot;: &quot;kube-admin&quot;, &quot;hosts&quot;: [ &quot;172.29.151.8&quot; ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Wuhan&quot;, &quot;L&quot;: &quot;Hubei&quot;, &quot;O&quot;: &quot;system:masters&quot;, &quot;OU&quot;: &quot;System&quot; } ] } 后续 kube-apiserver 使用 RBAC 对客户端(如 kubelet、kube-proxy、Pod)请求进行授权 kube-apiserver 预定义了一些 RBAC 使用的 RoleBindings，如 cluster-admin 将 Group system:masters 与 Role cluster-admin 绑定，该 Role 授予了调用kube-apiserver 的所有 API的权限 OU 指定该证书的 Group 为 system:masters，kubelet 使用该证书访问 kube-apiserver 时 ，由于证书被 CA 签名，所以认证通过，同时由于证书用户组为经过预授权的 system:masters，所以被授予访问所有 API 的权限 生成kube-admin证书和私钥 cd /opt/ssl cfssl gencert -ca=/opt/ssl/ca.pem \ -ca-key=/opt/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kube-admin-csr.json | cfssljson -bare kube-admin # 列出kube-admin有关证书 ls kube-admin* kube-admin.csr kube-admin-csr.json kube-admin-key.pem kube-admin.pem 2.9 创建kube-proxy证书配置，生成kube-proxy证书和私钥 在/opt/ssl 下添加文件 kube-proxy-csr.json，内容如下 kube-proxy-csr.json { &quot;CN&quot;: &quot;system:kube-proxy&quot;, &quot;hosts&quot;: [ ], &quot;key&quot;: { &quot;algo&quot;: &quot;rsa&quot;, &quot;size&quot;: 2048 }, &quot;names&quot;: [ { &quot;C&quot;: &quot;CN&quot;, &quot;ST&quot;: &quot;Wuhan&quot;, &quot;L&quot;: &quot;Hubei&quot;, &quot;O&quot;: &quot;system:kube-proxy&quot;, &quot;OU&quot;: &quot;System&quot; } ] } CN 指定该证书的 User 为 system:kube-proxy kube-apiserver 预定义的 RoleBinding cluster-admin 将User system:kube-proxy 与 ClusterRole system:node-proxier 绑定，该 ClusterRole 授予了调用 kube-apiserver Proxy 相关 API 的权限 生成kube-proxy证书和私钥 cd /opt/ssl cfssl gencert -ca=/opt/ssl/ca.pem \ -ca-key=/opt/ssl/ca-key.pem \ -config=/opt/ssl/config.json \ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy # 列出kube-proxy有关证书 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem 2.10 kubelet 证书和私钥 kubelet 其实也可以手动通过CA来进行签发，但是这只能针对少数机器，毕竟我们在进行证书签发的时候，是需要绑定对应Node的IP的，如果node太多了，加IP就会很幸苦， 所以这里我们使用TLS 认证，由apiserver自动给符合条件的node签发证书，允许节点加入集群。 kubelet 首次启动时想kube-apiserver发送TLS Bootstrapping请求，kube-apiserver验证kubelet请求中的token是否与它配置的token一致，如果一致则自动为kubelet生成证书和密钥。具体参考kubelet-tls-bootstrapping 我们在k8s-console上生成token并分发到所有的master节点 cd /opt/ssl head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 04d9b6c6fd3ed8a3488b3b0913e87d64 vim token.csv 04d9b6c6fd3ed8a3488b3b0913e87d64,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot; 2.11 证书分发 既然证书都创建好了，那么这时候，我们就需要将对应的证书分发到对应的节点上去 master cd /opt/ssl for IP in `seq 1 3`;do ssh root@172.29.151.$IP mkdir -p /etc/kubernetes/ssl ssh root@172.29.151.$IP mkdir -p /etc/etcd/ssl scp ca*.pem kube-apiserver*.pem kube-controller-manager*.pem kube-scheduler*.pem root@172.29.151.$IP:/etc/kubernetes/ssl/ ssh root@172.29.151.$IP chown -R kube:kube /etc/kubernetes/ssl scp ca*.pem etcd*.pem root@172.29.151.$IP:/etc/etcd/ssl ssh root@172.29.151.$IP chown -R etcd:etcd /etc/etcd/ssl done node cd /opt/ssl for IP in `seq 5 7`;do ssh root@172.29.151.$IP mkdir -p /etc/kubernetes/ssl scp ca*.pem kube-proxy*.pem root@172.29.151.$IP:/etc/kubernetes/ssl/ ssh root@172.29.151.$IP chown -R kube:kube /etc/kubernetes/ssl done console cd /opt/ssl cp kube-admin*.pem /etc/kubernetes/ssl/ chown -R kube:kube /etc/kubernetes/ssl/ 2.11 token分发 cd /opt/ssl for IP in `seq 1 3`;do ssh root@172.29.151.$IP mkdir -p /etc/kubernetes/known_token scp token.csv root@172.29.151.$IP:/etc/kubernetes/known_token/ ssh root@172.29.151.$IP chown -R kube:kube /etc/kubernetes/known_token done 三.分发二进制文件 分发etcd tar zxvf etcd-v3.2.7-linux-amd64.tar.gz cd etcd-v3.2.7-linux-amd64 for IP in `seq 1 3`;do scp etcd etcdctl root@172.29.151.$IP:/usr/bin/ done 分发kubernetes master tar zxvf kubernetes-server-linux-amd64.tar.gz cd kubernetes/server/bin for IP in `seq 1 3`;do scp kube-apiserver kube-controller-manager kube-scheduler root@172.29.151.$IP:/usr/local/bin/ done 分发kubernetes node tar zxvf kubernetes-node-linux-amd64.tar.gz cd kubernetes/node/bin for IP in `seq 5 7`;do scp kubelet kube-proxy root@172.29.151.$IP:/usr/local/bin/ done 分发kubectl,etcdctl cd kubernetes/node/bin cp kubectl /usr/local/bin/ cd etcd-v3.2.7-linux-amd64 cp etcdctl /usr/bin/ 四.etcd集群部署 添加etcd为系统服务 在每个master节点上添加etcd启动文件/usr/lib/systemd/system/etcd.service # etcd1 [Unit] Description=etcd server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify User=etcd WorkingDirectory=/var/lib/etcd/ EnvironmentFile=-/etc/etcd/etcd.conf ExecStart=/usr/bin/etcd \ --name etcd1 \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ --peer-cert-file=/etc/etcd/ssl/etcd.pem \ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \ --trusted-ca-file=/etc/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \ --initial-advertise-peer-urls https://172.29.151.1:2380 \ --listen-peer-urls https://172.29.151.1:2380 \ --listen-client-urls https://172.29.151.1:2379,https://127.0.0.1:2379 \ --advertise-client-urls https://172.29.151.1:2379 \ --initial-cluster-token k8s_etcd \ --initial-cluster etcd1=https://172.29.151.1:2380,etcd2=https://172.29.151.2:2380,etcd3=https://172.29.151.3:2380 \ --initial-cluster-state new \ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 # etcd2 [Unit] Description=etcd server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify User=etcd WorkingDirectory=/var/lib/etcd/ EnvironmentFile=-/etc/etcd/etcd.conf ExecStart=/usr/bin/etcd \ --name etcd2 \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ --peer-cert-file=/etc/etcd/ssl/etcd.pem \ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \ --trusted-ca-file=/etc/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \ --initial-advertise-peer-urls https://172.29.151.2:2380 \ --listen-peer-urls https://172.29.151.2:2380 \ --listen-client-urls https://172.29.151.2:2379,https://127.0.0.1:2379 \ --advertise-client-urls https://172.29.151.2:2379 \ --initial-cluster-token k8s_etcd \ --initial-cluster etcd1=https://172.29.151.1:2380,etcd2=https://172.29.151.2:2380,etcd3=https://172.29.151.3:2380 \ --initial-cluster-state new \ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 # etcd3 [Unit] Description=etcd server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify User=etcd WorkingDirectory=/var/lib/etcd/ EnvironmentFile=-/etc/etcd/etcd.conf ExecStart=/usr/bin/etcd \ --name etcd3 \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ --peer-cert-file=/etc/etcd/ssl/etcd.pem \ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \ --trusted-ca-file=/etc/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \ --initial-advertise-peer-urls https://172.29.151.3:2380 \ --listen-peer-urls https://172.29.151.3:2380 \ --listen-client-urls https://172.29.151.3:2379,https://127.0.0.1:2379 \ --advertise-client-urls https://172.29.151.3:2379 \ --initial-cluster-token k8s_etcd \ --initial-cluster etcd1=https://172.29.151.1:2380,etcd2=https://172.29.151.2:2380,etcd3=https://172.29.151.3:2380 \ --initial-cluster-state new \ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 验证etcd 集群状态 查看etcd集群状态 etcdctl \ --endpoints=https://172.29.151.1:2379 \ --cert-file=/etc/etcd/ssl/etcd.pem \ --ca-file=/etc/etcd/ssl/ca.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ cluster-health member 31a0c451ae46a2d0 is healthy: got healthy result from https://172.29.151.1:2379 member 72b18fe792c0a463 is healthy: got healthy result from https://172.29.151.3:2379 member d0c073403f6edbd3 is healthy: got healthy result from https://172.29.151.2:2379 cluster is healthy 查看etcd 集群成员 etcdctl \ --endpoints=https://172.29.151.1:2379 \ --cert-file=/etc/etcd/ssl/etcd.pem \ --ca-file=/etc/etcd/ssl/ca.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ member list 31a0c451ae46a2d0: name=etcd1 peerURLs=https://172.29.151.1:2380 clientURLs=https://172.29.151.1:2379 isLeader=true 72b18fe792c0a463: name=etcd3 peerURLs=https://172.29.151.3:2380 clientURLs=https://172.29.151.3:2379 isLeader=false d0c073403f6edbd3: name=etcd2 peerURLs=https://172.29.151.2:2380 clientURLs=https://172.29.151.2:2379 isLeader=false 五.kube-apiserver部署 添加kube-apiserver为系统服务 在每个master节点上添加/usr/lib/systemd/system/kube-apiserver.service，注意修改为各自节点的ip地址 # 创建日志目录文件 mkdir -p /var/log/kubernetes kube-apiserver.service文件内容如下 注意：安全端口监听在172.29.151.1，提供给node节点访问，非安全端口监听在127.0.0.1，只提供给同一台机器上的kube-controller-manager和kube-scheduler访问，这样就保证了安全性和稳定性 [Unit] Description=Kubernetes API Server After=network.target After=etcd.service [Service] EnvironmentFile=-/etc/kubernetes/apiserver ExecStart=/usr/local/bin/kube-apiserver \ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds \ --advertise-address=172.29.151.1 \ --bind-address=172.29.151.1 \ --insecure-bind-address=127.0.0.1 \ --service-cluster-ip-range=10.254.0.0/16 \ --service-node-port-range=30000-32000 \ --allow-privileged=true \ --apiserver-count=3 \ --logtostderr=true \ --v=0 \ --audit-log-maxage=30 \ --audit-log-maxbackup=3 \ --audit-log-maxsize=100 \ --audit-log-path=/var/log/kubernetes/audit.log \ --authorization-mode=RBAC \ --enable-swagger-ui=true \ --event-ttl=1h \ --secure-port=6443 \ --insecure-port=8080 \ --etcd-servers=https://172.29.151.1:2379,https://172.29.151.2:2379,https://172.29.151.3:2379 \ --etcd-cafile=/etc/etcd/ssl/ca.pem \ --etcd-certfile=/etc/etcd/ssl/etcd.pem \ --etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \ --storage-backend=etcd3 \ --tls-cert-file=/etc/kubernetes/ssl/kube-apiserver.pem \ --tls-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \ --client-ca-file=/etc/kubernetes/ssl/ca.pem \ --service-account-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \ --token-auth-file=/etc/kubernetes/known_token/token.csv \ --experimental-bootstrap-token-auth=true \ --kubelet-https=true \ --anonymous-auth=False Restart=on-failure Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target 启动kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver 六.kube-controller-manager部署 添加kube-controller-manager为系统服务 在每个master节点上添加/usr/lib/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager After=network.target After=kube-apiserver.service [Service] ExecStart=/usr/local/bin/kube-controller-manager \ --address=127.0.0.1 \ --master=http://127.0.0.1:8080 \ --allocate-node-cidrs=true \ --service-cluster-ip-range=10.254.0.0/16 \ --cluster-cidr=10.233.0.0/16 \ --cluster-name=kubernetes \ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \ --service-account-private-key-file=/etc/kubernetes/ssl/kube-apiserver-key.pem \ --root-ca-file=/etc/kubernetes/ssl/ca.pem \ --leader-elect=true \ --node-monitor-grace-period=40s \ --node-monitor-period=5s \ --pod-eviction-timeout=5m0s \ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 启动kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager 七.kube-scheduler部署 添加kube-scheduler为系统服务 在每个master节点上添加/usr/lib/systemd/system/kube-scheduler.service [Unit] Description=kube-scheduler After=network.target After=kube-apiserver.service [Service] EnvironmentFile=-/etc/kubernetes/scheduler ExecStart=/usr/local/bin/kube-scheduler \ --address=127.0.0.1 \ --logtostderr=true \ --v=2 \ --master=127.0.0.1:8080 \ --leader-elect=true Restart=on-failure Type=simple LimitNOFILE=65536 [Install] WantedBy=multi-user.target 启动kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler 八.Master HA配置 目前所谓的kubernetes HA 其实主要是API Server的HA，master上的其他组件，比如kube-controller-manager、kube-scheduler都是通过etcd做选举。而API Server一般有两种方式做HA；一种是多个API Server 做聚合为 VIP，另一种使用nginx反向代理，这里我们采用nginx的方式，如下图 kube-controller-manager、kube-scheduler通过etcd选举，而且与master直接通过127.0.0.1:8080通信，而其他node，则需要在每个node上启动一个nginx，每个nginx反代所有apiserver，node上的kubelet、kube-proxy、kubectl连接本地nginx代理端口，当nginx发现无法连接后端时会自动踢掉出问题的apiserver，从而实现api server的HA 在每个node节点和k8s-console上创建nginx代理 在每个节点上新建配置目录 mkdir -p /etc/nginx 在配置文件/etc/nginx/nginx.conf中下写入代理配置 error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.29.151.1:6443; server 172.29.151.2:6443; server 172.29.151.3:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } 更新权限 chmod +r /etc/nginx/nginx.conf 将nginx配置为docker启动，同时用systemd来进行守护 在每个node节点上添加/etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \ -v /etc/nginx:/etc/nginx \ --name nginx-proxy \ --net=host \ --restart=on-failure:5 \ --memory=512M \ nginx:1.13.3-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target 配置开机启动 systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy 最后我们在k8s-console上执行kubectl试试 $ kubectl --server=https://127.0.0.1:6443 --certificate-authority=/etc/kubernetes/ssl/ca.pem --client-certificate=/etc/kubernetes/ssl/kube-admin.pem --client-key=/etc/kubernetes/ssl/kube-admin-key.pem get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-1 Healthy {&quot;health&quot;: &quot;true&quot;} etcd-0 Healthy {&quot;health&quot;: &quot;true&quot;} etcd-2 Healthy {&quot;health&quot;: &quot;true&quot;} 九. 配置kubectl访问apiserver 前面我们使用kubect打印除了kubernetes核心组件的状态，但是每次使用的时候都需要指定apiserver的地址以及证书之类的，实在是有点繁琐，接下来，我们在k8s-console上创建kubeconfig文件。 cd /etc/kubernetes export KUBE_APISERVER=&quot;https://127.0.0.1:6443&quot; # 设置集群参数 kubectl config set-cluster kubernetes \ --certificate-authority=/opt/ssl/ca.pem \ --embed-certs=true \ --server=${KUBE_APISERVER} \ --kubeconfig=admin.conf # 设置客户端认证参数 kubectl config set-credentials kubernetes-admin \ --client-certificate=/opt/ssl/kube-admin.pem \ --embed-certs=true \ --client-key=/opt/ssl/kube-admin-key.pem \ --kubeconfig=admin.conf # 设置上下文参数 kubectl config set-context kubernetes-admin@kubernetes \ --cluster=kubernetes \ --user=kubernetes-admin \ --kubeconfig=admin.conf # 设置默认上下文 kubectl config use-context kubernetes-admin@kubernetes --kubeconfig=admin.conf # cp成~/.kube/config cp /etc/kubernetes/ssl/admin.conf ~/.kube/config 试试看是否生效 $ kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-2 Healthy {&quot;health&quot;: &quot;true&quot;} etcd-1 Healthy {&quot;health&quot;: &quot;true&quot;} etcd-0 Healthy {&quot;health&quot;: &quot;true&quot;} 十.kubelet配置 kubelet启动时向kube-apiserver发送 TLS bootstrapping请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap用户赋予system:node-bootstrapper角色，然后kubelet才有权限创建认证请求。 kubelet角色授权 # 在k8s-console上执行绑定操作 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap 在k8s-console上生成kubelet kubeconfig文件 配置集群 kubectl config set-cluster kubernetes \ --certificate-authority=/opt/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=bootstrap.kubeconfig 配置客户端认证参数 kubectl config set-credentials kubelet-bootstrap \ --token=04d9b6c6fd3ed8a3488b3b0913e87d64 \ --kubeconfig=bootstrap.kubeconfig 配置上下文关联 kubectl config set-context default \ --cluster=kubernetes \ --user=kubelet-bootstrap \ --kubeconfig=bootstrap.kubeconfig 配置默认上下文 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig 分发bootstrap.kubeconfig文件到每个node节点 cd /etc/kubernetes for IP in `seq 5 7`;do scp bootstrap.kubeconfig root@172.29.151.$IP:/etc/kubernetes/ done 添加kubelet为系统服务 创建kubelet工作目录 mkdir /var/lib/kubelet 添加/usr/lib/systemd/system/kubelet.service,注意修改你成你自己节点的ip [Unit] Description=Kubernetes Kubelet After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \ --cgroup-driver=systemd \ --address=172.29.151.5 \ --hostname-override=172.29.151.5 \ --pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0 \ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \ --require-kubeconfig \ --cert-dir=/etc/kubernetes/ssl \ --cluster_dns=10.254.0.2 \ --cluster_domain=cluster.local. \ --hairpin-mode promiscuous-bridge \ --allow-privileged=true \ --serialize-image-pulls=false \ --logtostderr=true \ --max-pods=512 \ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target 启动kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet 签发证书，验证nodes 查看csr，我们发现状态为Pending $ kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-MIqovZHmrYMe1Y6AspcfU6_keLdSWfbUqg4pcK-Hb9w 2m kubelet-bootstrap Pending node-csr-el6foG3yw6_9xCu1vC_upuT-xLR9Z9ASBNj5isBFcsY 2m kubelet-bootstrap Pending node-csr-oPWmprgtrRixLZXUvEFKnHI2qZEGorzHKZ1ktLMdGS8 5m kubelet-bootstrap Pending 签发证书 $ kubectl certificate approve node-csr-oPWmprgtrRixLZXUvEFKnHI2qZEGorzHKZ1ktLMdGS8 node-csr-el6foG3yw6_9xCu1vC_upuT-xLR9Z9ASBNj5isBFcsY node-csr-MIqovZHmrYMe1Y6AspcfU6_keLdSWfbUqg4pcK-Hb9w certificatesigningrequest &quot;node-csr-oPWmprgtrRixLZXUvEFKnHI2qZEGorzHKZ1ktLMdGS8&quot; approved certificatesigningrequest &quot;node-csr-el6foG3yw6_9xCu1vC_upuT-xLR9Z9ASBNj5isBFcsY&quot; approved certificatesigningrequest &quot;node-csr-MIqovZHmrYMe1Y6AspcfU6_keLdSWfbUqg4pcK-Hb9w&quot; approved 查看node $ kubectl get nodes NAME STATUS AGE VERSION 172.29.151.5 Ready 3m v1.7.6 172.29.151.6 Ready 45s v1.7.6 172.29.151.7 Ready 12s v1.7.6 成功后会自动生成配置文件和密钥 $ ll /etc/kubernetes/ssl -rw-r--r-- 1 root root 1042 Oct 9 17:46 kubelet-client.crt -rw------- 1 root root 227 Oct 9 17:18 kubelet-client.key -rw-r--r-- 1 root root 1111 Oct 9 17:46 kubelet.crt -rw------- 1 root root 1675 Oct 9 17:46 kubelet.key $ ll /etc/kubernetes/kubelet.kubeconfig -rw------- 1 root root 2260 Oct 9 17:46 /etc/kubernetes/kubelet.kubeconfig 十一.kube-proxy 配置 在k8s-console上生成kube-proxy kubeconfig文件 配置集群 kubectl config set-cluster kubernetes \ --certificate-authority=/opt/ssl/ca.pem \ --embed-certs=true \ --server=https://127.0.0.1:6443 \ --kubeconfig=kube-proxy.kubeconfig 配置客户端认证 kubectl config set-credentials kube-proxy \ --client-certificate=/opt/ssl/kube-proxy.pem \ --client-key=/opt/ssl/kube-proxy-key.pem \ --embed-certs=true \ --kubeconfig=kube-proxy.kubeconfig 配置上下文 kubectl config set-context default \ --cluster=kubernetes \ --user=kube-proxy \ --kubeconfig=kube-proxy.kubeconfig 配置默认上下文 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 分发到各个节点的/etc/kubernetes 目录 for IP in `seq 5 7`;do scp kube-proxy.kubeconfig root@172.29.151.$IP:/etc/kubernetes/ done 添加kube-proxy为系统服务 创建 kube-proxy目录 mkdir -p /var/lib/kube-proxy 添加/usr/lib/systemd/system/kube-proxy.service，注意修改为自己的节点ip [Unit] Description=Kubernetes Kube-Proxy Server After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \ --bind-address=172.29.151.5 \ --hostname-override=172.29.151.5 \ --cluster-cidr=10.254.0.0/16 \ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \ --logtostderr=true \ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target 启动kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy 十二.calico配置 网络组件采用calico，calico部署比较简单，只需要create 一下yml文件即可，具体参考calico官方文档 ，在使用calico网络的时候，官方的要求如下 kubelet 必须配置使用CNI插件--network-plugin=cni kube-proxy 必须以iptables的模式启动 kube-proxy 不能使用--masquerade-all启动(会与calico policy冲突) kubernetes networkpolicy api 至少需要kubernetes 1.3 版本以上 如果开启了RBAC，那么需要注意需要创建clusterrole和clusterrolebinding 在每个节点上修改kubelet.service 修改kubelet配置，增加--network-plugin=cni vi /usr/lib/systemd/system/kubelet.service --network-plugin=cni 重启kubelet systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet.service 准备依赖包和文件 下载calico.yaml 和rbac.yaml wget https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/calico.yaml wget https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/rbac.yaml 下载镜像 quay.io/calico/node:v2.6.1 quay.io/calico/cni:v1.11.0 quay.io/calico/kube-controllers:v1.0.0 修改配置文件 修改etcd_endpoints etcd_endpoints: &quot;https://172.29.151.1:2379,https://172.29.151.2:2379,https://172.29.151.3:2379&quot; 修改calico所需的etcd密钥信息 etcd_ca: &quot;/calico-secrets/etcd-ca&quot; etcd_cert: &quot;/calico-secrets/etcd-cert&quot; etcd_key: &quot;/calico-secrets/etcd-key&quot; 写入etcd-key、etcd-cert、etcd-ca的base64信息，将括号里面命令执行的结果填入即可 data: etcd-key: (cat /opt/ssl/etcd-key.pem | base64 | tr -d '\n') etcd-cert: (cat /opt/ssl/etcd.pem | base64 | tr -d '\n') etcd-ca: (cat /opt/ssl/ca.pem | base64 | tr -d '\n') 修改calico的网络段 - name: CALICO_IPV4POOL_CIDR value: &quot;10.233.0.0/16&quot; 注释掉calico-node 部分，这部分用systemctl来进行管理，因为用官方文档可能会出现无法获取到IP的情况 # Calico Version v2.6.1 # https://docs.projectcalico.org/v2.6/releases#v2.6.1 # This manifest includes the following component versions: # calico/node:v2.6.1 # calico/cni:v1.11.0 # calico/kube-controllers:v1.0.0 # This ConfigMap is used to configure a self-hosted Calico installation. kind: ConfigMap apiVersion: v1 metadata: name: calico-config namespace: kube-system data: # Configure this with the location of your etcd cluster. etcd_endpoints: &quot;https://172.29.151.1:2379,https://172.29.151.2:2379,https://172.29.151.3:2379&quot; # Configure the Calico backend to use. calico_backend: &quot;bird&quot; # The CNI network configuration to install on each node. cni_network_config: |- { &quot;name&quot;: &quot;k8s-pod-network&quot;, &quot;cniVersion&quot;: &quot;0.1.0&quot;, &quot;type&quot;: &quot;calico&quot;, &quot;etcd_endpoints&quot;: &quot;__ETCD_ENDPOINTS__&quot;, &quot;etcd_key_file&quot;: &quot;__ETCD_KEY_FILE__&quot;, &quot;etcd_cert_file&quot;: &quot;__ETCD_CERT_FILE__&quot;, &quot;etcd_ca_cert_file&quot;: &quot;__ETCD_CA_CERT_FILE__&quot;, &quot;log_level&quot;: &quot;info&quot;, &quot;mtu&quot;: 1500, &quot;ipam&quot;: { &quot;type&quot;: &quot;calico-ipam&quot; }, &quot;policy&quot;: { &quot;type&quot;: &quot;k8s&quot;, &quot;k8s_api_root&quot;: &quot;https://__KUBERNETES_SERVICE_HOST__:__KUBERNETES_SERVICE_PORT__&quot;, &quot;k8s_auth_token&quot;: &quot;__SERVICEACCOUNT_TOKEN__&quot; }, &quot;kubernetes&quot;: { &quot;kubeconfig&quot;: &quot;__KUBECONFIG_FILEPATH__&quot; } } # If you're using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: &quot;/calico-secrets/etcd-ca&quot; etcd_cert: &quot;/calico-secrets/etcd-cert&quot; etcd_key: &quot;/calico-secrets/etcd-key&quot; --- # The following contains k8s Secrets for use with a TLS enabled etcd cluster. # For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/ apiVersion: v1 kind: Secret type: Opaque metadata: name: calico-etcd-secrets namespace: kube-system data: # Populate the following files with etcd TLS configuration if desired, but leave blank if # not using TLS for etcd. # This self-hosted install expects three files with the following names. The values # should be base64 encoded strings of the entire contents of each file. etcd-key: 这块自己对 etcd 相关证书做 base64 etcd-cert: 这块自己对 etcd 相关证书做 base64 etcd-ca: 这块自己对 etcd 相关证书做 base64 --- # This manifest installs the calico/node container, as well # as the Calico CNI plugins and network config on # each master and worker node in a Kubernetes cluster. kind: DaemonSet apiVersion: extensions/v1beta1 metadata: name: calico-node namespace: kube-system labels: k8s-app: calico-node spec: selector: matchLabels: k8s-app: calico-node template: metadata: labels: k8s-app: calico-node annotations: scheduler.alpha.kubernetes.io/critical-pod: '' scheduler.alpha.kubernetes.io/tolerations: | [{&quot;key&quot;: &quot;dedicated&quot;, &quot;value&quot;: &quot;master&quot;, &quot;effect&quot;: &quot;NoSchedule&quot; }, {&quot;key&quot;:&quot;CriticalAddonsOnly&quot;, &quot;operator&quot;:&quot;Exists&quot;}] spec: hostNetwork: true serviceAccountName: calico-node containers: # Runs calico/node container on each Kubernetes node. This # container programs network policy and routes on each # host. # 从这里开始注释掉calico-node的部分 # - name: calico-node # image: quay.io/calico/node:v2.6.1 # env: # # The location of the Calico etcd cluster. # - name: ETCD_ENDPOINTS # valueFrom: # configMapKeyRef: # name: calico-config # key: etcd_endpoints # # Choose the backend to use. # - name: CALICO_NETWORKING_BACKEND # valueFrom: # configMapKeyRef: # name: calico-config # key: calico_backend # # Cluster type to identify the deployment type # - name: CLUSTER_TYPE # value: &quot;k8s,bgp&quot; # # Disable file logging so `kubectl logs` works. # - name: CALICO_DISABLE_FILE_LOGGING # value: &quot;true&quot; # # Set Felix endpoint to host default action to ACCEPT. # - name: FELIX_DEFAULTENDPOINTTOHOSTACTION # value: &quot;ACCEPT&quot; # # Configure the IP Pool from which Pod IPs will be chosen. # - name: CALICO_IPV4POOL_CIDR # value: &quot;10.233.0.0/16&quot; # - name: CALICO_IPV4POOL_IPIP # value: &quot;always&quot; # # Disable IPv6 on Kubernetes. # - name: FELIX_IPV6SUPPORT # value: &quot;false&quot; # # Set Felix logging to &quot;info&quot; # - name: FELIX_LOGSEVERITYSCREEN # value: &quot;info&quot; # # Set MTU for tunnel device used if ipip is enabled # - name: FELIX_IPINIPMTU # value: &quot;1440&quot; # # Location of the CA certificate for etcd. # - name: ETCD_CA_CERT_FILE # valueFrom: # configMapKeyRef: # name: calico-config # key: etcd_ca # # Location of the client key for etcd. # - name: ETCD_KEY_FILE # valueFrom: # configMapKeyRef: # name: calico-config # key: etcd_key # # Location of the client certificate for etcd. # - name: ETCD_CERT_FILE # valueFrom: # configMapKeyRef: # name: calico-config # key: etcd_cert # # Auto-detect the BGP IP address. # - name: IP # value: &quot;&quot; # - name: FELIX_HEALTHENABLED # value: &quot;true&quot; # securityContext: # privileged: true # resources: # requests: # cpu: 250m # livenessProbe: # httpGet: # path: /liveness # port: 9099 # periodSeconds: 10 # initialDelaySeconds: 10 # failureThreshold: 6 # readinessProbe: # httpGet: # path: /readiness # port: 9099 # periodSeconds: 10 # volumeMounts: # - mountPath: /lib/modules # name: lib-modules # readOnly: true # - mountPath: /var/run/calico # name: var-run-calico # readOnly: false # - mountPath: /calico-secrets # name: etcd-certs # This container installs the Calico CNI binaries # and CNI network config file on each node. - name: install-cni image: quay.io/calico/cni:v1.11.0 command: [&quot;/install-cni.sh&quot;] env: # The location of the Calico etcd cluster. - name: ETCD_ENDPOINTS valueFrom: configMapKeyRef: name: calico-config key: etcd_endpoints # The CNI network config to install on each node. - name: CNI_NETWORK_CONFIG valueFrom: configMapKeyRef: name: calico-config key: cni_network_config volumeMounts: - mountPath: /host/opt/cni/bin name: cni-bin-dir - mountPath: /host/etc/cni/net.d name: cni-net-dir - mountPath: /calico-secrets name: etcd-certs volumes: # Used by calico/node. - name: lib-modules hostPath: path: /lib/modules - name: var-run-calico hostPath: path: /var/run/calico # Used to install CNI. - name: cni-bin-dir hostPath: path: /opt/cni/bin - name: cni-net-dir hostPath: path: /etc/cni/net.d # Mount in the etcd TLS secrets. - name: etcd-certs secret: secretName: calico-etcd-secrets --- # This manifest deploys the Calico Kubernetes controllers. # See https://github.com/projectcalico/kube-controllers apiVersion: extensions/v1beta1 kind: Deployment metadata: name: calico-kube-controllers namespace: kube-system labels: k8s-app: calico-kube-controllers annotations: scheduler.alpha.kubernetes.io/critical-pod: '' scheduler.alpha.kubernetes.io/tolerations: | [{&quot;key&quot;: &quot;dedicated&quot;, &quot;value&quot;: &quot;master&quot;, &quot;effect&quot;: &quot;NoSchedule&quot; }, {&quot;key&quot;:&quot;CriticalAddonsOnly&quot;, &quot;operator&quot;:&quot;Exists&quot;}] spec: # The controllers can only have a single active instance. replicas: 1 strategy: type: Recreate template: metadata: name: calico-kube-controllers namespace: kube-system labels: k8s-app: calico-kube-controllers spec: # The controllers must run in the host network namespace so that # it isn't governed by policy that would prevent it from working. hostNetwork: true serviceAccountName: calico-kube-controllers containers: - name: calico-kube-controllers image: quay.io/calico/kube-controllers:v1.0.0 env: # The location of the Calico etcd cluster. - name: ETCD_ENDPOINTS valueFrom: configMapKeyRef: name: calico-config key: etcd_endpoints # Location of the CA certificate for etcd. - name: ETCD_CA_CERT_FILE valueFrom: configMapKeyRef: name: calico-config key: etcd_ca # Location of the client key for etcd. - name: ETCD_KEY_FILE valueFrom: configMapKeyRef: name: calico-config key: etcd_key # Location of the client certificate for etcd. - name: ETCD_CERT_FILE valueFrom: configMapKeyRef: name: calico-config key: etcd_cert volumeMounts: # Mount in the etcd TLS secrets. - mountPath: /calico-secrets name: etcd-certs volumes: # Mount in the etcd TLS secrets. - name: etcd-certs secret: secretName: calico-etcd-secrets --- # This deployment turns off the old &quot;policy-controller&quot;. It should remain at 0 replicas, and then # be removed entirely once the new kube-controllers deployment has been deployed above. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: calico-policy-controller namespace: kube-system labels: k8s-app: calico-policy spec: # Turn this deployment off in favor of the kube-controllers deployment above. replicas: 0 strategy: type: Recreate template: metadata: name: calico-policy-controller namespace: kube-system labels: k8s-app: calico-policy spec: hostNetwork: true serviceAccountName: calico-kube-controllers containers: - name: calico-policy-controller image: quay.io/calico/kube-controllers:v1.0.0 env: # The location of the Calico etcd cluster. - name: ETCD_ENDPOINTS valueFrom: configMapKeyRef: name: calico-config key: etcd_endpoints --- apiVersion: v1 kind: ServiceAccount metadata: name: calico-kube-controllers namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: calico-node namespace: kube-system 安装calico 在每个node节点上创建calico所需的目录，并分发证书 cd /opt/ssl for IP in `seq 5 7`;do ssh root@172.29.151.$IP mkdir -p /etc/calico/certs scp ca.pem etcd*.pem root@172.29.151.$IP:/etc/calico/certs/ done 在每个node节点上添加/etc/calico/calico.env文件,注意修改你自己的ip和hostname ETCD_ENDPOINTS=&quot;https://172.29.151.1:2379,https://172.29.151.2:2379,https://172.29.151.3:2379&quot; ETCD_CA_CERT_FILE=&quot;/etc/calico/certs/ca.pem&quot; ETCD_CERT_FILE=&quot;/etc/calico/certs/etcd.pem&quot; ETCD_KEY_FILE=&quot;/etc/calico/certs/etcd-key.pem&quot; CALICO_IP=&quot;172.29.151.5&quot; CALICO_IP6=&quot;&quot; CALICO_LIBNETWORK_ENABLED=&quot;true&quot; CALICO_IPV4POOL_CIDR=&quot;10.233.0.0/16&quot; CALICO_IPV4POOL_IPIP=&quot;always&quot; CALICO_HOSTNAME=&quot;k8s-mds-node01&quot; 在每个node节点上添加calico.service为系统服务 [Unit] Description=calico-node After=docker.service Requires=docker.service [Service] EnvironmentFile=/etc/calico/calico.env ExecStartPre=-/usr/bin/docker rm -f calico-node ExecStart=/usr/bin/docker run --net=host --privileged \ --name=calico-node \ -e ETCD_ENDPOINTS=${ETCD_ENDPOINTS} \ -e ETCD_CA_CERT_FILE=${ETCD_CA_CERT_FILE} \ -e ETCD_CERT_FILE=${ETCD_CERT_FILE} \ -e ETCD_KEY_FILE=${ETCD_KEY_FILE} \ -e HOSTNAME=${CALICO_HOSTNAME} \ -e IP=${CALICO_IP} \ -e IP6=${CALICO_IP6} \ -e CALICO_NETWORKING_BACKEND=${CALICO_NETWORKING_BACKEND} \ -e CALICO_LIBNETWORK_ENABLED=${CALICO_LIBNETWORK_ENABLED} \ -e CALICO_IPV4POOL_CIDR=${CALICO_IPV4POOL_CIDR} \ -e CALICO_IPV4POOL_IPIP=${CALICO_IPV4POOL_IPIP} \ -e CALICO_DISABLE_FILE_LOGGING=${CALICO_DISABLE_FILE_LOGGING} \ -e FELIX_DEFAULTENDPOINTTOHOSTACTION=RETURN \ -e FELIX_IPV6SUPPORT=false \ -e FELIX_LOGSEVERITYSCREEN=info \ -e AS=${CALICO_AS} \ -v /var/log/calico:/var/log/calico \ -v /run/docker/plugins:/run/docker/plugins \ -v /lib/modules:/lib/modules \ -v /var/run/calico:/var/run/calico \ -v /var/run/docker.sock:/var/run/docker.sock \ -v /etc/calico/certs:/etc/calico/certs:ro \ --memory=500M --cpu-shares=300 \ quay.io/calico/node:v2.6.1 Restart=always RestartSec=10s ExecStop=-/usr/bin/docker stop calico-node [Install] WantedBy=multi-user.target 启动calico-node systemctl daemon-reload systemctl enable calico-node systemctl start calico-node systemctl status calico-node 在k8s-console上执行calico安装 $ kubectl apply -f calico.yaml configmap &quot;calico-config&quot; created secret &quot;calico-etcd-secrets&quot; created daemonset &quot;calico-node&quot; created deployment &quot;calico-policy-controller&quot; created serviceaccount &quot;calico-policy-controller&quot; created serviceaccount &quot;calico-node&quot; created $ kubectl apply -f rbac.yaml clusterrole &quot;calico-policy-controller&quot; created clusterrolebinding &quot;calico-policy-controller&quot; created clusterrole &quot;calico-node&quot; created clusterrolebinding &quot;calico-node&quot; created 验证calico $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-3994748863-0dpcp 1/1 Running 0 1h calico-node-74d64 1/1 Running 0 14h calico-node-rbrw3 1/1 Running 0 14h calico-node-vtcrs 1/1 Running 0 14h 安装calicoctl 在k8s-console上下载calicoctl并分发到各个node节点 wget https://github.com/projectcalico/calicoctl/releases/download/v1.6.1/calicoctl chmod +x calicoctl for IP in `seq 5 7`;do scp calicoctl root@172.29.151.$IP:/usr/local/bin/ done 在节点上看看calico的状态 $ calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 172.29.151.6 | node-to-node mesh | up | 12:40:50 | Established | | 172.29.151.7 | node-to-node mesh | up | 12:40:50 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 测试跨主机通信 创建一个nginxdeployment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx 查看创建结果 $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-2214564181-bplwr 1/1 Running 0 3m 10.233.136.3 172.29.151.7 nginx-dm-2214564181-qsl5c 1/1 Running 0 3m 10.233.203.2 172.29.151.6 $ kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-dm 2 2 2 2 4m $ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.254.0.1 &amp;lt;none&amp;gt; 443/TCP 14h nginx-svc 10.254.149.124 &amp;lt;none&amp;gt; 80/TCP 4m 在pod里ping另一个pod $ kubectl exec -ti nginx-dm-2214564181-bplwr /bin/sh / # ping 10.233.203.2 PING 10.233.203.2 (10.233.203.2): 56 data bytes 64 bytes from 10.233.203.2: seq=0 ttl=62 time=0.592 ms 64 bytes from 10.233.203.2: seq=1 ttl=62 time=0.894 ms 64 bytes from 10.233.203.2: seq=2 ttl=62 time=0.559 ms 在node节点上curl测试一下 $ curl 10.254.149.124 &amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt; &amp;lt;style&amp;gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } &amp;lt;/style&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;For online documentation and support please refer to &amp;lt;a href=&quot;http://nginx.org/&quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt; Commercial support is available at &amp;lt;a href=&quot;http://nginx.com/&quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 十三.部署DNS 部署集群dns 获取对应的yaml文件 wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kube-dns.yaml.sed mv kube-dns.yaml.sed kube-dns.yaml 修改如下配置 sed -i 's/$DNS_DOMAIN/cluster.local/gi' kube-dns.yaml sed -i 's/$DNS_SERVER_IP/10.254.0.2/gi' kube-dns.yaml 创建 kubectl create -f kube-dns.yaml 查看创建结果 $ kubectl get pods -n kube-system |grep kube-dns kube-dns-3468831164-2kl0h 3/3 Running 0 14m 进入刚刚创建的nginx pod中访问nginx-svc测试 $ kubectl exec -ti nginx-dm-2214564181-bplwr /bin/sh / # curl nginx-svc &amp;lt;!DOCTYPE html&amp;gt; &amp;lt;html&amp;gt; &amp;lt;head&amp;gt; &amp;lt;title&amp;gt;Welcome to nginx!&amp;lt;/title&amp;gt; &amp;lt;style&amp;gt; body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } &amp;lt;/style&amp;gt; &amp;lt;/head&amp;gt; &amp;lt;body&amp;gt; &amp;lt;h1&amp;gt;Welcome to nginx!&amp;lt;/h1&amp;gt; &amp;lt;p&amp;gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;For online documentation and support please refer to &amp;lt;a href=&quot;http://nginx.org/&quot;&amp;gt;nginx.org&amp;lt;/a&amp;gt;.&amp;lt;br/&amp;gt; Commercial support is available at &amp;lt;a href=&quot;http://nginx.com/&quot;&amp;gt;nginx.com&amp;lt;/a&amp;gt;.&amp;lt;/p&amp;gt; &amp;lt;p&amp;gt;&amp;lt;em&amp;gt;Thank you for using nginx.&amp;lt;/em&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 测试外网 $ kubectl exec -ti nginx-dm-2214564181-bplwr /bin/sh / # curl https://baidu.com &amp;lt;html&amp;gt; &amp;lt;head&amp;gt;&amp;lt;title&amp;gt;302 Found&amp;lt;/title&amp;gt;&amp;lt;/head&amp;gt; &amp;lt;body bgcolor=&quot;white&quot;&amp;gt; &amp;lt;center&amp;gt;&amp;lt;h1&amp;gt;302 Found&amp;lt;/h1&amp;gt;&amp;lt;/center&amp;gt; &amp;lt;hr&amp;gt;&amp;lt;center&amp;gt;bfe/1.0.8.18&amp;lt;/center&amp;gt; &amp;lt;/body&amp;gt; &amp;lt;/html&amp;gt; 部署dns自动扩容 下载对应的yaml文件，不需要任何修改 wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml 在1.7.6中rbac还是beta版本，所以，这里我们要修改文件中的authentication.k8s.io/v1 为 authentication.k8s.io/vibeta1 sed -i 's/rbac.authorization.k8s.io\/v1/rbac.authorization.k8s.io\/v1beta1/gi' dns-horizontal-autoscaler.yaml 创建 kubectl create -f dns-horizontal-autoscaler.yaml 查看创建结果 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-3994748863-0dpcp 1/1 Running 0 7h calico-node-74d64 1/1 Running 0 20h calico-node-rbrw3 1/1 Running 0 20h calico-node-vtcrs 1/1 Running 0 20h kube-dns-3468831164-2kl0h 3/3 Running 0 5h kube-dns-3468831164-zjgzp 3/3 Running 0 13m kube-dns-autoscaler-244676396-bpfpw 1/1 Running 0 13m 十四.kubernetes周边组件配置 kubernetes-dashboard配置 kubernetes基础环境搭建好之后，我们第一步要搭建的就是我们的kubernetes-dashboard 准备所需image gcr.io/google_containers/kubernetes-dashboard-amd64:v1.7.1 gcr.io/google_containers/kubernetes-dashboard-init-amd64:v1.0.0 下载所需yaml文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 为了方便测试，我们在最后添加NodePort，后期如果有了ingress或traffic,再将其去掉即可 kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kube-system spec: type: NodePort ports: - port: 443 targetPort: 8443 nodePort: 30001 selector: k8s-app: kubernetes-dashboard 创建 $ kubectl create -f kubernetes-dashboard.yaml secret &quot;kubernetes-dashboard-certs&quot; created serviceaccount &quot;kubernetes-dashboard&quot; created role &quot;kubernetes-dashboard-minimal&quot; created rolebinding &quot;kubernetes-dashboard-minimal&quot; created deployment &quot;kubernetes-dashboard&quot; created service &quot;kubernetes-dashboard&quot; created 查看创建结果 $ kubectl get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE calico-kube-controllers-3994748863-0dpcp 1/1 Running 1 1d 172.29.151.6 172.29.151.6 calico-node-74d64 1/1 Running 1 1d 172.29.151.6 172.29.151.6 calico-node-rbrw3 1/1 Running 1 1d 172.29.151.5 172.29.151.5 calico-node-vtcrs 1/1 Running 1 1d 172.29.151.7 172.29.151.7 kube-dns-3468831164-2kl0h 3/3 Running 3 23h 10.233.136.10 172.29.151.7 kube-dns-3468831164-zjgzp 3/3 Running 3 18h 10.233.161.7 172.29.151.5 kube-dns-autoscaler-244676396-bpfpw 1/1 Running 1 18h 10.233.136.9 172.29.151.7 kubernetes-dashboard-3625439193-tgtmm 1/1 Running 0 8s 10.233.136.15 172.29.151.7 $ kubectl get svc -n kube-system NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns 10.254.0.2 &amp;lt;none&amp;gt; 53/UDP,53/TCP 23h kubernetes-dashboard 10.254.116.15 &amp;lt;nodes&amp;gt; 443:30001/TCP 1m 最后我们来访问 https://$NODEIP:30001试试，我们发现新的kubernetes提供了认证，就算是skip进去之后，也看不到啥东西 这里我们使用token认证，那么token来自于哪呢，我们创建一个kubernetes-dashboard-rbac.yaml,内容如下 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: dashboard-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: default namespace: kube-system 创建之后，我们来获取它的token值 我们看到这里的serviceaccount是在kube-system的default的，所以我们直接查看kube-system中的default secret就可以了 kubectl create -f kubernetes-dashboard-rbac.yaml $ kubectl describe secret default-token-d9jjg -n kube-system Name: default-token-d9jjg Namespace: kube-system Labels: &amp;lt;none&amp;gt; Annotations: kubernetes.io/service-account.name=default kubernetes.io/service-account.uid=458abfc9-aef6-11e7-aa7b-00155dfa7a1a Type: kubernetes.io/service-account-token Data ==== ca.crt: 1346 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLWQ5ampnIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI0NThhYmZjOS1hZWY2LTExZTctYWE3Yi0wMDE1NWRmYTdhMWEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06ZGVmYXVsdCJ9.gRfeCeQSRPOP7yZ94STPZ8GLb77Gx2wAgyVmyATbyoYR7ZMOgqIOMX0lmgZIzCkA1hFnPHcQ863Q9lW_uvkDbHYWA2B2DrRrdkBOYnq_FF2RM09qrwqspS5u3L0w1vgo7S--Rs-mG-yYnMw0EwBtl9rd6Lx7q59sDvWzU47YoQD3HyYZNuIiaIhuZiugvpkJGeKrrsHpd-wh4_rMcTp0GnUKdqSoIpeth2jvudnu34Wv_Jh5q2rhvhMSgb-qEW7JqB5wnDzXLaxkdW7i5PVDZD5RGCQGDwxqr4opfg53JrJQ9ojEjmR7Q0GfgWyKkudwlBm9nPT0VaW4LJkaM37vpQ 我们输入token登录看看，发现可以看到内容了 heapster kubernetes-dashboard搭建好之后，我们配套的搭建下heapster 准备所需镜像 gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 gcr.io/google_containers/heapster-amd64:v1.3.0 gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 下载所需yaml文件 wget https://github.com/kubernetes/heapster/archive/v1.4.3.tar.gz 进入heapster-1.4.3/deploy/kube-config/influxdb，修改一下grafana.yaml里面的镜像版本，如果你想要通过NodePort查看下grafana的数据测试一下，可以注释掉service中的 type: NodePort gcr.io/google_containers/heapster-grafana-amd64:v4.2.0 -&amp;gt; gcr.io/google_containers/heapster-grafana-amd64:v4.0.2 type: NodePort 执行构建 cd heapster-1.4.3/deploy/kube-config/influxdb kubectl create -f . cd heapster-1.4.3/deploy/kube-config/rbac kubectl create -f . 修改kubernetes-dashboard.yaml 文件，添加如下内容 # - --apiserver-host=http://my-address:port - --heapster-host=http://heapster.kube-system.svc.cluster.local 重新构建kubernetes-dashboard kubectl create -f kubernetes-dashboard.yaml 查看构建结果 $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-3994748863-0dpcp 1/1 Running 1 1d calico-node-74d64 1/1 Running 1 1d calico-node-rbrw3 1/1 Running 2 1d calico-node-vtcrs 1/1 Running 1 1d heapster-84017538-54dkm 1/1 Running 0 1h kube-dns-3468831164-2kl0h 3/3 Running 3 1d kube-dns-3468831164-9hsbm 3/3 Running 0 3h kube-dns-autoscaler-244676396-bpfpw 1/1 Running 1 22h kubernetes-dashboard-2923351285-pzgx5 1/1 Running 0 24m monitoring-grafana-2115417091-lgqsc 1/1 Running 0 1h monitoring-influxdb-3570645011-dp51l 1/1 Running 0 1h 登录kubernetes-dashboard，查看是否有数据了 登录到grafana查看数据 ingress 配置 见第二章</summary></entry><entry><title type="html">kubernetes ceph 笔记 3</title><link href="https://kevinguo.me/2017/09/20/kubernetes-ceph-3/" rel="alternate" type="text/html" title="kubernetes ceph 笔记 3" /><published>2017-09-20T00:00:00+08:00</published><updated>2017-09-20T00:00:00+08:00</updated><id>https://kevinguo.me/2017/09/20/kubernetes-ceph-3</id><content type="html" xml:base="https://kevinguo.me/2017/09/20/kubernetes-ceph-3/">&lt;blockquote&gt;
  &lt;p&gt;前面花了两章的时间介绍了ceph存储集群，简单的讲了ceph的组件、架构、寻址过程以及关于rbd,cephfs,cephGW,rados,osd,mon,mds,pool,pg,object等的操作过程，这一章主要记录下kubernetes使用ceph的相关配置过程。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;通过&lt;a href=&quot;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#ceph-rbd&quot;&gt;官网&lt;/a&gt;，我们发现在kubernetes中使用的是ceph的RBD&lt;/p&gt;

&lt;h3 id=&quot;部署集群&quot;&gt;部署集群&lt;/h3&gt;

&lt;p&gt;具体的集群部署这里就不在赘述，请参考&lt;a href=&quot;https://kevinguo.me/2017/09/06/kubernetes-ceph-1/&quot;&gt;kubernetes ceph 笔记 1&lt;/a&gt;、&lt;a href=&quot;https://kevinguo.me/2017/09/12/kubernetes-ceph-2/&quot;&gt;kubernetes ceph 笔记 2&lt;/a&gt;，这里我们只是额外添加一组实验所需的osd，命令如下&lt;/p&gt;

&lt;h5 id=&quot;添加osd&quot;&gt;添加osd&lt;/h5&gt;

&lt;p&gt;为每台机器添加一个sdc的硬盘，我这里用目录代替硬盘&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 在每台osd节点上执行&lt;/span&gt;
sudo mkdir -p /data/sdc
chown ceph.ceph /data/sdc

&lt;span class=&quot;c&quot;&gt;#在管理节点上执行`ceph-deploy`来准备OSD&lt;/span&gt;
ceph-deploy osd prepare k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc

&lt;span class=&quot;c&quot;&gt;# 激活OSD&lt;/span&gt;
ceph-deploy osd activate k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc

&lt;span class=&quot;c&quot;&gt;# 检测集群状态&lt;/span&gt;
ceph halth
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;创建rbd&quot;&gt;创建RBD&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 创建存储池&lt;/span&gt;
rados mkpool data

&lt;span class=&quot;c&quot;&gt;# 创建image&lt;/span&gt;
rbd create data --size 10G -p data

&lt;span class=&quot;c&quot;&gt;# 关闭不支持特性&lt;/span&gt;
rbd feature disable data exclusive-lock, object-map, fast-diff, deep-flatten -p data

&lt;span class=&quot;c&quot;&gt;# 映射image到块设备(每个节点都需要隐射)&lt;/span&gt;
rbd map data --name client.admin -p data

&lt;span class=&quot;c&quot;&gt;# 格式化块设备&lt;/span&gt;
mkfs.xfs /dev/rbd0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;kubernetes-使用ceph&quot;&gt;Kubernetes 使用Ceph&lt;/h3&gt;

&lt;h4 id=&quot;pv--pvc方式&quot;&gt;PV &amp;amp; PVC方式&lt;/h4&gt;

&lt;p&gt;传统使用分布式存储的方式一般为 &lt;code class=&quot;highlighter-rouge&quot;&gt;PV &amp;amp; PVC&lt;/code&gt; 的方式，也就是说管理员必须预先创建好PV 和 PVC ，然后对应的deployment或者replication挂载PVC来使用&lt;/p&gt;

&lt;h5 id=&quot;创建secret&quot;&gt;创建secret&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 获取管理 key 并进行 base64 编码&lt;/span&gt;
ceph auth get-key client.admin | base64

&lt;span class=&quot;c&quot;&gt;# 创建一个secret 配置(key 为上一条命令生成)&lt;/span&gt;
vim ceph-secret.yml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
data:
  key: &lt;span class=&quot;nv&quot;&gt;QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 创建secret&lt;/span&gt;
kubectl create -f ceph-secret.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;创建pv&quot;&gt;创建PV&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#创建ceph-pv文件&lt;/span&gt;
vim ceph-pv.yml

apiVersion: v1
kind: PersistentVolume
metadata:
  name: ceph-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteMany
  rbd:
    monitors:
      - 172.30.33.90:6789
      - 172.30.33.91:6789
      - 172.30.33.92:6789
    pool: data
    image: data
    user: admin
    secretRef:
      name: ceph-secret
    fsType: xfs
    readOnly: &lt;span class=&quot;nb&quot;&gt;false
  &lt;/span&gt;persistentVolumeReclaimPolicy: Recycle

&lt;span class=&quot;c&quot;&gt;# 创建PV&lt;/span&gt;
kubectl create -f ceph-pv.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;创建pvc&quot;&gt;创建PVC&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 新建ceph-pvc.yml文件&lt;/span&gt;
vim ceph-pvc.yml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ceph-pvc
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi

&lt;span class=&quot;c&quot;&gt;# 创建PVC&lt;/span&gt;
kubectl create -f ceph-pvc.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;创建一个测试的deployment来挂载&quot;&gt;创建一个测试的deployment来挂载&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 新建nginx.yml&lt;/span&gt;
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: ceph-nginx
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: ceph-nginx
    spec:
      containers:
      - name: ceph-nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - mountPath: &lt;span class=&quot;s2&quot;&gt;&quot;/data&quot;&lt;/span&gt;
            name: data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: ceph-pvc

&lt;span class=&quot;c&quot;&gt;# 创建nginx deployment&lt;/span&gt;
kubectl create -f ceph-nginx.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;配置k8s-node&quot;&gt;配置k8s node&lt;/h5&gt;

&lt;p&gt;我们创建好PV 和 PVC之后，进行查看时可能会出现&lt;code class=&quot;highlighter-rouge&quot;&gt;with: rbd: failed to modprobe rbd error:exit status 1&lt;/code&gt;的报错，所以这时候我们需要对所有k8s-node进行如下配置&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 在所有k8s node上安装ceph-common&lt;/span&gt;
yum install -y ceph-common

&lt;span class=&quot;c&quot;&gt;# 拷贝ceph.conf和ceph.client.admin.keyring到/etc/ceph/目录下&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 配置kubelet有关ceph的参数，增加如下内容&lt;/span&gt;
vim /usr/local/bin/kubelet

-v /sbin/modprobe:/sbin/modprobe:ro &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-v /lib/modules:/lib/modules:ro &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
-v /etc/ceph:/etc/ceph:ro &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 重启kubelet&lt;/span&gt;
systemctl restart kubelet

&lt;span class=&quot;c&quot;&gt;# 查看pod是否启动成功&lt;/span&gt;
kubectl get pods
NAME                          READY     STATUS    RESTARTS   AGE
ceph-nginx-2497831062-569lw   1/1       Running   0          15m
ceph-nginx-2497831062-589j9   1/1       Running   0          59m
ceph-nginx-2497831062-5t01s   1/1       Running   0          12m

&lt;span class=&quot;c&quot;&gt;# 然后进入其中一个pod，写入一个1G的文件&lt;/span&gt;
kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -ti ceph-nginx-2497831062-569lw  /bin/bash

dd &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-file &lt;span class=&quot;nv&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1G &lt;span class=&quot;nv&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

&lt;span class=&quot;c&quot;&gt;# 然后查看是否已经占用了rbd中的空间呢&lt;/span&gt;
ceph df
GLOBAL:
    SIZE     AVAIL     RAW USED     %RAW USED
    575G      344G         230G         40.09
POOLS:
    NAME                      ID     USED      %USED     MAX AVAIL     OBJECTS
    data                      14     1038M      1.50        68277M         280

&lt;span class=&quot;c&quot;&gt;# 然后我们删除这个pod&lt;/span&gt;
kubectl delete pods ceph-nginx-2497831062-569lw

&lt;span class=&quot;c&quot;&gt;# 查看新的pod,发现文件依旧在&lt;/span&gt;
kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -ti ceph-nginx-2497831062-rgkcl ls /data
&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-file
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;storageclass方式之statefulset&quot;&gt;StorageClass方式之StatefulSet&lt;/h4&gt;

&lt;p&gt;重头戏来了，洋洋洒洒写了近3篇文章，最终就是要使用这个StorageClass这个东西；这个东西在前面的&lt;a href=&quot;https://kevinguo.me/2017/09/01/kubernetes-one-section/#%E5%8A%A8%E6%80%81&quot;&gt;kubernetes入门&lt;/a&gt;有简单的提到过，就是说动态创建PV，不用再事先固定PV的大小，直接创建PVC即可分配使用。&lt;/p&gt;

&lt;h5 id=&quot;创建secret-1&quot;&gt;创建secret&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 获取管理 key 并进行 base64 编码&lt;/span&gt;
ceph auth get-key client.admin | base64

&lt;span class=&quot;c&quot;&gt;# 创建一个secret 配置(key 为上一条命令生成)&lt;/span&gt;
vim ceph-storageclass-secret.yml

apiVersion: v1
kind: Secret
metadata:
  name: ceph-storageclass-secret
data:
  key: &lt;span class=&quot;nv&quot;&gt;QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: kubernetes.io/rbd

&lt;span class=&quot;c&quot;&gt;# 创建一个namespace的secret&lt;/span&gt;
vim ceph-storageclass-secret-system.yml
apiVersion: v1
kind: Secret
metadata:
  name: ceph-storageclass-secret
  namespace: kube-system
data:
  key: &lt;span class=&quot;nv&quot;&gt;QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: kubernetes.io/rbd

&lt;span class=&quot;c&quot;&gt;# 创建secret&lt;/span&gt;
kubectl create -f ceph-storageclass-secret.yml
kubectl create -f ceph-storageclass-secret-system.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;创建一个storageclass&quot;&gt;创建一个storageclass&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 新建ceph-storageclass.yml文件&lt;/span&gt;
vim ceph-storageclass.yml

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ceph-storageclass
provisioner: kubernetes.io/rbd
parameters:
  monitors: 172.30.33.90:6789,172.30.33.91:6789,172.30.33.92:6789
  adminId: admin
  adminSecretName: ceph-storageclass-secret
  adminSecretNamespace: kube-system
  pool: data
  userId: admin
  userSecretName: ceph-storageclass-secret

&lt;span class=&quot;c&quot;&gt;# 新建storageclass&lt;/span&gt;
kubectl create -f ceph-storageclass.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;创建statefulset&quot;&gt;创建statefulset&lt;/h5&gt;

&lt;p&gt;我们在使用StorageClass的时候，可以自己手动创建PVC，然后所有pods共享一个pvc；也可以定义&lt;code class=&quot;highlighter-rouge&quot;&gt;volumeClaimTemplates&lt;/code&gt;来为自动为每个pod创建一个单独的pvc，如下所示&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 新建ceph-storageclass-nginx.yml&lt;/span&gt;
vim ceph-storageclass-nginx.yml

apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: ceph-storageclass-nginx
spec:
  serviceName: &lt;span class=&quot;s2&quot;&gt;&quot;ceph-storageclass-nginx-service&quot;&lt;/span&gt;
  replicas: 3
  template:
    metadata:
      labels:
        app: ceph-storageclass-nginx
    spec:
      containers:
      - name: ceph-storageclass-nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - mountPath: &lt;span class=&quot;s2&quot;&gt;&quot;/data&quot;&lt;/span&gt;
            name: data
  volumeClaimTemplates:
  - metadata:
      name: data
      annotations:
        volume.beta.kubernetes.io/storage-class: ceph-storageclass-pvc
    spec:
      accessModes: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ReadWriteOnce&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
      resources:
        requests:
          storage: 5Gi

&lt;span class=&quot;c&quot;&gt;# 新建ceph-storageclass-nginx-service.yml&lt;/span&gt;
vim ce ph-storageclass-nginx-service.yml

apiVersion: v1
kind: Service
metadata:
  name: ceph-storageclass-nginx-service
  labels:
    app: ceph-storageclass-nginx-service
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: ceph-storageclass-nginx

&lt;span class=&quot;c&quot;&gt;# 创建statefulSet&lt;/span&gt;
kubectl create -f ceph-storageclass-nginx.yml
kubectl create -f ceph-storageclass-nginx-service.yml

&lt;span class=&quot;c&quot;&gt;# 查看pods&lt;/span&gt;
kubectl get pods
NAME                          READY     STATUS    RESTARTS   AGE
ceph-storageclass-nginx-0     1/1       Running   0          11m
ceph-storageclass-nginx-1     1/1       Running   0          11m
ceph-storageclass-nginx-2     1/1       Running   0          11m

&lt;span class=&quot;c&quot;&gt;# 查看自动创建的pv和pvc&lt;/span&gt;
kubectl get pv
NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                                    STORAGECLASS        REASON    AGE
pvc-1af65cba-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-0   ceph-storageclass             15m
pvc-934ed5ad-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-1   ceph-storageclass             12m
pvc-9ff7359d-9dca-11e7-84a9-00155d201312   5Gi        RWO           Delete          Bound     default/data-ceph-storageclass-nginx-2   ceph-storageclass             11m


kubectl get pvc
NAME                             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGE
data-ceph-storageclass-nginx-0   Bound     pvc-1af65cba-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   15m
data-ceph-storageclass-nginx-1   Bound     pvc-934ed5ad-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   12m
data-ceph-storageclass-nginx-2   Bound     pvc-9ff7359d-9dca-11e7-84a9-00155d201312   5Gi        RWO           ceph-storageclass   11m

&lt;span class=&quot;c&quot;&gt;# 进入pod查看使用情况，发现/data使用大小5G&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;root@k8s-master01 k8s-quark]# kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -ti ceph-storageclass-nginx-1 -- df -Th
Filesystem                                                                                      Type   Size  Used Avail Use% Mounted on
/dev/rbd1                                                                                       ext4   4.8G   10M  4.6G   1% /data
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;storageclass方式之deployment&quot;&gt;StorageClass方式之Deployment&lt;/h4&gt;

&lt;p&gt;我们接着使用上面创建的StorageClass，只不过这个时候我们需要手动来创建一个PVC&lt;/p&gt;

&lt;h5 id=&quot;创建pvc-1&quot;&gt;创建PVC&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 新建ceph-storageclass-pvc.yml&lt;/span&gt;
vim ceph-storageclass-pvc.yml

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pvc
  annotations:
    volume.beta.kubernetes.io/storage-class: ceph-storageclass
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Gi

&lt;span class=&quot;c&quot;&gt;# 创建PVC&lt;/span&gt;
kubectl create -f ceph-storageclass-pvc.yml
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h5 id=&quot;创建deployment&quot;&gt;创建deployment&lt;/h5&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 新建ceph-storageclass-nginx-deployment.yml&lt;/span&gt;
vim ceph-storageclass-nginx-deployment.yml

apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: ceph-nginx
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: ceph-nginx
    spec:
      containers:
      - name: ceph-nginx
        image: nginx
        ports:
        - containerPort: 80
        volumeMounts:
          - mountPath: &lt;span class=&quot;s2&quot;&gt;&quot;/data&quot;&lt;/span&gt;
            name: data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pvc

&lt;span class=&quot;c&quot;&gt;# 创建deployment&lt;/span&gt;
kubectl create -f ceph-storageclass-nginx-deployment.yml

&lt;span class=&quot;c&quot;&gt;# 查看pods&lt;/span&gt;
kubectl get pods
NAME                          READY     STATUS    RESTARTS   AGE
ceph-nginx-3206996150-29q7j   1/1       Running   0          7m
ceph-nginx-3206996150-94tzk   1/1       Running   0          7m
ceph-nginx-3206996150-xvkzh   1/1       Running   0          7m

&lt;span class=&quot;c&quot;&gt;# 查看PV和PVC&lt;/span&gt;
kubectl get pvc
NAME                             STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGE
&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pvc                         Bound     pvc-76a48238-9dcf-11e7-84a9-00155d201312   50Gi       RWX           ceph-storageclass   9m

kubectl get pv
NAME                                       CAPACITY   ACCESSMODES   RECLAIMPOLICY   STATUS    CLAIM                                    STORAGECLASS        REASON    AGE
pvc-76a48238-9dcf-11e7-84a9-00155d201312   50Gi       RWX           Delete          Bound     default/test-pvc                         ceph-storageclass             10m

&lt;span class=&quot;c&quot;&gt;# 进入pod 查看使用情况，看到/data总共50G&lt;/span&gt;
kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; -ti ceph-nginx-3206996150-29q7j -- df -Th
Filesystem                                                                                      Type   Size  Used Avail Use% Mounted on
/dev/rbd0                                                                                       ext4    50G   52M   47G   1% /data
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;至此kubernetes结合ceph RBD的实验基本上已经完成，我们发现，storageclass确实是个好东西，省去了创建PV的步骤，并且，可以根据PVC中定义的class来选择创建不同的PVC&lt;/p&gt;</content><author><name>KevinGuo</name></author><summary type="html">前面花了两章的时间介绍了ceph存储集群，简单的讲了ceph的组件、架构、寻址过程以及关于rbd,cephfs,cephGW,rados,osd,mon,mds,pool,pg,object等的操作过程，这一章主要记录下kubernetes使用ceph的相关配置过程。 通过官网，我们发现在kubernetes中使用的是ceph的RBD 部署集群 具体的集群部署这里就不在赘述，请参考kubernetes ceph 笔记 1、kubernetes ceph 笔记 2，这里我们只是额外添加一组实验所需的osd，命令如下 添加osd 为每台机器添加一个sdc的硬盘，我这里用目录代替硬盘 # 在每台osd节点上执行 sudo mkdir -p /data/sdc chown ceph.ceph /data/sdc #在管理节点上执行`ceph-deploy`来准备OSD ceph-deploy osd prepare k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc # 激活OSD ceph-deploy osd activate k8s-master01:/data/sdc k8s-master02:/data/sdc k8s-master03:/data/sdc k8s-node01:/data/sdc k8s-node02:/data/sdc k8s-registry:/data/sdc # 检测集群状态 ceph halth 创建RBD # 创建存储池 rados mkpool data # 创建image rbd create data --size 10G -p data # 关闭不支持特性 rbd feature disable data exclusive-lock, object-map, fast-diff, deep-flatten -p data # 映射image到块设备(每个节点都需要隐射) rbd map data --name client.admin -p data # 格式化块设备 mkfs.xfs /dev/rbd0 Kubernetes 使用Ceph PV &amp;amp; PVC方式 传统使用分布式存储的方式一般为 PV &amp;amp; PVC 的方式，也就是说管理员必须预先创建好PV 和 PVC ，然后对应的deployment或者replication挂载PVC来使用 创建secret # 获取管理 key 并进行 base64 编码 ceph auth get-key client.admin | base64 # 创建一个secret 配置(key 为上一条命令生成) vim ceph-secret.yml apiVersion: v1 kind: Secret metadata: name: ceph-secret data: key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ== # 创建secret kubectl create -f ceph-secret.yml 创建PV #创建ceph-pv文件 vim ceph-pv.yml apiVersion: v1 kind: PersistentVolume metadata: name: ceph-pv spec: capacity: storage: 2Gi accessModes: - ReadWriteMany rbd: monitors: - 172.30.33.90:6789 - 172.30.33.91:6789 - 172.30.33.92:6789 pool: data image: data user: admin secretRef: name: ceph-secret fsType: xfs readOnly: false persistentVolumeReclaimPolicy: Recycle # 创建PV kubectl create -f ceph-pv.yml 创建PVC # 新建ceph-pvc.yml文件 vim ceph-pvc.yml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: ceph-pvc spec: accessModes: - ReadWriteMany resources: requests: storage: 2Gi # 创建PVC kubectl create -f ceph-pvc.yml 创建一个测试的deployment来挂载 # 新建nginx.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: ceph-nginx spec: replicas: 3 template: metadata: labels: app: ceph-nginx spec: containers: - name: ceph-nginx image: nginx ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/data&quot; name: data volumes: - name: data persistentVolumeClaim: claimName: ceph-pvc # 创建nginx deployment kubectl create -f ceph-nginx.yml 配置k8s node 我们创建好PV 和 PVC之后，进行查看时可能会出现with: rbd: failed to modprobe rbd error:exit status 1的报错，所以这时候我们需要对所有k8s-node进行如下配置 # 在所有k8s node上安装ceph-common yum install -y ceph-common # 拷贝ceph.conf和ceph.client.admin.keyring到/etc/ceph/目录下 # 配置kubelet有关ceph的参数，增加如下内容 vim /usr/local/bin/kubelet -v /sbin/modprobe:/sbin/modprobe:ro \ -v /lib/modules:/lib/modules:ro \ -v /etc/ceph:/etc/ceph:ro \ # 重启kubelet systemctl restart kubelet # 查看pod是否启动成功 kubectl get pods NAME READY STATUS RESTARTS AGE ceph-nginx-2497831062-569lw 1/1 Running 0 15m ceph-nginx-2497831062-589j9 1/1 Running 0 59m ceph-nginx-2497831062-5t01s 1/1 Running 0 12m # 然后进入其中一个pod，写入一个1G的文件 kubectl exec -ti ceph-nginx-2497831062-569lw /bin/bash dd if=/dev/zero of=test-file bs=1G count=1 # 然后查看是否已经占用了rbd中的空间呢 ceph df GLOBAL: SIZE AVAIL RAW USED %RAW USED 575G 344G 230G 40.09 POOLS: NAME ID USED %USED MAX AVAIL OBJECTS data 14 1038M 1.50 68277M 280 # 然后我们删除这个pod kubectl delete pods ceph-nginx-2497831062-569lw # 查看新的pod,发现文件依旧在 kubectl exec -ti ceph-nginx-2497831062-rgkcl ls /data test-file StorageClass方式之StatefulSet 重头戏来了，洋洋洒洒写了近3篇文章，最终就是要使用这个StorageClass这个东西；这个东西在前面的kubernetes入门有简单的提到过，就是说动态创建PV，不用再事先固定PV的大小，直接创建PVC即可分配使用。 创建secret # 获取管理 key 并进行 base64 编码 ceph auth get-key client.admin | base64 # 创建一个secret 配置(key 为上一条命令生成) vim ceph-storageclass-secret.yml apiVersion: v1 kind: Secret metadata: name: ceph-storageclass-secret data: key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ== type: kubernetes.io/rbd # 创建一个namespace的secret vim ceph-storageclass-secret-system.yml apiVersion: v1 kind: Secret metadata: name: ceph-storageclass-secret namespace: kube-system data: key: QVFCZmVyWlpFS1hGTHhBQWhsekVscG0yTWhoYkJHQjRUbk5Wa0E9PQ== type: kubernetes.io/rbd # 创建secret kubectl create -f ceph-storageclass-secret.yml kubectl create -f ceph-storageclass-secret-system.yml 创建一个storageclass # 新建ceph-storageclass.yml文件 vim ceph-storageclass.yml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: ceph-storageclass provisioner: kubernetes.io/rbd parameters: monitors: 172.30.33.90:6789,172.30.33.91:6789,172.30.33.92:6789 adminId: admin adminSecretName: ceph-storageclass-secret adminSecretNamespace: kube-system pool: data userId: admin userSecretName: ceph-storageclass-secret # 新建storageclass kubectl create -f ceph-storageclass.yml 创建statefulset 我们在使用StorageClass的时候，可以自己手动创建PVC，然后所有pods共享一个pvc；也可以定义volumeClaimTemplates来为自动为每个pod创建一个单独的pvc，如下所示 # 新建ceph-storageclass-nginx.yml vim ceph-storageclass-nginx.yml apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: ceph-storageclass-nginx spec: serviceName: &quot;ceph-storageclass-nginx-service&quot; replicas: 3 template: metadata: labels: app: ceph-storageclass-nginx spec: containers: - name: ceph-storageclass-nginx image: nginx ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/data&quot; name: data volumeClaimTemplates: - metadata: name: data annotations: volume.beta.kubernetes.io/storage-class: ceph-storageclass-pvc spec: accessModes: [&quot;ReadWriteOnce&quot;] resources: requests: storage: 5Gi # 新建ceph-storageclass-nginx-service.yml vim ce ph-storageclass-nginx-service.yml apiVersion: v1 kind: Service metadata: name: ceph-storageclass-nginx-service labels: app: ceph-storageclass-nginx-service spec: ports: - port: 80 name: web clusterIP: None selector: app: ceph-storageclass-nginx # 创建statefulSet kubectl create -f ceph-storageclass-nginx.yml kubectl create -f ceph-storageclass-nginx-service.yml # 查看pods kubectl get pods NAME READY STATUS RESTARTS AGE ceph-storageclass-nginx-0 1/1 Running 0 11m ceph-storageclass-nginx-1 1/1 Running 0 11m ceph-storageclass-nginx-2 1/1 Running 0 11m # 查看自动创建的pv和pvc kubectl get pv NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-1af65cba-9dca-11e7-84a9-00155d201312 5Gi RWO Delete Bound default/data-ceph-storageclass-nginx-0 ceph-storageclass 15m pvc-934ed5ad-9dca-11e7-84a9-00155d201312 5Gi RWO Delete Bound default/data-ceph-storageclass-nginx-1 ceph-storageclass 12m pvc-9ff7359d-9dca-11e7-84a9-00155d201312 5Gi RWO Delete Bound default/data-ceph-storageclass-nginx-2 ceph-storageclass 11m kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE data-ceph-storageclass-nginx-0 Bound pvc-1af65cba-9dca-11e7-84a9-00155d201312 5Gi RWO ceph-storageclass 15m data-ceph-storageclass-nginx-1 Bound pvc-934ed5ad-9dca-11e7-84a9-00155d201312 5Gi RWO ceph-storageclass 12m data-ceph-storageclass-nginx-2 Bound pvc-9ff7359d-9dca-11e7-84a9-00155d201312 5Gi RWO ceph-storageclass 11m # 进入pod查看使用情况，发现/data使用大小5G [root@k8s-master01 k8s-quark]# kubectl exec -ti ceph-storageclass-nginx-1 -- df -Th Filesystem Type Size Used Avail Use% Mounted on /dev/rbd1 ext4 4.8G 10M 4.6G 1% /data StorageClass方式之Deployment 我们接着使用上面创建的StorageClass，只不过这个时候我们需要手动来创建一个PVC 创建PVC # 新建ceph-storageclass-pvc.yml vim ceph-storageclass-pvc.yml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-pvc annotations: volume.beta.kubernetes.io/storage-class: ceph-storageclass spec: accessModes: - ReadWriteMany resources: requests: storage: 50Gi # 创建PVC kubectl create -f ceph-storageclass-pvc.yml 创建deployment # 新建ceph-storageclass-nginx-deployment.yml vim ceph-storageclass-nginx-deployment.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: ceph-nginx spec: replicas: 3 template: metadata: labels: app: ceph-nginx spec: containers: - name: ceph-nginx image: nginx ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/data&quot; name: data volumes: - name: data persistentVolumeClaim: claimName: test-pvc # 创建deployment kubectl create -f ceph-storageclass-nginx-deployment.yml # 查看pods kubectl get pods NAME READY STATUS RESTARTS AGE ceph-nginx-3206996150-29q7j 1/1 Running 0 7m ceph-nginx-3206996150-94tzk 1/1 Running 0 7m ceph-nginx-3206996150-xvkzh 1/1 Running 0 7m # 查看PV和PVC kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE test-pvc Bound pvc-76a48238-9dcf-11e7-84a9-00155d201312 50Gi RWX ceph-storageclass 9m kubectl get pv NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-76a48238-9dcf-11e7-84a9-00155d201312 50Gi RWX Delete Bound default/test-pvc ceph-storageclass 10m # 进入pod 查看使用情况，看到/data总共50G kubectl exec -ti ceph-nginx-3206996150-29q7j -- df -Th Filesystem Type Size Used Avail Use% Mounted on /dev/rbd0 ext4 50G 52M 47G 1% /data 至此kubernetes结合ceph RBD的实验基本上已经完成，我们发现，storageclass确实是个好东西，省去了创建PV的步骤，并且，可以根据PVC中定义的class来选择创建不同的PVC</summary></entry><entry><title type="html">kubernetes ceph 笔记 2</title><link href="https://kevinguo.me/2017/09/12/kubernetes-ceph-2/" rel="alternate" type="text/html" title="kubernetes ceph 笔记 2" /><published>2017-09-12T00:00:00+08:00</published><updated>2017-09-12T00:00:00+08:00</updated><id>https://kevinguo.me/2017/09/12/kubernetes-ceph-2</id><content type="html" xml:base="https://kevinguo.me/2017/09/12/kubernetes-ceph-2/">&lt;blockquote&gt;
  &lt;p&gt;其实我们在上一章的时候，已经简单的介绍了ceph中的基础组件以及其他软体架构，那么这一章，主要是来详细的介绍下ceph的工作原理、流程以及更实际的操作，比如ceph 内部的CRUSH bucket调整，PG/PGP参数调整等。并在最后简单的介绍了些CEPH集群硬件要求等。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;寻址流程&quot;&gt;寻址流程&lt;/h3&gt;

&lt;p&gt;通过对Ceph系统中寻址流程的了解来熟悉Ceph中的几个概念，寻址流程如下图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/Addressing.png&quot; alt=&quot;Addressing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;上图的几个概念说明如下：&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;1file&quot;&gt;1.File&lt;/h4&gt;

&lt;p&gt;此处的file就是用户需要存储或者访问的文件。对于一个基于Ceph开发的对象存储而言，这个file也就对应于应用中的”对象”，也就是用户直接操作的“对象”。&lt;/p&gt;

&lt;h4 id=&quot;2object&quot;&gt;2.Object&lt;/h4&gt;

&lt;p&gt;Object是Ceph中最小存储单元，是由一个数据和一个元数据绑定的整体，元数据中存放了具体数据的属性信息等。Object和File区别在于，object的最大size由RADOS限定（通常为2MB或者4MB），以便实现底层存储的组织管理。因此，当上层应用向RADOS存入size很大的File时，需要将File切分成统一大小的一系列Object(最后一个大小可以不同)进行存储。&lt;/p&gt;

&lt;h4 id=&quot;3pg&quot;&gt;3.PG&lt;/h4&gt;

&lt;p&gt;PG的用途就是对Object的存储进行组织和位置映射。具体而言，一个PG负责组织若干个object(可以为数千个甚至更多)，但一个object只能被映射到一个PG中，即PG和object之间是“一对多”映射关系。同时，一个PG会被映射到n个OSD上，而每个OSD上都会承载大量的PG，即PG和OSD之间是“多对多”映射关系。在实践中，n至少为2，如果用于生产环境，则至少为3。一个OSD上的PG则可达到数百个。事实上，PG数量的设置牵扯到数据分布的均匀性问题。这一点后面再讲。&lt;/p&gt;

&lt;h4 id=&quot;4osd&quot;&gt;4.OSD&lt;/h4&gt;

&lt;p&gt;这个&lt;a href=&quot;https://kevinguo.me/2017/09/06/kubernetes-ceph-1/&quot;&gt;上一篇&lt;/a&gt;已经讲过了，这里就不再赘述。唯一需要说明的是，OSD的数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。在实践中，至少也得数十上百个的量级才有助于Ceph系统的设计发挥其应有的优势。&lt;/p&gt;

&lt;h4 id=&quot;5crush&quot;&gt;5.CRUSH&lt;/h4&gt;

&lt;p&gt;在传统的文件存储系统中，元数据占着及其重要的位置，每次系统中数据更新时，元数据会首先被更新，然后才是实际数据的写入；在较小的存储系统中(GB/TB)，这种将元数据存储在某个固定几点或者磁盘阵列中的做法还能满足需求；但当数据量增大到PB/ZB时，元数据查找的性能会成为很大的一个瓶颈；而且元数据的统一存放还可能造成单点故障，即当元数据丢失后，实际数据将无法被找回；与传统文件存储系统不同的是，Ceph使用&lt;code class=&quot;highlighter-rouge&quot;&gt;CRUSH&lt;/code&gt;算法来精确的计算数据应该被写入那里/从哪里读取；CRUSH按需计算元数据，而不是存储元数据，从而解决了传统文件存储系统的瓶颈。&lt;/p&gt;

&lt;h4 id=&quot;6crush计算&quot;&gt;6.CRUSH计算&lt;/h4&gt;

&lt;p&gt;CRUSH是伪随机算法，算法通过每个设备的权重(容量大小)来计算数据对象的分布。对象分布式由cluster map和data distribution policy决定的。
cluster map由device和bucket组成。描述了可用存储资源和层级结构(比如有多少个机架，每个机架上有多少个服务器，每个服务器上有多少个磁盘)。
data distribution policy由 placement rules组成。rule决定了每个数据对象有多少个副本，这些副本存储的限制条件(比如3个副本放在不同的机架中)。&lt;/p&gt;

&lt;p&gt;在Ceph中，元数据的计算和负载也是分布式的，并且只有在需要的时候才会执行，元数据的计算过程称之为CRUSH计算，不同于其他分布式文件系统，Ceph的CRUSH计算是由客户端使用自己的资源来完成，从而去除了中心查找带来的性能及单节点故障问题。&lt;/p&gt;

&lt;p&gt;CRUSH查找时候，ceph client先通过MON 获取cluster map，然后获取到通过Hash算法算出的PG ID，最后再根据CRUSH算法计算出PG中，主和次OSD的ID，最终找到OSD的位置，进行数据的读写。&lt;/p&gt;

&lt;h4 id=&quot;7层级的cluster-map&quot;&gt;7.层级的Cluster map&lt;/h4&gt;

&lt;p&gt;在Ceph中，CRUSH是可以支持各种基础设施和用户自定义的；CRUSH设备列表中预先定义了一系列的设备，用户可以通过自定义配置将不同的OSD分配到不同的区域。这样就在物理上避免了所有数据都在同一个机架上，防止某个机架突然塌了以后数据全部丢失。&lt;/p&gt;

&lt;h4 id=&quot;8恢复和再平衡&quot;&gt;8.恢复和再平衡&lt;/h4&gt;

&lt;p&gt;在Ceph集群中，如果OSD挂了，且老师处于&lt;code class=&quot;highlighter-rouge&quot;&gt;degraded&lt;/code&gt;状态，Ceph 都会将其标记为 down 和 out 状态；然后默认情况下 Ceph 会等待 300秒之后进行数据恢复和再平衡，这个值可以通过在配置文件中的 mon osd down out interval 参数来调整&lt;/p&gt;

&lt;h4 id=&quot;总结&quot;&gt;总结&lt;/h4&gt;

&lt;p&gt;基于上述的定义，便可以对寻址流程进行解释了。具体而言，Ceph中的寻址至少要经历三次映射：&lt;/p&gt;

&lt;p&gt;1.File -&amp;gt; Object&lt;/p&gt;

&lt;p&gt;将用户操作的File，映射为RADOS能处理的object，每个object都会有一个唯一的oid。本质上就是按照object的最大size对file进行切分，切分成大小一致的object(最后的大小可以不一样)&lt;/p&gt;

&lt;p&gt;2.Object -&amp;gt; PG&lt;/p&gt;

&lt;p&gt;当File被映射为一个或多个object后，就需要将object映射到PG中,得到PG ID。这有一个计算公式&lt;/p&gt;

&lt;p&gt;hash(oid) &amp;amp; mask –&amp;gt; pgid&lt;/p&gt;

&lt;p&gt;由此可见，其计算由两步组成。首先是使用Ceph指定的静态hash算法计算出oid的hash值。然后将这个随机值和mask(mask的值=PG数-1)按位相与，最终得到PG ID。&lt;/p&gt;

&lt;p&gt;3.PG -&amp;gt; OSD&lt;/p&gt;

&lt;p&gt;最后根据前面得到的PG ID，通过CRUSH算法最终找到OSD的位置。&lt;/p&gt;

&lt;h3 id=&quot;ceph组件调整及操作&quot;&gt;Ceph组件调整及操作&lt;/h3&gt;

&lt;p&gt;1.pool 操作&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 列出池&lt;/span&gt;
ceph osd lspools

&lt;span class=&quot;c&quot;&gt;# 在配置文件中调整默认PG 数量以及副本数&lt;/span&gt;
osd pool default size &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 5
osd pool default pg num &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 100
osd pool default pgp num &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 100

&lt;span class=&quot;c&quot;&gt;# 创建池&lt;/span&gt;
ceph osd pool create k8s-pool 30

&lt;span class=&quot;c&quot;&gt;# 获取存储池选项值&lt;/span&gt;
ceph osd pool get k8s-pool pg_num/pgp_num

&lt;span class=&quot;c&quot;&gt;# 调整副本数&lt;/span&gt;
ceph osd pool &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;k8s-pool size 10

&lt;span class=&quot;c&quot;&gt;# 获取对象副本数&lt;/span&gt;
ceph osd dump | grep &lt;span class=&quot;s1&quot;&gt;'replicated size'&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 删除池&lt;/span&gt;
ceph osd pool delete k8s-pool k8s-pool --yes-i-really-really-mean-it
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.object操作&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 将对象放入到池内&lt;/span&gt;
rados put &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-object testfile.txt -p cephfs_data

&lt;span class=&quot;c&quot;&gt;# 列出池中对象&lt;/span&gt;
rados ls -p cephfs_data

&lt;span class=&quot;c&quot;&gt;# 检查池中对象位置&lt;/span&gt;
ceph osd map cephfs_data &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-object

&lt;span class=&quot;c&quot;&gt;# 删除对象&lt;/span&gt;
rados rm &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-object -p cephfs_data
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.PG和PGP操作&lt;/p&gt;

&lt;p&gt;预设Ceph集群中的PG数至关重要，公式如下:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PG 总数 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;OSD 数 &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; 100&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; / 最大副本数
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;集群中单个池的PG数计算公式如下：&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;PG 总数 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;OSD 数 &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; 100&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; / 最大副本数 / 池数
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;PGP是为了实现定位而设计的PG，PGP的值应该和PG数量保持一致；pgp_num 数值才是 CRUSH 算法采用的真实值。虽然 pg_num 的增加引起了PG的分割，但是只有当 pgp_num增加以后，数据才会被迁移到新PG中，这样才会重新开始平衡。&lt;/p&gt;

&lt;p&gt;获取PG和PGP的方式如下：&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph osd pool get cephfs_data pg_num
ceph osd pool get cephfs_data pgp_num
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;调整方式如下：&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph osd pool &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;cephfs_data pgp_num 32
ceph osd pool &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;cephfs_data pgp_num 32
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.Cluster map 操作&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 查看现有集群布局&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 基本上一台机器上一个osd&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$  &lt;/span&gt;ceph osd tree
ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.28134 root default                                            
-2 0.04689     host k8s-master01                                   
 0 0.04689         osd.0              up  1.00000          1.00000
-3 0.04689     host k8s-master02                                   
 1 0.04689         osd.1              up  1.00000          1.00000
-4 0.04689     host k8s-master03                                   
 2 0.04689         osd.2              up  1.00000          1.00000
-5 0.04689     host k8s-node01                                     
 3 0.04689         osd.3              up  1.00000          1.00000
-6 0.04689     host k8s-node02                                     
 4 0.04689         osd.4              up  1.00000          1.00000
-7 0.04689     host k8s-registry                                   
 5 0.04689         osd.5              up  1.00000          1.00000


&lt;span class=&quot;c&quot;&gt;# 添加逻辑上的机架&lt;/span&gt;
ceph osd crush add-bucket rack01 rack
ceph osd crush add-bucket rack02 rack
ceph osd crush add-bucket rack03 rack

&lt;span class=&quot;c&quot;&gt;# 将机器移动到不同的机架上&lt;/span&gt;
ceph osd crush move k8s-master01 &lt;span class=&quot;nv&quot;&gt;rack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rack01
ceph osd crush move k8s-master02 &lt;span class=&quot;nv&quot;&gt;rack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rack02
ceph osd crush move k8s-master03 &lt;span class=&quot;nv&quot;&gt;rack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rack03

&lt;span class=&quot;c&quot;&gt;# 移动每个机架到默认的根下&lt;/span&gt;
ceph osd crush move rack01 &lt;span class=&quot;nv&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default
ceph osd crush move rack02 &lt;span class=&quot;nv&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default
ceph osd crush move rack03 &lt;span class=&quot;nv&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;default
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;最终集群整体布局如下，我们可以看到每个机器都被分配到了对应的机架下面，从逻辑上进行了分隔&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph osd tree
ID  WEIGHT  TYPE NAME                 UP/DOWN REWEIGHT PRIMARY-AFFINITY
 -1 0.28134 root default                                                
 -5 0.04689     host k8s-node01                                         
  3 0.04689         osd.3                  up  1.00000          1.00000
 -6 0.04689     host k8s-node02                                         
  4 0.04689         osd.4                  up  1.00000          1.00000
 -7 0.04689     host k8s-registry                                       
  5 0.04689         osd.5                  up  1.00000          1.00000
 -8 0.04689     rack rack01                                             
 -2 0.04689         host k8s-master01                                   
  0 0.04689             osd.0              up  1.00000          1.00000
 -9 0.04689     rack rack02                                             
 -3 0.04689         host k8s-master02                                   
  1 0.04689             osd.1              up  1.00000          1.00000
-10 0.04689     rack rack03                                             
 -4 0.04689         host k8s-master03                                   
  2 0.04689             osd.2              up  1.00000          1.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;ceph硬件配置&quot;&gt;Ceph硬件配置&lt;/h3&gt;

&lt;h4 id=&quot;mon需求&quot;&gt;MON需求&lt;/h4&gt;

&lt;p&gt;Ceph monitor通过维护整个集群的map从而完成集群的健康处理；但是monitor并不参与实际的数据存储，所以实际上monitor节点CPU占用、内存占用都比较少；一般单核CPU加几个G的内存即可满足需求；虽然monitor节点不参与实际存储工作，但是monitor的网卡至少应该是冗余的，否则一旦网络出现故障则集群健康会难以保证。&lt;/p&gt;

&lt;h4 id=&quot;osd需求&quot;&gt;OSD需求&lt;/h4&gt;

&lt;p&gt;OSD作为Ceph的主要存储设备，其会占用一定的CPU和内存资源，一般推荐做法是每个节点的每块硬盘作为一个OSD；同时OSD还需写入日志，所以应当为OSD集成日志留充足的空间；在出现故障时，OSD需求的资源可能会更多，所以OSD节点根据实际情况(每个OSD会有一个线程)应该分配更多的CPU和内存；固态硬盘也会增加OSD存取速度和恢复速度&lt;/p&gt;

&lt;h4 id=&quot;mds需求&quot;&gt;MDS需求&lt;/h4&gt;

&lt;p&gt;MDS服务专门为CephFS存储元数据，所以相对于monitor和OSD节点，这个MDS节点的CPU需求会大得多，同时内存占用也是海量的，所以MDS一般会使用一个强劲的物理机单独搭建。&lt;/p&gt;</content><author><name>KevinGuo</name></author><summary type="html">其实我们在上一章的时候，已经简单的介绍了ceph中的基础组件以及其他软体架构，那么这一章，主要是来详细的介绍下ceph的工作原理、流程以及更实际的操作，比如ceph 内部的CRUSH bucket调整，PG/PGP参数调整等。并在最后简单的介绍了些CEPH集群硬件要求等。 寻址流程 通过对Ceph系统中寻址流程的了解来熟悉Ceph中的几个概念，寻址流程如下图 上图的几个概念说明如下： 1.File 此处的file就是用户需要存储或者访问的文件。对于一个基于Ceph开发的对象存储而言，这个file也就对应于应用中的”对象”，也就是用户直接操作的“对象”。 2.Object Object是Ceph中最小存储单元，是由一个数据和一个元数据绑定的整体，元数据中存放了具体数据的属性信息等。Object和File区别在于，object的最大size由RADOS限定（通常为2MB或者4MB），以便实现底层存储的组织管理。因此，当上层应用向RADOS存入size很大的File时，需要将File切分成统一大小的一系列Object(最后一个大小可以不同)进行存储。 3.PG PG的用途就是对Object的存储进行组织和位置映射。具体而言，一个PG负责组织若干个object(可以为数千个甚至更多)，但一个object只能被映射到一个PG中，即PG和object之间是“一对多”映射关系。同时，一个PG会被映射到n个OSD上，而每个OSD上都会承载大量的PG，即PG和OSD之间是“多对多”映射关系。在实践中，n至少为2，如果用于生产环境，则至少为3。一个OSD上的PG则可达到数百个。事实上，PG数量的设置牵扯到数据分布的均匀性问题。这一点后面再讲。 4.OSD 这个上一篇已经讲过了，这里就不再赘述。唯一需要说明的是，OSD的数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。在实践中，至少也得数十上百个的量级才有助于Ceph系统的设计发挥其应有的优势。 5.CRUSH 在传统的文件存储系统中，元数据占着及其重要的位置，每次系统中数据更新时，元数据会首先被更新，然后才是实际数据的写入；在较小的存储系统中(GB/TB)，这种将元数据存储在某个固定几点或者磁盘阵列中的做法还能满足需求；但当数据量增大到PB/ZB时，元数据查找的性能会成为很大的一个瓶颈；而且元数据的统一存放还可能造成单点故障，即当元数据丢失后，实际数据将无法被找回；与传统文件存储系统不同的是，Ceph使用CRUSH算法来精确的计算数据应该被写入那里/从哪里读取；CRUSH按需计算元数据，而不是存储元数据，从而解决了传统文件存储系统的瓶颈。 6.CRUSH计算 CRUSH是伪随机算法，算法通过每个设备的权重(容量大小)来计算数据对象的分布。对象分布式由cluster map和data distribution policy决定的。 cluster map由device和bucket组成。描述了可用存储资源和层级结构(比如有多少个机架，每个机架上有多少个服务器，每个服务器上有多少个磁盘)。 data distribution policy由 placement rules组成。rule决定了每个数据对象有多少个副本，这些副本存储的限制条件(比如3个副本放在不同的机架中)。 在Ceph中，元数据的计算和负载也是分布式的，并且只有在需要的时候才会执行，元数据的计算过程称之为CRUSH计算，不同于其他分布式文件系统，Ceph的CRUSH计算是由客户端使用自己的资源来完成，从而去除了中心查找带来的性能及单节点故障问题。 CRUSH查找时候，ceph client先通过MON 获取cluster map，然后获取到通过Hash算法算出的PG ID，最后再根据CRUSH算法计算出PG中，主和次OSD的ID，最终找到OSD的位置，进行数据的读写。 7.层级的Cluster map 在Ceph中，CRUSH是可以支持各种基础设施和用户自定义的；CRUSH设备列表中预先定义了一系列的设备，用户可以通过自定义配置将不同的OSD分配到不同的区域。这样就在物理上避免了所有数据都在同一个机架上，防止某个机架突然塌了以后数据全部丢失。 8.恢复和再平衡 在Ceph集群中，如果OSD挂了，且老师处于degraded状态，Ceph 都会将其标记为 down 和 out 状态；然后默认情况下 Ceph 会等待 300秒之后进行数据恢复和再平衡，这个值可以通过在配置文件中的 mon osd down out interval 参数来调整 总结 基于上述的定义，便可以对寻址流程进行解释了。具体而言，Ceph中的寻址至少要经历三次映射： 1.File -&amp;gt; Object 将用户操作的File，映射为RADOS能处理的object，每个object都会有一个唯一的oid。本质上就是按照object的最大size对file进行切分，切分成大小一致的object(最后的大小可以不一样) 2.Object -&amp;gt; PG 当File被映射为一个或多个object后，就需要将object映射到PG中,得到PG ID。这有一个计算公式 hash(oid) &amp;amp; mask –&amp;gt; pgid 由此可见，其计算由两步组成。首先是使用Ceph指定的静态hash算法计算出oid的hash值。然后将这个随机值和mask(mask的值=PG数-1)按位相与，最终得到PG ID。 3.PG -&amp;gt; OSD 最后根据前面得到的PG ID，通过CRUSH算法最终找到OSD的位置。 Ceph组件调整及操作 1.pool 操作 # 列出池 ceph osd lspools # 在配置文件中调整默认PG 数量以及副本数 osd pool default size = 5 osd pool default pg num = 100 osd pool default pgp num = 100 # 创建池 ceph osd pool create k8s-pool 30 # 获取存储池选项值 ceph osd pool get k8s-pool pg_num/pgp_num # 调整副本数 ceph osd pool set k8s-pool size 10 # 获取对象副本数 ceph osd dump | grep 'replicated size' # 删除池 ceph osd pool delete k8s-pool k8s-pool --yes-i-really-really-mean-it 2.object操作 # 将对象放入到池内 rados put test-object testfile.txt -p cephfs_data # 列出池中对象 rados ls -p cephfs_data # 检查池中对象位置 ceph osd map cephfs_data test-object # 删除对象 rados rm test-object -p cephfs_data 3.PG和PGP操作 预设Ceph集群中的PG数至关重要，公式如下: PG 总数 = (OSD 数 * 100) / 最大副本数 集群中单个池的PG数计算公式如下： PG 总数 = (OSD 数 * 100) / 最大副本数 / 池数 PGP是为了实现定位而设计的PG，PGP的值应该和PG数量保持一致；pgp_num 数值才是 CRUSH 算法采用的真实值。虽然 pg_num 的增加引起了PG的分割，但是只有当 pgp_num增加以后，数据才会被迁移到新PG中，这样才会重新开始平衡。 获取PG和PGP的方式如下： ceph osd pool get cephfs_data pg_num ceph osd pool get cephfs_data pgp_num 调整方式如下： ceph osd pool set cephfs_data pgp_num 32 ceph osd pool set cephfs_data pgp_num 32 3.Cluster map 操作 # 查看现有集群布局 # 基本上一台机器上一个osd $ ceph osd tree ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 0.28134 root default -2 0.04689 host k8s-master01 0 0.04689 osd.0 up 1.00000 1.00000 -3 0.04689 host k8s-master02 1 0.04689 osd.1 up 1.00000 1.00000 -4 0.04689 host k8s-master03 2 0.04689 osd.2 up 1.00000 1.00000 -5 0.04689 host k8s-node01 3 0.04689 osd.3 up 1.00000 1.00000 -6 0.04689 host k8s-node02 4 0.04689 osd.4 up 1.00000 1.00000 -7 0.04689 host k8s-registry 5 0.04689 osd.5 up 1.00000 1.00000 # 添加逻辑上的机架 ceph osd crush add-bucket rack01 rack ceph osd crush add-bucket rack02 rack ceph osd crush add-bucket rack03 rack # 将机器移动到不同的机架上 ceph osd crush move k8s-master01 rack=rack01 ceph osd crush move k8s-master02 rack=rack02 ceph osd crush move k8s-master03 rack=rack03 # 移动每个机架到默认的根下 ceph osd crush move rack01 root=default ceph osd crush move rack02 root=default ceph osd crush move rack03 root=default 最终集群整体布局如下，我们可以看到每个机器都被分配到了对应的机架下面，从逻辑上进行了分隔 $ ceph osd tree ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 0.28134 root default -5 0.04689 host k8s-node01 3 0.04689 osd.3 up 1.00000 1.00000 -6 0.04689 host k8s-node02 4 0.04689 osd.4 up 1.00000 1.00000 -7 0.04689 host k8s-registry 5 0.04689 osd.5 up 1.00000 1.00000 -8 0.04689 rack rack01 -2 0.04689 host k8s-master01 0 0.04689 osd.0 up 1.00000 1.00000 -9 0.04689 rack rack02 -3 0.04689 host k8s-master02 1 0.04689 osd.1 up 1.00000 1.00000 -10 0.04689 rack rack03 -4 0.04689 host k8s-master03 2 0.04689 osd.2 up 1.00000 1.00000 Ceph硬件配置 MON需求 Ceph monitor通过维护整个集群的map从而完成集群的健康处理；但是monitor并不参与实际的数据存储，所以实际上monitor节点CPU占用、内存占用都比较少；一般单核CPU加几个G的内存即可满足需求；虽然monitor节点不参与实际存储工作，但是monitor的网卡至少应该是冗余的，否则一旦网络出现故障则集群健康会难以保证。 OSD需求 OSD作为Ceph的主要存储设备，其会占用一定的CPU和内存资源，一般推荐做法是每个节点的每块硬盘作为一个OSD；同时OSD还需写入日志，所以应当为OSD集成日志留充足的空间；在出现故障时，OSD需求的资源可能会更多，所以OSD节点根据实际情况(每个OSD会有一个线程)应该分配更多的CPU和内存；固态硬盘也会增加OSD存取速度和恢复速度 MDS需求 MDS服务专门为CephFS存储元数据，所以相对于monitor和OSD节点，这个MDS节点的CPU需求会大得多，同时内存占用也是海量的，所以MDS一般会使用一个强劲的物理机单独搭建。</summary></entry><entry><title type="html">kubernetes ceph 笔记 1</title><link href="https://kevinguo.me/2017/09/06/kubernetes-ceph-1/" rel="alternate" type="text/html" title="kubernetes ceph 笔记 1" /><published>2017-09-06T00:00:00+08:00</published><updated>2017-09-06T00:00:00+08:00</updated><id>https://kevinguo.me/2017/09/06/kubernetes-ceph-1</id><content type="html" xml:base="https://kevinguo.me/2017/09/06/kubernetes-ceph-1/">&lt;blockquote&gt;
  &lt;p&gt;该教程主要是为statefulset有状态服务集群提供持久化存储提供基础，在讲statefulset之前，我们先搭建我们的ceph集群
；具备了极好的可靠性、统一性；经过近几年的发展，ceph开辟了一个全新的数据存储途径。ceph具备了企业级存储的分布式、可大规模扩展、没有当节点故障等特点，越来越受人们的青睐。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;

&lt;p&gt;Ceph是一个符合POSIX、开源的分布式存储系统，不论你是想提供&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph 对象存储&lt;/code&gt;或&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph 块设备&lt;/code&gt;，还是想部署一个&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph文件系统&lt;/code&gt;或者把ceph作为他用，所有 &lt;code class=&quot;highlighter-rouge&quot;&gt;Ceph存储集群&lt;/code&gt; 的部署都始于部署一个个 &lt;code class=&quot;highlighter-rouge&quot;&gt;ceph节点&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;网络&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph存储集群&lt;/code&gt;，ceph 存储集群至少需要一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;ceph monitor&lt;/code&gt;和两个&lt;code class=&quot;highlighter-rouge&quot;&gt;osd守护进程&lt;/code&gt;。而运行&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph文件系统客户端&lt;/code&gt;，则必须需要MDS(元数据服务器)。&lt;/p&gt;

&lt;p&gt;基础组件:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ceph OSDs: Ceph OSD 守护进程的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当Ceph 存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能到到&lt;code class=&quot;highlighter-rouge&quot;&gt;active+clean&lt;/code&gt;状态(Ceph 默认有3个副本，你可以调整&lt;code class=&quot;highlighter-rouge&quot;&gt;osd poll default size&lt;/code&gt;)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ceph Monitors: Ceph Monitor基于PAXOS算法维护着 展示集群状态的各种图表，包括监视器图、OSD图、PG图、CRUSH图。Ceph 保存着发生在Monitors、OSD和PG上的每一次状态变更的历史信息(称为 epoch)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;MDSs：Ceph MDS为 Ceph 文件系统存储元数据(也就是说，Ceph块设备和Ceph对象存储是不是用MDS)。MDS使得POSIX文件系统的用户们，可以在部队Ceph存储集群造成负担的前提下，执行诸如 &lt;code class=&quot;highlighter-rouge&quot;&gt;ls&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;find&lt;/code&gt;等基本命令&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下图展示了Ceph的基础架构图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/posts/Ceph-soft-topu.png&quot; alt=&quot;ceph-topu&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1基础存储系统rados&quot;&gt;1.基础存储系统RADOS&lt;/h4&gt;

&lt;p&gt;RADOS (Reliable Autonomic Distributed Object Store),这一层本身就是一个完整的对象存储系统，包括Ceph的基础服务(OSD,MON,MDS)，所有存储在ceph中的用户数据实际上最终都是由这一层来存储的。而Ceph的高可用，高扩展性，高自动化等特性本质上也是有这一层所提供的，因此，RADOS是ceph的核心精华部分。&lt;/p&gt;

&lt;h4 id=&quot;2基础库librados&quot;&gt;2.基础库LibRados&lt;/h4&gt;

&lt;p&gt;这一层是对RADOS进行抽象和封装，并向上层提供不同的API，这样上层的RBD、RGW、CephFS才能访问RADOS，RADOS所提供的原生librados API包括C和C++两种。&lt;/p&gt;

&lt;h4 id=&quot;3高层存储应用接口&quot;&gt;3.高层存储应用接口&lt;/h4&gt;

&lt;p&gt;这一层包含了RGW、RBD和CephFS这几个部分，其作用是在librados库的基础上提供抽象层次更高，更便于应用和用户端使用的上层接口。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;RGW: 是一个提供与S3和Swift兼容的RESTful API的gateway，以供对应的对象存储应用开发使用。通过RGW可以将RADOS响应转化为HTTP响应，同样也可以将外部的HTTP响应状花为RADOS调用。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;RBD：提供一个标准的块设备接口服务。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;CephFS: 提供一个POSIX兼容的分布式文件系统。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;4client层&quot;&gt;4.client层&lt;/h4&gt;

&lt;p&gt;这一层其实就是不同场景下对于ceph各个应用接口的各种应用方式，例如基于librados直接开发的对象存储应用，基于RGW开发的对象存储应用，基于RBD实现的云硬盘等等。&lt;/p&gt;

&lt;h4 id=&quot;5其他&quot;&gt;5.其他&lt;/h4&gt;

&lt;p&gt;一个Cluster逻辑上可以划分为多个Pool，一个Pool由若干个逻辑PG组成。&lt;/p&gt;

&lt;p&gt;一个文件会被切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD，其中第一个OSD（Primary OSD）为主，其余是备，OSD间通过心跳来互相监控存活状态。&lt;/p&gt;

&lt;p&gt;CRUSH： CRUSH是ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。&lt;/p&gt;

&lt;h3 id=&quot;一快速安装&quot;&gt;一.快速安装&lt;/h3&gt;

&lt;h4 id=&quot;11-安装前准备&quot;&gt;1.1 安装前准备&lt;/h4&gt;

&lt;p&gt;所需机器如下&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;IP&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;HostName&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;OS&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;role&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;172.30.33.31&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;deploy-node&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;centos7.3.1611&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;deploy node&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;172.30.33.90&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;k8s-master01&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;centos7.3.1611&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;monitor osd node1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;172.30.33.91&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;k8s-master02&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;centos7.3.1611&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;monitor osd node2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;172.30.33.92&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;k8s-master03&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;centos7.3.1611&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;monitor osd node3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;172.30.33.89&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;k8s-registry&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;centos7.3.1611&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;osd mds&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;172.30.33.93&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;k8s-node01&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;centos7.3.1611&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;osd mds&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;172.30.33.94&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;k8s-node02&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;centos7.3.1611&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;osd mds&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;在管理节点上操作&quot;&gt;在管理节点上操作&lt;/h5&gt;
&lt;p&gt;1.添加ceph源&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ceph-noarch]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Ceph noarch packages
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/rpm-jewel/el7/noarch
&lt;span class=&quot;nv&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rpm-md
&lt;span class=&quot;nv&quot;&gt;gpgkey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/keys/release.asc

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum update -y &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; yum install ceph-deploy -y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.更新并安装&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph-deploy&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo yum update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo yum install ceph-deploy
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.配置从部署机器到所有其他节点的免密钥登录，具体参考&lt;a href=&quot;https://kevinguo.me/2017/07/06/ansible-client/&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;

&lt;h5 id=&quot;在节点上操作&quot;&gt;在节点上操作&lt;/h5&gt;

&lt;p&gt;1.安装epel源&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum install yum-plugin-priorities
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum install epel-release -y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.校对时间，由于ceph使用Paxos算法保证数据一致性，所以安装前要先保证各个节点的时间同步&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo yum install ntp ntpdate ntp-doc

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ntpdate 0.cn.pool.ntp.org
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.开放所需端口或关闭防火墙&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;system stop firewalld
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo firewall-cmd --zone&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;public --add-port&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;6789/tcp --permanent
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.关闭selinux&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo setenforce 0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;12-创建集群&quot;&gt;1.2 创建集群&lt;/h4&gt;

&lt;p&gt;1.由于ceph-deploy工具部署集群前需要创建一些集群配置信息，其保存在&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph.conf&lt;/code&gt;文件中，这个文件将来会被复制到每个节点的 &lt;code class=&quot;highlighter-rouge&quot;&gt;/etc/ceph/ceph.conf&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 创建集群配置目录&lt;/span&gt;
mkdir ceph-cluster &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;ceph-cluster
&lt;span class=&quot;c&quot;&gt;# 创建 monitor-node&lt;/span&gt;
ceph-deploy new k8s-master01
&lt;span class=&quot;c&quot;&gt;# 追加 OSD 副本数量(测试虚拟机总共有3台)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;osd pool default size = 3&quot;&lt;/span&gt; &amp;gt;&amp;gt; ceph.conf
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.创建集群使用 &lt;code class=&quot;highlighter-rouge&quot;&gt;ceph-deploy&lt;/code&gt;工具在部署节点上执行即可&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 安装ceph&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy install k8s-master01 k8s-master02 k8s-master03
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;注意：在部署节点部署的时候，可能会因为网络原因导致无法安装ceph和ceph-radosgw，这时候，我们在各个节点上手动安装一下&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 添加ceph 源&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Ceph]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Ceph packages &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$basearch&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://download.ceph.com/rpm-jewel/el7/&lt;span class=&quot;nv&quot;&gt;$basearch&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rpm-md
&lt;span class=&quot;nv&quot;&gt;gpgkey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/keys/release.asc
&lt;span class=&quot;nv&quot;&gt;priority&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Ceph-noarch]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Ceph noarch packages
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://download.ceph.com/rpm-jewel/el7/noarch
&lt;span class=&quot;nv&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rpm-md
&lt;span class=&quot;nv&quot;&gt;gpgkey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/keys/release.asc
&lt;span class=&quot;nv&quot;&gt;priority&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ceph-source]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;Ceph &lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;packages
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://download.ceph.com/rpm-jewel/el7/SRPMS
&lt;span class=&quot;nv&quot;&gt;enabled&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;rpm-md
&lt;span class=&quot;nv&quot;&gt;gpgkey&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;https://download.ceph.com/keys/release.asc
&lt;span class=&quot;nv&quot;&gt;priority&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1


&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;yum install ceph ceph-radosgw -y
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.初始化monitor node 和密钥文件&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy mon create-initial
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.在osd节点上创建一个目录作为 osd 存储，并修改其权限,千万别创建在&lt;code class=&quot;highlighter-rouge&quot;&gt;/usr/local&lt;/code&gt;目录下，否则，你的osd可能会无法挂载&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ssh k8s-master01
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo mkdir /data/osd1
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;chown -R ceph:ceph osd1
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ssh k8s-master2
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo mkdir /data/osd2
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;chown -R ceph:ceph osd2
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ssh k8s-master3
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo mkdir /data/osd3
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;chown -R ceph:ceph osd3
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;exit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;5.然后，在管理节点上初始化 osd&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy osd prepare k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;6.最后，在管理节点上激活 osd&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy osd activate k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd3
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;7.在管理节点上部署 ceph cli 工具和密钥文件&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy admin k8s-master01 k8s-master02 k8s-master03
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;8.确保你对 &lt;code class=&quot;highlighter-rouge&quot;&gt;ceph.client.admin.keyring&lt;/code&gt;有正确的操作权限，在每个节点上执行&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo chmod +r /etc/ceph/ceph.client.admin.keyring
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;9.最后，检查集群的健康状态&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph health
HEALTH_OK

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph osd tree
ID WEIGHT  TYPE NAME             UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 0.14067 root default                                            
-2 0.04689     host k8s-master01                                   
 0 0.04689         osd.0              up  1.00000          1.00000
-3 0.04689     host k8s-master02                                   
 1 0.04689         osd.1              up  1.00000          1.00000
-4 0.04689     host k8s-master03                                   
 2 0.04689         osd.2              up  1.00000          1.00000
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;我们看到状态是OK的，而且每个节点上的osd的状态都是up的&lt;/p&gt;

&lt;p&gt;如果在某些地方碰到麻烦，想从头再来，可以用下列命令来清楚配置：&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy purgedata &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;ceph-node&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;ceph-node&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy forgetkeys
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;用下列命令可以连Ceph安装包一起清除：&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy purge &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;ceph-node&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[{&lt;/span&gt;ceph-node&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;二操作集群&quot;&gt;二.操作集群&lt;/h3&gt;

&lt;h4 id=&quot;21-基础操作&quot;&gt;2.1 基础操作&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 创建MDS&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy mds create &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;ceph-node&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 创建RGW&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy rgw create &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;ceph-node&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 添加mon&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;public network = 172.30.33.0/24&quot;&lt;/span&gt; &amp;gt;&amp;gt; ceph.conf
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy mon add &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;ceph-node&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 查看仲裁&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph quorum_status --format json-pretty
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;22-对象存储测试&quot;&gt;2.2 对象存储测试&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 创建pool&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# rados mkpool {pool-name}&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rados mkpool &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pool

&lt;span class=&quot;c&quot;&gt;# 创建测试文件&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;dd &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;testfile &lt;span class=&quot;nv&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1G &lt;span class=&quot;nv&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1

&lt;span class=&quot;c&quot;&gt;# 创建一个对象(这时候也将对象放入了pool中)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# rados put {object-name} {file-path} --pool={pool-name}&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rados put &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-object testfile --pool&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pool

&lt;span class=&quot;c&quot;&gt;# 检查存储池，确认ceph存储了此对象&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rados -p &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pool ls

&lt;span class=&quot;c&quot;&gt;# 定位对象，会输出对象位置&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph osd map &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pool &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-object
osdmap e42 pool &lt;span class=&quot;s1&quot;&gt;'test-pool'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;3&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; object &lt;span class=&quot;s1&quot;&gt;'test-file'&lt;/span&gt; -&amp;gt; pg 3.b79653d4 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;3.4&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; -&amp;gt; up &lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;1,5,2,3,4], p1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; acting &lt;span class=&quot;o&quot;&gt;([&lt;/span&gt;1,5,2,3,4], p1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 删除对象&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rados -p data rm &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-object

&lt;span class=&quot;c&quot;&gt;# 删除存储池&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rados rmpool &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pool &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-pool --yes-i-really-really-mean-it
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;随着集群的运行，对象的位置可能会动态改变。Ceph有动态均衡机制，无需手动干预即可完成。&lt;/p&gt;

&lt;h4 id=&quot;23-块存储测试&quot;&gt;2.3 块存储测试&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;官方建议使用RBD的客户端最好不要和OSD在同一台物理机上(除非它们都是VM)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1.确认你使用了合适的内核版本，详情&lt;a href=&quot;http://docs.ceph.com/docs/master/start/os-recommendations/&quot;&gt;参见&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lab_release -a
uname -a
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.在管理节点上用 &lt;code class=&quot;highlighter-rouge&quot;&gt;ceph-deploy&lt;/code&gt;安装ceph&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph-deploy install &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;rbd-client&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;3.在管理节点上部署ceph cli工具和密钥&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph-deploy admin &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;rbd-client&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;4.在rbd节点上创建块设备image&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd create &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block --size 4096
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;5.映射image到块设备&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;rbd map &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block --name client.admin
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;在上面的map映射操作时，会出现如下报错&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rbd map &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block --name client.admin
rbd: sysfs write failed
RBD image feature &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;mismatch. You can disable features unsupported by the kernel with &lt;span class=&quot;s2&quot;&gt;&quot;rbd feature disable&quot;&lt;/span&gt;.
In some cases useful info is found &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;syslog - try &lt;span class=&quot;s2&quot;&gt;&quot;dmesg | tail&quot;&lt;/span&gt; or so.
rbd: map failed: &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;6&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; No such device or address
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;大致意思是说features不匹配，可以通过disable features关掉一些特性来让内核支持。这是因为在Ceph高本本进行 map image时，默认ceph在创建image(上文test-block)时，会增加很多features，这些features需要内核支持，centos7上的支持有限，所以，我们需要关掉一些&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们可以用 &lt;code class=&quot;highlighter-rouge&quot;&gt;rbd info data&lt;/code&gt; 看看创建的image目前有哪些features&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rbd info &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block
rbd image &lt;span class=&quot;s1&quot;&gt;'test-block'&lt;/span&gt;:
	size 4096 MB &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;1024 objects
	order 22 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;4096 kB objects&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
	block_name_prefix: rbd_data.10bb238e1f29
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	flags:
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在features中，我们可以看到默认开启了很多：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;layering 支持分层&lt;/li&gt;
  &lt;li&gt;exclusive-lock 支持独占锁&lt;/li&gt;
  &lt;li&gt;object-map 支持对象映射(依赖exclusive-lock)&lt;/li&gt;
  &lt;li&gt;fast-diff 快速计算差异(依赖object-map)&lt;/li&gt;
  &lt;li&gt;deep-flatten 支持快照扁平化操作&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;而实际上在CentOS7的3.10内核中只支持layering，所以我们需要手动关闭一些features，然后重新map；如果想要一劳永逸，可以在 ceph.conf 中加入 rbd_default_features = 1 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;6.关闭不支持的特性之后重新map&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 关闭不支持的features&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rbd feature disable &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block exclusive-lock, object-map, fast-diff, deep-flatten

&lt;span class=&quot;c&quot;&gt;# 重新map&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;rbd map &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block --name client.admin
/dev/rbd0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;7.格式化之后挂载到系统目录&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 格式化&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkfs.xfs /dev/rbd0
meta-data&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/rbd0              &lt;span class=&quot;nv&quot;&gt;isize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;512    &lt;span class=&quot;nv&quot;&gt;agcount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;9, &lt;span class=&quot;nv&quot;&gt;agsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;130048 blks
         &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;                       &lt;span class=&quot;nv&quot;&gt;sectsz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;512   &lt;span class=&quot;nv&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2, &lt;span class=&quot;nv&quot;&gt;projid32bit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
         &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;                       &lt;span class=&quot;nv&quot;&gt;crc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1        &lt;span class=&quot;nv&quot;&gt;finobt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0, &lt;span class=&quot;nv&quot;&gt;sparse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
data     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;                       &lt;span class=&quot;nv&quot;&gt;bsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4096   &lt;span class=&quot;nv&quot;&gt;blocks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1048576, &lt;span class=&quot;nv&quot;&gt;imaxpct&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;25
         &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;                       &lt;span class=&quot;nv&quot;&gt;sunit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1024   &lt;span class=&quot;nv&quot;&gt;swidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1024 blks
naming   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;version 2              &lt;span class=&quot;nv&quot;&gt;bsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4096   ascii-ci&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 &lt;span class=&quot;nv&quot;&gt;ftype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
log      &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;internal log           &lt;span class=&quot;nv&quot;&gt;bsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4096   &lt;span class=&quot;nv&quot;&gt;blocks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2560, &lt;span class=&quot;nv&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2
         &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;                       &lt;span class=&quot;nv&quot;&gt;sectsz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;512   &lt;span class=&quot;nv&quot;&gt;sunit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8 blks, lazy-count&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
realtime &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;none                   &lt;span class=&quot;nv&quot;&gt;extsz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4096   &lt;span class=&quot;nv&quot;&gt;blocks&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0, &lt;span class=&quot;nv&quot;&gt;rtextents&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0

&lt;span class=&quot;c&quot;&gt;# 挂载&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mkdir &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;mount /dev/rbd0 &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block

&lt;span class=&quot;c&quot;&gt;# 写入测试&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;dd &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block/test-file &lt;span class=&quot;nv&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1G &lt;span class=&quot;nv&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
1+0 records &lt;span class=&quot;k&quot;&gt;in
&lt;/span&gt;1+0 records out
1073741824 bytes &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1.1 GB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; copied, 2.96071 s, 363 MB/s

&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ls &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-block/
&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-file
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;24-cephfs-测试&quot;&gt;2.4 CephFS 测试&lt;/h4&gt;

&lt;p&gt;1.创建MDS&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy mds create k8s-node01 k8s-node02 k8s-registry
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.创建pool和fs，创建pool需要指定PG数量&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ceph osd pool create cephfs_data 32
ceph osd pool create cephfs_metadata 32
ceph fs new &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-fs cephfs_metadata cephfs_data
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;PG 概念：&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;当Ceph集群接受到存储请求时，ceph会将一个文件会切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD(根据副本数)；一般来说增加PG的数量能降低OSD负载，一般每个OSD大约分配50～100PG，关于PG数量指定，一般遵循以下公式&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;集群PG总数 = (OSD总数 * 100)/数据最大副本数&lt;/li&gt;
    &lt;li&gt;单个存储池PG数 = (OSD总数 * 100)/数据最大副本数/存储池数&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;注意：PG的最终结果应当以最接近以上计算公式的2的N次幂(向上取值)；如我的虚拟机环境的每个存储池 PG数 = 6(OSD) * 100 / 5(副本数) / 4（4个存储池）= 30，向上取2的N次幂为32(即，2的5次方=32，最接近30)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;3.挂载CephFS有两种方式，一种是使用内核驱动挂载，一种是使用 &lt;code class=&quot;highlighter-rouge&quot;&gt;ceph-fuse&lt;/code&gt;用户空间挂载&lt;/p&gt;

&lt;p&gt;内核挂载需要提取ceph管理key，方式如下：&lt;/p&gt;

&lt;p&gt;在密钥文件中找到与某用户对于的密钥&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;cat /etc/ceph/ceph.client.admin.keyring
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;client.admin]
	key &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;复制密钥到文件中保存，并确保其权限&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA==&quot;&lt;/span&gt; &amp;gt; ceph-key
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;创建目录挂载&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mkdir &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-fs
mount -t ceph 172.30.33.90:6789:/ /root/test-fs -o &lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;admin,secretfile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ceph-key

&lt;span class=&quot;c&quot;&gt;#写入数据测试&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;dd &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-fs/test-fs &lt;span class=&quot;nv&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1G &lt;span class=&quot;nv&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
1+0 records &lt;span class=&quot;k&quot;&gt;in
&lt;/span&gt;1+0 records out
1073741824 bytes &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1.1 GB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; copied, 2.77355 s, 387 MB/s
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ceph-fuse&lt;/code&gt;用户空间挂载的方式也比较简单，需要先安装ceph-fuse，同时也需要key&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 按照前面的步骤添加ceph源&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;vi /etc/yum.repos.d/ceph.repo
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ceph]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ceph
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ceph-noarch]
&lt;span class=&quot;nv&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cephnoarch
&lt;span class=&quot;nv&quot;&gt;baseurl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/
&lt;span class=&quot;nv&quot;&gt;gpgcheck&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0

&lt;span class=&quot;c&quot;&gt;# 安装ceph-fuse&lt;/span&gt;
yum install ceph-fuse -y

&lt;span class=&quot;c&quot;&gt;# 复制配置和ceph key到client端&lt;/span&gt;
sudo mkdir -p /etc/ceph
sudo scp root@172.30.33.91:/etc/ceph/ceph.conf /etc/ceph/ceph.conf
sudo scp root@172.30.33.91:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring

&lt;span class=&quot;c&quot;&gt;# 创建目录挂载&lt;/span&gt;
mkdir &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-fs-fuse
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo ceph-fuse -m 172.30.33.91:6789 &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-fs-fuse
ceph-fuse[60551]: starting ceph client
2017-09-12 14:47:24.137929 7f63d9719ec0 -1 init, newargv &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 0x7f63e51d4840 &lt;span class=&quot;nv&quot;&gt;newargc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;11
ceph-fuse[60551]: starting fuse

&lt;span class=&quot;c&quot;&gt;# 写入数据测试&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;sudo dd &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/zero &lt;span class=&quot;nv&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-fs-fuse/test-fs-fuse &lt;span class=&quot;nv&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1G &lt;span class=&quot;nv&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
1+0 records &lt;span class=&quot;k&quot;&gt;in
&lt;/span&gt;1+0 records out
1073741824 bytes &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;1.1 GB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; copied, 10.5426 s, 102 MB/s

&lt;span class=&quot;c&quot;&gt;# 查看确认，发现我们上面通过内核挂载的文件也还在&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ls -lh
total 2.0G
-rw-r--r-- 1 root root 1.0G Sep 12 13:59 &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-fs
-rw-r--r-- 1 root root 1.0G Sep 12 14:49 &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;-fs-fuse
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;25-ceph对象网关&quot;&gt;2.5 Ceph对象网关&lt;/h4&gt;

&lt;p&gt;1.对象网关创建&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 创建RGW&lt;/span&gt;
&lt;span class=&quot;gp&quot;&gt;$ &lt;/span&gt;ceph-deploy rgw create k8s-node02
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;2.直接访问&lt;code class=&quot;highlighter-rouge&quot;&gt;http://ceph-node-ip:7480&lt;/code&gt;返回结果如下&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;ListAllMyBucketsResult&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;xmlns=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;Owner&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;ID&amp;gt;&lt;/span&gt;anonymous&lt;span class=&quot;nt&quot;&gt;&amp;lt;/ID&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;DisplayName/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/Owner&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;Buckets/&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/ListAllMyBucketsResult&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;这就说明网关OK了，但是因为没有读写环境，所以暂时测不了。&lt;/p&gt;

&lt;p&gt;本文参考了&lt;a href=&quot;http://docs.ceph.com/docs/master/start/&quot;&gt;ceph官方文档&lt;/a&gt;及漠然的&lt;a href=&quot;https://mritd.me/2017/05/27/ceph-note-1/&quot;&gt;ceph笔记(一)&lt;/a&gt;部分&lt;/p&gt;</content><author><name>KevinGuo</name></author><summary type="html">该教程主要是为statefulset有状态服务集群提供持久化存储提供基础，在讲statefulset之前，我们先搭建我们的ceph集群 ；具备了极好的可靠性、统一性；经过近几年的发展，ceph开辟了一个全新的数据存储途径。ceph具备了企业级存储的分布式、可大规模扩展、没有当节点故障等特点，越来越受人们的青睐。 简介 Ceph是一个符合POSIX、开源的分布式存储系统，不论你是想提供ceph 对象存储或ceph 块设备，还是想部署一个ceph文件系统或者把ceph作为他用，所有 Ceph存储集群 的部署都始于部署一个个 ceph节点、网络和ceph存储集群，ceph 存储集群至少需要一个 ceph monitor和两个osd守护进程。而运行ceph文件系统客户端，则必须需要MDS(元数据服务器)。 基础组件: Ceph OSDs: Ceph OSD 守护进程的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当Ceph 存储集群设定为有2个副本时，至少需要2个OSD守护进程，集群才能到到active+clean状态(Ceph 默认有3个副本，你可以调整osd poll default size) Ceph Monitors: Ceph Monitor基于PAXOS算法维护着 展示集群状态的各种图表，包括监视器图、OSD图、PG图、CRUSH图。Ceph 保存着发生在Monitors、OSD和PG上的每一次状态变更的历史信息(称为 epoch) MDSs：Ceph MDS为 Ceph 文件系统存储元数据(也就是说，Ceph块设备和Ceph对象存储是不是用MDS)。MDS使得POSIX文件系统的用户们，可以在部队Ceph存储集群造成负担的前提下，执行诸如 ls、find等基本命令 下图展示了Ceph的基础架构图 1.基础存储系统RADOS RADOS (Reliable Autonomic Distributed Object Store),这一层本身就是一个完整的对象存储系统，包括Ceph的基础服务(OSD,MON,MDS)，所有存储在ceph中的用户数据实际上最终都是由这一层来存储的。而Ceph的高可用，高扩展性，高自动化等特性本质上也是有这一层所提供的，因此，RADOS是ceph的核心精华部分。 2.基础库LibRados 这一层是对RADOS进行抽象和封装，并向上层提供不同的API，这样上层的RBD、RGW、CephFS才能访问RADOS，RADOS所提供的原生librados API包括C和C++两种。 3.高层存储应用接口 这一层包含了RGW、RBD和CephFS这几个部分，其作用是在librados库的基础上提供抽象层次更高，更便于应用和用户端使用的上层接口。 RGW: 是一个提供与S3和Swift兼容的RESTful API的gateway，以供对应的对象存储应用开发使用。通过RGW可以将RADOS响应转化为HTTP响应，同样也可以将外部的HTTP响应状花为RADOS调用。 RBD：提供一个标准的块设备接口服务。 CephFS: 提供一个POSIX兼容的分布式文件系统。 4.client层 这一层其实就是不同场景下对于ceph各个应用接口的各种应用方式，例如基于librados直接开发的对象存储应用，基于RGW开发的对象存储应用，基于RBD实现的云硬盘等等。 5.其他 一个Cluster逻辑上可以划分为多个Pool，一个Pool由若干个逻辑PG组成。 一个文件会被切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD，其中第一个OSD（Primary OSD）为主，其余是备，OSD间通过心跳来互相监控存活状态。 CRUSH： CRUSH是ceph使用的数据分布算法，类似一致性哈希，让数据分配到预期的地方。 一.快速安装 1.1 安装前准备 所需机器如下 IP HostName OS role 172.30.33.31 deploy-node centos7.3.1611 deploy node 172.30.33.90 k8s-master01 centos7.3.1611 monitor osd node1 172.30.33.91 k8s-master02 centos7.3.1611 monitor osd node2 172.30.33.92 k8s-master03 centos7.3.1611 monitor osd node3 172.30.33.89 k8s-registry centos7.3.1611 osd mds 172.30.33.93 k8s-node01 centos7.3.1611 osd mds 172.30.33.94 k8s-node02 centos7.3.1611 osd mds 在管理节点上操作 1.添加ceph源 [ceph-noarch] name=Ceph noarch packages baseurl=https://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc $ yum update -y &amp;amp;&amp;amp; yum install ceph-deploy -y 2.更新并安装ceph-deploy $ sudo yum update &amp;amp;&amp;amp; sudo yum install ceph-deploy 2.配置从部署机器到所有其他节点的免密钥登录，具体参考这里 在节点上操作 1.安装epel源 $ yum install yum-plugin-priorities $ yum install epel-release -y 2.校对时间，由于ceph使用Paxos算法保证数据一致性，所以安装前要先保证各个节点的时间同步 $ sudo yum install ntp ntpdate ntp-doc $ ntpdate 0.cn.pool.ntp.org 3.开放所需端口或关闭防火墙 $ system stop firewalld $ sudo firewall-cmd --zone=public --add-port=6789/tcp --permanent 4.关闭selinux $ sudo setenforce 0 1.2 创建集群 1.由于ceph-deploy工具部署集群前需要创建一些集群配置信息，其保存在ceph.conf文件中，这个文件将来会被复制到每个节点的 /etc/ceph/ceph.conf # 创建集群配置目录 mkdir ceph-cluster &amp;amp;&amp;amp; cd ceph-cluster # 创建 monitor-node ceph-deploy new k8s-master01 # 追加 OSD 副本数量(测试虚拟机总共有3台) echo &quot;osd pool default size = 3&quot; &amp;gt;&amp;gt; ceph.conf 2.创建集群使用 ceph-deploy工具在部署节点上执行即可 # 安装ceph $ ceph-deploy install k8s-master01 k8s-master02 k8s-master03 注意：在部署节点部署的时候，可能会因为网络原因导致无法安装ceph和ceph-radosgw，这时候，我们在各个节点上手动安装一下 # 添加ceph 源 [Ceph] name=Ceph packages for $basearch baseurl=http://download.ceph.com/rpm-jewel/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://download.ceph.com/rpm-jewel/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [ceph-source] name=Ceph source packages baseurl=http://download.ceph.com/rpm-jewel/el7/SRPMS enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 $ yum install ceph ceph-radosgw -y 3.初始化monitor node 和密钥文件 $ ceph-deploy mon create-initial 4.在osd节点上创建一个目录作为 osd 存储，并修改其权限,千万别创建在/usr/local目录下，否则，你的osd可能会无法挂载 $ ssh k8s-master01 $ sudo mkdir /data/osd1 $ chown -R ceph:ceph osd1 $ exit $ ssh k8s-master2 $ sudo mkdir /data/osd2 $ chown -R ceph:ceph osd2 $ exit $ ssh k8s-master3 $ sudo mkdir /data/osd3 $ chown -R ceph:ceph osd3 $ exit 5.然后，在管理节点上初始化 osd $ ceph-deploy osd prepare k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd3 6.最后，在管理节点上激活 osd $ ceph-deploy osd activate k8s-master01:/data/osd1 k8s-master02:/data/osd2 k8s-master03:/data/osd3 7.在管理节点上部署 ceph cli 工具和密钥文件 $ ceph-deploy admin k8s-master01 k8s-master02 k8s-master03 8.确保你对 ceph.client.admin.keyring有正确的操作权限，在每个节点上执行 $ sudo chmod +r /etc/ceph/ceph.client.admin.keyring 9.最后，检查集群的健康状态 $ ceph health HEALTH_OK $ ceph osd tree ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 0.14067 root default -2 0.04689 host k8s-master01 0 0.04689 osd.0 up 1.00000 1.00000 -3 0.04689 host k8s-master02 1 0.04689 osd.1 up 1.00000 1.00000 -4 0.04689 host k8s-master03 2 0.04689 osd.2 up 1.00000 1.00000 我们看到状态是OK的，而且每个节点上的osd的状态都是up的 如果在某些地方碰到麻烦，想从头再来，可以用下列命令来清楚配置： $ ceph-deploy purgedata {ceph-node} [{ceph-node}] $ ceph-deploy forgetkeys 用下列命令可以连Ceph安装包一起清除： $ ceph-deploy purge {ceph-node} [{ceph-node}] 二.操作集群 2.1 基础操作 # 创建MDS $ ceph-deploy mds create {ceph-node} # 创建RGW $ ceph-deploy rgw create {ceph-node} # 添加mon $ echo &quot;public network = 172.30.33.0/24&quot; &amp;gt;&amp;gt; ceph.conf $ ceph-deploy mon add {ceph-node} # 查看仲裁 $ ceph quorum_status --format json-pretty 2.2 对象存储测试 # 创建pool # rados mkpool {pool-name} $ rados mkpool test-pool # 创建测试文件 $ dd if=/dev/zero of=testfile bs=1G count=1 # 创建一个对象(这时候也将对象放入了pool中) # rados put {object-name} {file-path} --pool={pool-name} $ rados put test-object testfile --pool=test-pool # 检查存储池，确认ceph存储了此对象 $ rados -p test-pool ls # 定位对象，会输出对象位置 $ ceph osd map test-pool test-object osdmap e42 pool 'test-pool' (3) object 'test-file' -&amp;gt; pg 3.b79653d4 (3.4) -&amp;gt; up ([1,5,2,3,4], p1) acting ([1,5,2,3,4], p1) # 删除对象 $ rados -p data rm test-object # 删除存储池 $ rados rmpool test-pool test-pool --yes-i-really-really-mean-it 随着集群的运行，对象的位置可能会动态改变。Ceph有动态均衡机制，无需手动干预即可完成。 2.3 块存储测试 官方建议使用RBD的客户端最好不要和OSD在同一台物理机上(除非它们都是VM) 1.确认你使用了合适的内核版本，详情参见 lab_release -a uname -a 2.在管理节点上用 ceph-deploy安装ceph ceph-deploy install {rbd-client} 3.在管理节点上部署ceph cli工具和密钥 ceph-deploy admin {rbd-client} 4.在rbd节点上创建块设备image rbd create test-block --size 4096 5.映射image到块设备 rbd map test-block --name client.admin 在上面的map映射操作时，会出现如下报错 $ rbd map test-block --name client.admin rbd: sysfs write failed RBD image feature set mismatch. You can disable features unsupported by the kernel with &quot;rbd feature disable&quot;. In some cases useful info is found in syslog - try &quot;dmesg | tail&quot; or so. rbd: map failed: (6) No such device or address 大致意思是说features不匹配，可以通过disable features关掉一些特性来让内核支持。这是因为在Ceph高本本进行 map image时，默认ceph在创建image(上文test-block)时，会增加很多features，这些features需要内核支持，centos7上的支持有限，所以，我们需要关掉一些 我们可以用 rbd info data 看看创建的image目前有哪些features $ rbd info test-block rbd image 'test-block': size 4096 MB in 1024 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.10bb238e1f29 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: 在features中，我们可以看到默认开启了很多： layering 支持分层 exclusive-lock 支持独占锁 object-map 支持对象映射(依赖exclusive-lock) fast-diff 快速计算差异(依赖object-map) deep-flatten 支持快照扁平化操作 而实际上在CentOS7的3.10内核中只支持layering，所以我们需要手动关闭一些features，然后重新map；如果想要一劳永逸，可以在 ceph.conf 中加入 rbd_default_features = 1 来设置默认 features(数值仅是 layering 对应的 bit 码所对应的整数值) 6.关闭不支持的特性之后重新map # 关闭不支持的features $ rbd feature disable test-block exclusive-lock, object-map, fast-diff, deep-flatten # 重新map $ rbd map test-block --name client.admin /dev/rbd0 7.格式化之后挂载到系统目录 # 格式化 $ mkfs.xfs /dev/rbd0 meta-data=/dev/rbd0 isize=512 agcount=9, agsize=130048 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0 data = bsize=4096 blocks=1048576, imaxpct=25 = sunit=1024 swidth=1024 blks naming =version 2 bsize=4096 ascii-ci=0 ftype=1 log =internal log bsize=4096 blocks=2560, version=2 = sectsz=512 sunit=8 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 # 挂载 $ mkdir test-block $ mount /dev/rbd0 test-block # 写入测试 $ dd if=/dev/zero of=test-block/test-file bs=1G count=1 1+0 records in 1+0 records out 1073741824 bytes (1.1 GB) copied, 2.96071 s, 363 MB/s $ ls test-block/ test-file 2.4 CephFS 测试 1.创建MDS $ ceph-deploy mds create k8s-node01 k8s-node02 k8s-registry 2.创建pool和fs，创建pool需要指定PG数量 ceph osd pool create cephfs_data 32 ceph osd pool create cephfs_metadata 32 ceph fs new test-fs cephfs_metadata cephfs_data PG 概念： 当Ceph集群接受到存储请求时，ceph会将一个文件会切分为多个Object，每个Object会被映射到一个PG，每个PG 会根据CRUSH算法映射到一组OSD(根据副本数)；一般来说增加PG的数量能降低OSD负载，一般每个OSD大约分配50～100PG，关于PG数量指定，一般遵循以下公式 集群PG总数 = (OSD总数 * 100)/数据最大副本数 单个存储池PG数 = (OSD总数 * 100)/数据最大副本数/存储池数 注意：PG的最终结果应当以最接近以上计算公式的2的N次幂(向上取值)；如我的虚拟机环境的每个存储池 PG数 = 6(OSD) * 100 / 5(副本数) / 4（4个存储池）= 30，向上取2的N次幂为32(即，2的5次方=32，最接近30) 3.挂载CephFS有两种方式，一种是使用内核驱动挂载，一种是使用 ceph-fuse用户空间挂载 内核挂载需要提取ceph管理key，方式如下： 在密钥文件中找到与某用户对于的密钥 $ cat /etc/ceph/ceph.client.admin.keyring [client.admin] key = AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA== 复制密钥到文件中保存，并确保其权限 echo &quot;AQBferZZEKXFLxAAhlzElpm2MhhbBGB4TnNVkA==&quot; &amp;gt; ceph-key 创建目录挂载 mkdir test-fs mount -t ceph 172.30.33.90:6789:/ /root/test-fs -o name=admin,secretfile=ceph-key #写入数据测试 $ dd if=/dev/zero of=test-fs/test-fs bs=1G count=1 1+0 records in 1+0 records out 1073741824 bytes (1.1 GB) copied, 2.77355 s, 387 MB/s ceph-fuse用户空间挂载的方式也比较简单，需要先安装ceph-fuse，同时也需要key # 按照前面的步骤添加ceph源 $ vi /etc/yum.repos.d/ceph.repo [ceph] name=ceph baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/x86_64/ gpgcheck=0 [ceph-noarch] name=cephnoarch baseurl=http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/ gpgcheck=0 # 安装ceph-fuse yum install ceph-fuse -y # 复制配置和ceph key到client端 sudo mkdir -p /etc/ceph sudo scp root@172.30.33.91:/etc/ceph/ceph.conf /etc/ceph/ceph.conf sudo scp root@172.30.33.91:/etc/ceph/ceph.client.admin.keyring /etc/ceph/ceph.client.admin.keyring # 创建目录挂载 mkdir test-fs-fuse $ sudo ceph-fuse -m 172.30.33.91:6789 test-fs-fuse ceph-fuse[60551]: starting ceph client 2017-09-12 14:47:24.137929 7f63d9719ec0 -1 init, newargv = 0x7f63e51d4840 newargc=11 ceph-fuse[60551]: starting fuse # 写入数据测试 $ sudo dd if=/dev/zero of=test-fs-fuse/test-fs-fuse bs=1G count=1 1+0 records in 1+0 records out 1073741824 bytes (1.1 GB) copied, 10.5426 s, 102 MB/s # 查看确认，发现我们上面通过内核挂载的文件也还在 $ ls -lh total 2.0G -rw-r--r-- 1 root root 1.0G Sep 12 13:59 test-fs -rw-r--r-- 1 root root 1.0G Sep 12 14:49 test-fs-fuse 2.5 Ceph对象网关 1.对象网关创建 # 创建RGW $ ceph-deploy rgw create k8s-node02 2.直接访问http://ceph-node-ip:7480返回结果如下 &amp;lt;ListAllMyBucketsResult xmlns=&quot;http://s3.amazonaws.com/doc/2006-03-01/&quot;&amp;gt; &amp;lt;Owner&amp;gt; &amp;lt;ID&amp;gt;anonymous&amp;lt;/ID&amp;gt; &amp;lt;DisplayName/&amp;gt; &amp;lt;/Owner&amp;gt; &amp;lt;Buckets/&amp;gt; &amp;lt;/ListAllMyBucketsResult&amp;gt; 这就说明网关OK了，但是因为没有读写环境，所以暂时测不了。 本文参考了ceph官方文档及漠然的ceph笔记(一)部分</summary></entry></feed>